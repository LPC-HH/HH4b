{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c56774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reloads imported files on edits\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from HH4b import utils\n",
    "from HH4b import postprocessing\n",
    "import itertools\n",
    "import correctionlib\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = [\"2022\", \"2022EE\", \"2023\", \"2023BPix\"]\n",
    "YEARS_COMBINED_DICT = {\n",
    "    \"2022All\": [\"2022\", \"2022EE\"],\n",
    "    \"2023All\": [\"2023\", \"2023BPix\"],\n",
    "}\n",
    "# YEARS = [\"2023BPix\"]\n",
    "# YEARS_COMBINED_DICT = {\n",
    "#     \"2023All\": [\"2023BPix\"],\n",
    "# }\n",
    "\n",
    "PROCESSED_PATH: Path = Path(\"Zbb_events_combined.pkl\")\n",
    "PROCESSED_PATH_ERAS: Path = Path(\"Zbb_events_eras.pkl\")\n",
    "REPROCESS: bool = False  # if True, reprocess from the skimmed ntuples\n",
    "APPLY_Zto2Q_CORR: bool = True\n",
    "APPLY_TRIGGER_SF: bool = True\n",
    "\n",
    "SAMPLES_DICT = {\n",
    "    \"data\": [f\"{key}_Run\" for key in [\"JetMET\"]],\n",
    "    \"ttbar\": [\"TTto4Q\", \"TTtoLNu2Q\"],\n",
    "    \"qcd\": [\"QCD_HT\"],\n",
    "    \"hbb\": [\"GluGluHto2B_M-125\"],\n",
    "    \"Zto2Q\": [\"Zto2Q-4Jets\"],\n",
    "    \"Wto2Q\": [\"Wto2Q-3Jets\"],\n",
    "}\n",
    "MC_SAMPLES_LIST = [sample for sample in SAMPLES_DICT.keys() if sample != \"data\"]\n",
    "\n",
    "# Columns to load from the ntuples\n",
    "sys_vars = [\"FSRPartonShower\", \"ISRPartonShower\", \"pileup\"]\n",
    "\n",
    "fatjet_vars = [\n",
    "    \"bbFatJetPt\",\n",
    "    \"bbFatJetEta\",\n",
    "    \"bbFatJetMsd\",\n",
    "    \"bbFatJetParTmassVis\",\n",
    "    \"bbFatJetPNetMassLegacy\",\n",
    "    \"bbFatJetParTTXbb\",\n",
    "]\n",
    "\n",
    "pt_variations = []\n",
    "for jesr, ud in itertools.product([\"JES\", \"JER\"], [\"up\", \"down\"]):\n",
    "    pt_variations.append(f\"bbFatJetPt_{jesr}_{ud}\")\n",
    "\n",
    "mass_variations = []\n",
    "for jmsr, ud in itertools.product([\"JMS\", \"JMR\"], [\"up\", \"down\"]):\n",
    "    mass_variations.append(f\"bbFatJetMsd_{jmsr}_{ud}\")\n",
    "    mass_variations.append(f\"bbFatJetParTmassVis_{jmsr}_{ud}\")\n",
    "    mass_variations.append(f\"bbFatJetPNetMassLegacy_{jmsr}_{ud}\")\n",
    "\n",
    "\n",
    "base_columns = [(var, 2) for var in fatjet_vars] + [(\"weight\", 1)]\n",
    "\n",
    "triggers = {\n",
    "    \"2022\": [\n",
    "        \"AK8PFJet500\",\n",
    "        \"AK8PFJet420_MassSD30\",\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "    ],\n",
    "    \"2022EE\": [\n",
    "        \"AK8PFJet500\",\n",
    "        \"AK8PFJet420_MassSD30\",\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "    ],\n",
    "    \"2023\": [\n",
    "        \"AK8PFJet500\",\n",
    "        \"AK8PFJet420_MassSD30\",\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "        \"AK8PFJet230_SoftDropMass40_PNetBB0p06\",\n",
    "    ],\n",
    "    \"2023BPix\": [\n",
    "        \"AK8PFJet500\",\n",
    "        \"AK8PFJet420_MassSD30\",\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet230_SoftDropMass40_PNetBB0p06\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "load_columns_pt_var = []\n",
    "for pt_var in pt_variations:\n",
    "    load_columns_pt_var.append((pt_var, 2))\n",
    "\n",
    "load_columns_mass_var = []\n",
    "for mass_var in mass_variations:\n",
    "    load_columns_mass_var.append((mass_var, 2))\n",
    "\n",
    "load_weight_shifts = []\n",
    "for var, ud in itertools.product(sys_vars, [\"Up\", \"Down\"]):\n",
    "    load_weight_shifts.append((f\"weight_{var}{ud}\", 1))\n",
    "\n",
    "MC_common_extra_columns = load_columns_mass_var + load_columns_pt_var + load_weight_shifts\n",
    "\n",
    "ZQQ_extra_columns = [(\"GenZPt\", 1), (\"GenZBB\", 1), (\"GenZCC\", 1), (\"bbFatJetVQQMatch\", 2)]\n",
    "WQQ_extra_columns = [(\"GenWPt\", 1), (\"GenWCS\", 1), (\"GenWUD\", 1), (\"bbFatJetVQQMatch\", 2)]\n",
    "\n",
    "extra_columns_dict = {\n",
    "    \"data\": [],\n",
    "    \"qcd\": load_weight_shifts,\n",
    "    \"ttbar\": MC_common_extra_columns,\n",
    "    \"hbb\": MC_common_extra_columns,\n",
    "    \"Zto2Q\": MC_common_extra_columns + ZQQ_extra_columns,\n",
    "    \"Wto2Q\": MC_common_extra_columns + WQQ_extra_columns,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger efficiency corrections\n",
    "trigger_sf_dir = Path(\"../corrections/data/trigger_sfs\").resolve()\n",
    "trigger_eff_txbb = {\n",
    "    year: correctionlib.CorrectionSet.from_file(\n",
    "        str(trigger_sf_dir / f\"fatjet_triggereff_{year}_txbbGloParT_QCD.json\")\n",
    "    )\n",
    "    for year in YEARS\n",
    "}\n",
    "trigger_eff_ptmsd = {\n",
    "    year: correctionlib.CorrectionSet.from_file(\n",
    "        str(trigger_sf_dir / f\"fatjet_triggereff_{year}_ptmsd_QCD.json\")\n",
    "    )\n",
    "    for year in YEARS\n",
    "}\n",
    "\n",
    "\n",
    "def _compute_SF(mc_eff_set, data_eff_set, *args):\n",
    "    \"\"\"Helper function to compute scale factor and error for a given efficiency set.\"\"\"\n",
    "    # Evaluate MC efficiencies\n",
    "    mc_eff_nom = mc_eff_set.evaluate(*args, \"nominal\")\n",
    "    mc_eff_err_up = mc_eff_set.evaluate(*args, \"stat_up\")\n",
    "    mc_eff_err_down = mc_eff_set.evaluate(*args, \"stat_dn\")\n",
    "    mc_eff_err = np.maximum(np.abs(mc_eff_err_up), np.abs(mc_eff_err_down))\n",
    "\n",
    "    # Evaluate data efficiencies\n",
    "    data_eff_nom = data_eff_set.evaluate(*args, \"nominal\")\n",
    "    data_eff_up = data_eff_set.evaluate(*args, \"stat_up\")\n",
    "    data_eff_down = data_eff_set.evaluate(*args, \"stat_dn\")\n",
    "    data_eff_err = np.maximum(np.abs(data_eff_up), np.abs(data_eff_down))\n",
    "\n",
    "    # Compute scale factor and propagate errors\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        sf_nom = data_eff_nom / mc_eff_nom\n",
    "        sf_err = sf_nom * np.sqrt(\n",
    "            (data_eff_err / data_eff_nom) ** 2 + (mc_eff_err / mc_eff_nom) ** 2\n",
    "        )\n",
    "\n",
    "    # set sf to 1 if mc_eff_nom is zero to avoid division by zero\n",
    "    sf_nom = np.where(mc_eff_nom == 0, 1.0, sf_nom)\n",
    "    sf_err = np.where(mc_eff_nom == 0, 0.0, sf_err)\n",
    "    sf_err = np.where(data_eff_nom == 0, 0.0, sf_err)\n",
    "    # sf_nom = np.where(sf_nom > 2.0, 1.0, sf_nom)  # restrict scale factor to a maximum of 2.0\n",
    "    # sf_err = np.where(sf_nom > 2.0, 0.0, sf_err)  # restrict scale factor error to a maximum of 2.0\n",
    "\n",
    "    return sf_nom, sf_err\n",
    "\n",
    "\n",
    "def eval_trigger_sf(\n",
    "    txbb: np.ndarray, pt: np.ndarray, msd: np.ndarray, year: str\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Evaluate trigger scale factors with error propagation.\"\"\"\n",
    "\n",
    "    # txbb scale factor\n",
    "    mc_eff_set = trigger_eff_txbb[year][f\"fatjet_triggereffmc_{year}_txbbGloParT\"]\n",
    "    data_eff_set = trigger_eff_txbb[year][f\"fatjet_triggereffdata_{year}_txbbGloParT\"]\n",
    "    sf_txbb_nom, sf_txbb_err = _compute_SF(mc_eff_set, data_eff_set, txbb)\n",
    "\n",
    "    # ptmsd scale factor\n",
    "    mc_eff_set = trigger_eff_ptmsd[year][f\"fatjet_triggereffmc_{year}_ptmsd\"]\n",
    "    data_eff_set = trigger_eff_ptmsd[year][f\"fatjet_triggereffdata_{year}_ptmsd\"]\n",
    "    sf_ptmsd_nom, sf_ptmsd_err = _compute_SF(mc_eff_set, data_eff_set, pt, msd)\n",
    "\n",
    "    # Combine scale factors\n",
    "    sf = sf_txbb_nom * sf_ptmsd_nom\n",
    "    sf_err = sf * np.sqrt((sf_txbb_err / sf_txbb_nom) ** 2 + (sf_ptmsd_err / sf_ptmsd_nom) ** 2)\n",
    "\n",
    "    sf_up = sf + sf_err\n",
    "    sf_down = sf - sf_err\n",
    "\n",
    "    return sf, sf_up, sf_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba476244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, apply the Z->2Q corrections from ZMuMu measurement\n",
    "if APPLY_Zto2Q_CORR:\n",
    "\n",
    "    corr_dir = Path(\"ZMuMu_corrs\")\n",
    "    corr_dict = {}\n",
    "\n",
    "    for year in [\"2022\", \"2023\"]:\n",
    "        corr_file = corr_dir / f\"corr_{year}.json\"\n",
    "        if not corr_file.exists():\n",
    "            raise FileNotFoundError(f\"Correction file {corr_file} does not exist.\")\n",
    "\n",
    "        # Load the correction\n",
    "        corr = correctionlib.CorrectionSet.from_file(str(corr_file))\n",
    "        corr_dict[year] = corr\n",
    "        print(f\"Loaded correction for {year} from {corr_file}\")\n",
    "else:\n",
    "    corr_dict = None\n",
    "    print(\"Z->2Q corrections are not applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93acecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPROCESS or not PROCESSED_PATH.exists():\n",
    "    path_dir = \"/ceph/cms/store/user/zichun/bbbb/skimmer/ZbbHT25May28_v12v2_private_zbb/\"\n",
    "\n",
    "    events_dict = {}\n",
    "    for year in YEARS:\n",
    "        # for year in [\"2022\"]:\n",
    "        events_dict[year] = {}\n",
    "\n",
    "        # Have to load the samples separately because branches vary\n",
    "        for sample, sample_list in SAMPLES_DICT.items():\n",
    "            print(f\"Loading {sample} for {year}...\")\n",
    "            triggers_cols = [(trigger, 1) for trigger in triggers[year]]\n",
    "\n",
    "            # append the event dictionary for each year\n",
    "            columns = triggers_cols + base_columns + extra_columns_dict.get(sample, [])\n",
    "            dataframes = {\n",
    "                **utils.load_samples(\n",
    "                    data_dir=path_dir,\n",
    "                    samples={sample: sample_list},\n",
    "                    year=year,\n",
    "                    columns=utils.format_columns(columns),\n",
    "                    variations=True,\n",
    "                    weight_shifts=[\"FSRPartonShower\", \"ISRPartonShower\", \"pileup\"],\n",
    "                )\n",
    "            }\n",
    "            # concatenate all dataframes in this sample\n",
    "            events_dict[year][sample] = []\n",
    "            for key, df in dataframes.items():\n",
    "                # if pT variations are not present, set them to pT\n",
    "                for pt_var in [\"bbFatJetPt\"] + pt_variations:\n",
    "                    if pt_var not in df.columns:\n",
    "                        for i in range(2):\n",
    "                            df[f\"{pt_var}{i}\"] = df[(\"bbFatJetPt\", i)].copy()\n",
    "\n",
    "                # if mass variations are not present, set them to mass\n",
    "                for mass_var in [\n",
    "                    \"bbFatJetMsd\",\n",
    "                    \"bbFatJetParTmassVis\",\n",
    "                    \"bbFatJetPNetMassLegacy\",\n",
    "                ] + mass_variations:\n",
    "                    if mass_var not in df.columns:\n",
    "                        for i in range(2):\n",
    "                            df[f\"{mass_var}{i}\"] = df[(mass_var.split(\"_\")[0], i)].copy()\n",
    "\n",
    "                if sample != \"data\":\n",
    "                    # evalute trigger scale factors\n",
    "                    sf, sf_up, sf_down = eval_trigger_sf(\n",
    "                        txbb=df[(\"bbFatJetParTTXbb\", 0)].values,\n",
    "                        pt=df[(\"bbFatJetPt\", 0)].values,\n",
    "                        msd=df[(\"bbFatJetMsd\", 0)].values,\n",
    "                        year=year,\n",
    "                    )\n",
    "                    df[\"SF_trigger\"] = sf\n",
    "                    df[\"SF_trigger_up\"] = sf_up\n",
    "                    df[\"SF_trigger_down\"] = sf_down\n",
    "\n",
    "                events_dict[year][sample].append(df)\n",
    "\n",
    "            # concatenate all dataframes for this sample\n",
    "            events_dict[year][sample] = pd.concat(events_dict[year][sample], ignore_index=True)\n",
    "\n",
    "    # Combine events from different years into a single dictionary\n",
    "    events_combined = {year: {} for year in YEARS_COMBINED_DICT.keys()}\n",
    "    for sample in SAMPLES_DICT.keys():\n",
    "        for combined_year, year_list in YEARS_COMBINED_DICT.items():\n",
    "            events_combined[combined_year][sample] = pd.concat(\n",
    "                [events_dict[year][sample] for year in year_list if sample in events_dict[year]]\n",
    "            )\n",
    "\n",
    "    # Store events_combined as a pickle file\n",
    "    with PROCESSED_PATH.open(\"wb\") as f:\n",
    "        pd.to_pickle(events_combined, f)\n",
    "    print(f\"Events combined and saved to {PROCESSED_PATH}\")\n",
    "\n",
    "    with PROCESSED_PATH_ERAS.open(\"wb\") as f:\n",
    "        pd.to_pickle(events_dict, f)\n",
    "    print(f\"Events by eras and saved to {PROCESSED_PATH_ERAS}\")\n",
    "\n",
    "    del events_dict  # Free memory\n",
    "\n",
    "else:\n",
    "    # Directly load the processed file\n",
    "    print(f\"Loading events from {PROCESSED_PATH}...\")\n",
    "    with PROCESSED_PATH.open(\"rb\") as f:\n",
    "        events_combined = pd.read_pickle(f)\n",
    "    print(f\"Loaded events from {PROCESSED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07954dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply ZQQ corrections if needed\n",
    "if APPLY_Zto2Q_CORR:\n",
    "    print(\"Applying Zto2Q corrections...\")\n",
    "    for year in YEARS_COMBINED_DICT.keys():\n",
    "        # apply corrections to the events\n",
    "        corr = corr_dict[year.replace(\"All\", \"\")][\"GenZPtWeight\"]\n",
    "        GenZ_pt = events_combined[year][\"Zto2Q\"][\"GenZPt\"].values[:, 0]\n",
    "        sf_nom = corr.evaluate(GenZ_pt, \"nominal\")\n",
    "        sf_up = corr.evaluate(GenZ_pt, \"stat_up\")\n",
    "        sf_down = corr.evaluate(GenZ_pt, \"stat_down\")\n",
    "        events_combined[year][\"Zto2Q\"][\"SF_GenZPt\"] = sf_nom\n",
    "        events_combined[year][\"Zto2Q\"][\"SF_GenZPt_up\"] = sf_up\n",
    "        events_combined[year][\"Zto2Q\"][\"SF_GenZPt_down\"] = sf_down\n",
    "\n",
    "        # apply the scale factors to the final weight\n",
    "        weight = events_combined[year][\"Zto2Q\"][\"finalWeight\"]\n",
    "        events_combined[year][\"Zto2Q\"][\"finalWeight\"] = weight * sf_nom\n",
    "        events_combined[year][\"Zto2Q\"][\"weight_GenZPtUp\"] = weight * sf_up\n",
    "        events_combined[year][\"Zto2Q\"][\"weight_GenZPtDown\"] = weight * sf_down\n",
    "    print(\"Zto2Q corrections applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0274383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further split Zto2Q and Wto2Q events into different categories\n",
    "for year in YEARS_COMBINED_DICT.keys():\n",
    "    Zto2Q = events_combined[year][\"Zto2Q\"]\n",
    "    matched = Zto2Q[(\"bbFatJetVQQMatch\", 0)] == 1\n",
    "    is_ZBB = Zto2Q[(\"GenZBB\", 0)]\n",
    "    is_ZCC = Zto2Q[(\"GenZCC\", 0)]\n",
    "    is_ZQQ = ~(is_ZBB | is_ZCC)  # u, d, s quarks\n",
    "    ZtoBB = is_ZBB & matched\n",
    "    ZtoCC = is_ZCC & matched\n",
    "    ZtoQQ = is_ZQQ & matched\n",
    "    Z_unmatched = ~matched\n",
    "    events_combined[year][\"Zto2Q_BB\"] = Zto2Q[ZtoBB]\n",
    "    events_combined[year][\"Zto2Q_CC\"] = Zto2Q[ZtoCC]\n",
    "    events_combined[year][\"Zto2Q_QQ\"] = Zto2Q[ZtoQQ]\n",
    "    events_combined[year][\"Zto2Q_unmatched\"] = Zto2Q[Z_unmatched]\n",
    "\n",
    "MC_SAMPLES_FINAL_LIST = MC_SAMPLES_LIST + [\"Zto2Q_BB\", \"Zto2Q_CC\", \"Zto2Q_QQ\", \"Zto2Q_unmatched\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass and fail regions\n",
    "txbb_bins = [0.95, 0.975, 0.99, 1.0]\n",
    "min_txbb = txbb_bins[0]\n",
    "# pT bins\n",
    "pt_bins = [350, 450, 550, 10000]\n",
    "# pt_bins = [350, 450, 500, 550, 10000]\n",
    "\n",
    "txbb_bins = list(zip(txbb_bins[:-1], txbb_bins[1:]))\n",
    "pt_bins = list(zip(pt_bins[:-1], pt_bins[1:]))\n",
    "\n",
    "# Mass bins\n",
    "m_low, m_high = 50, 150\n",
    "bins = 5\n",
    "n_mass_bins = int((m_high - m_low) / bins)\n",
    "\n",
    "\n",
    "def save_to_root(outfile: Path, templates: dict):\n",
    "    with uproot.recreate(str(outfile)) as f_out:\n",
    "        for category in templates.keys():\n",
    "            hist = templates[category]\n",
    "            categories, _ = hist.axes\n",
    "            for sample in list(categories):\n",
    "                h = templates[category][{\"Sample\": sample}]\n",
    "                f_out[f\"{sample}_{category}\"] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_triggers(events, trigger_list):\n",
    "    print(f\"Selecting events with triggers: {trigger_list}\")\n",
    "    events_filtered = {}\n",
    "\n",
    "    for year in events.keys():\n",
    "        events_filtered[year] = {}\n",
    "        for sample in events[year].keys():\n",
    "            df = events[year][sample].copy()\n",
    "            mask = np.zeros(len(df), dtype=bool)\n",
    "            for trigger in trigger_list:\n",
    "                if trigger in df.columns:\n",
    "                    mask = mask | (df[trigger].values.reshape(-1) == 1)\n",
    "            events_filtered[year][sample] = df[mask].copy()\n",
    "            num_sel = mask.sum()\n",
    "            num_total = len(df)\n",
    "            print(\n",
    "                f\"Year: {year}, Sample: {sample}, Selected: {num_sel}, Total: {num_total}, Efficiency: {num_sel / num_total:.2%}\"\n",
    "            )\n",
    "    return events_filtered\n",
    "\n",
    "\n",
    "trigger_list_high_pt = [\n",
    "    \"AK8PFJet500\",\n",
    "    \"AK8PFJet420_MassSD30\",\n",
    "    \"AK8PFJet425_SoftDropMass40\",\n",
    "]\n",
    "\n",
    "trigger_list_PNet = [\n",
    "    \"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "    \"AK8PFJet230_SoftDropMass40_PNetBB0p06\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2fc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_high_pt = select_triggers(events_combined, trigger_list_high_pt)\n",
    "events_PNet = select_triggers(events_combined, trigger_list_PNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42369211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply trigger sf to events_PNet\n",
    "if APPLY_TRIGGER_SF:\n",
    "    print(\"Applying trigger scale factors to events_PNet...\")\n",
    "    for year in events_PNet.keys():\n",
    "        for sample in events_PNet[year].keys():\n",
    "            df = events_PNet[year][sample]\n",
    "            if sample != \"data\":\n",
    "                sf_nom = df[\"SF_trigger\"]\n",
    "                sf_up = df[\"SF_trigger_up\"]\n",
    "                sf_down = df[\"SF_trigger_down\"]\n",
    "\n",
    "                # apply the scale factors to the final weight\n",
    "                weight = df[\"finalWeight\"]\n",
    "                df[\"finalWeight\"] = df[\"finalWeight\"] * sf_nom\n",
    "                df[\"weight_TriggerUp\"] = weight * sf_up\n",
    "                df[\"weight_TriggerDown\"] = weight * sf_down\n",
    "            events_PNet[year][sample] = df\n",
    "else:\n",
    "    print(\"Trigger scale factors are not applied to events_PNet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bkg_keys = [\"Zto2Q_CC\", \"Zto2Q_QQ\", \"Zto2Q_unmatched\", \"Wto2Q\", \"hbb\", \"ttbar\", \"qcd\"]\n",
    "# sig_keys = [\"Zto2Q_BB\"]\n",
    "# use this if you want to include Zto2Q_BB in the stack plot\n",
    "bkg_keys = [\"Zto2Q_BB\", \"Zto2Q_CC\", \"Zto2Q_QQ\", \"Zto2Q_unmatched\", \"Wto2Q\", \"hbb\", \"ttbar\", \"qcd\"]\n",
    "sig_keys = []\n",
    "bg_order = list(reversed(bkg_keys))\n",
    "\n",
    "jshift_keys = [\"\"]\n",
    "for var, ud in itertools.product([\"JES\", \"JER\", \"JMS\", \"JMR\"], [\"up\", \"down\"]):\n",
    "    jshift_keys.append(f\"{var}_{ud}\")\n",
    "\n",
    "weight_shifts = {\n",
    "    \"pileup\": postprocessing.Syst(\n",
    "        samples=MC_SAMPLES_FINAL_LIST, label=\"Pileup\", years=list(YEARS_COMBINED_DICT.keys())\n",
    "    ),\n",
    "    # \"pdf\": postprocessing.Syst(samples=sig_keys, label=\"PDFAcc\", years=list(YEARS_COMBINED_DICT.keys())),\n",
    "    \"ISRPartonShower\": postprocessing.Syst(\n",
    "        samples=MC_SAMPLES_FINAL_LIST,\n",
    "        label=\"ISR Parton Shower\",\n",
    "        years=list(YEARS_COMBINED_DICT.keys()),\n",
    "    ),\n",
    "    \"FSRPartonShower\": postprocessing.Syst(\n",
    "        samples=MC_SAMPLES_FINAL_LIST,\n",
    "        label=\"FSR Parton Shower\",\n",
    "        years=list(YEARS_COMBINED_DICT.keys()),\n",
    "    ),\n",
    "}\n",
    "\n",
    "if APPLY_Zto2Q_CORR:\n",
    "    weight_shifts[\"GenZPt\"] = postprocessing.Syst(\n",
    "        samples=[\"Zto2Q_BB\", \"Zto2Q_CC\", \"Zto2Q_QQ\", \"Zto2Q_unmatched\"],\n",
    "        label=\"Gen Z pT correction derived from ZMuMu\",\n",
    "        years=list(YEARS_COMBINED_DICT.keys()),\n",
    "    )\n",
    "\n",
    "if APPLY_TRIGGER_SF:\n",
    "    weight_shifts_trig_sf = {\n",
    "        \"Trigger\": postprocessing.Syst(\n",
    "            samples=MC_SAMPLES_FINAL_LIST,\n",
    "            label=\"Trigger SF of the PNet trigger\",\n",
    "            years=list(YEARS_COMBINED_DICT.keys()),\n",
    "        ),\n",
    "    }\n",
    "else:\n",
    "    weight_shifts_trig_sf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a93376",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in YEARS_COMBINED_DICT:\n",
    "    out_dir = Path(f\"templates_zbb\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cutflows_dir = Path(f\"{out_dir}/cutflows/{year}\")\n",
    "    cutflows_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    plot_dir = Path(f\"{out_dir}/plots/{year}\")\n",
    "    plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    template_dir = out_dir\n",
    "    template_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    templates = {}\n",
    "    for jshift in jshift_keys:\n",
    "        # Determine the pt and mass variations\n",
    "        if jshift == \"\":\n",
    "            pt_branch = \"bbFatJetPt0\"\n",
    "            mass_branch = \"bbFatJetParTmassVis0\"\n",
    "        elif jshift.startswith(\"JES\") or jshift.startswith(\"JER\"):\n",
    "            pt_branch = f\"bbFatJetPt_{jshift}0\"\n",
    "            mass_branch = \"bbFatJetParTmassVis0\"\n",
    "        elif jshift.startswith(\"JMS\") or jshift.startswith(\"JMR\"):\n",
    "            pt_branch = \"bbFatJetPt0\"\n",
    "            mass_branch = f\"bbFatJetParTmassVis_{jshift}0\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown jshift: {jshift}\")\n",
    "\n",
    "        # Different different pass regions based on TXbb and pT bins\n",
    "        selection_regions = {}\n",
    "        for (txbb_low, txbb_high), (pt_low, pt_high) in itertools.product(txbb_bins, pt_bins):\n",
    "            # Convert to strings\n",
    "            txbb_low_str = str(txbb_low).replace(\".\", \"p\")\n",
    "            txbb_high_str = str(txbb_high).replace(\".\", \"p\")\n",
    "            pt_low_str = str(pt_low)\n",
    "            pt_high_str = str(pt_high)\n",
    "            region_key = f\"pass_TXbb{txbb_low_str}to{txbb_high_str}_pT{pt_low_str}to{pt_high_str}\"\n",
    "\n",
    "            # determine which trigger to use\n",
    "            if pt_low < 550:\n",
    "                events = events_PNet[year]\n",
    "                weight_shifts_final = {\n",
    "                    **weight_shifts,\n",
    "                    **weight_shifts_trig_sf,\n",
    "                }\n",
    "                print(f\"Using PNet trigger for {region_key} in {year}\")\n",
    "            else:\n",
    "                events = events_high_pt[year]\n",
    "                weight_shifts_final = weight_shifts\n",
    "                print(f\"Using high pT trigger for {region_key} in {year}\")\n",
    "\n",
    "            cutflows = {}\n",
    "            for sample in events:\n",
    "                cutflows[sample] = OrderedDict()\n",
    "                cutflows[sample][\"Skimmer Preselection\"] = events_combined[year][sample][\n",
    "                    \"finalWeight\"\n",
    "                ].sum()\n",
    "                cutflows[sample][\"HLT\"] = events_combined[year][sample][\"finalWeight\"].sum()\n",
    "            cutflows = pd.DataFrame.from_dict(cutflows).transpose()\n",
    "\n",
    "            # Create a region\n",
    "            selection_regions[region_key] = postprocessing.Region(\n",
    "                cuts={\n",
    "                    pt_branch: [pt_low, pt_high],\n",
    "                    mass_branch: [m_low, m_high],\n",
    "                    \"bbFatJetParTTXbb0\": [txbb_low, txbb_high],\n",
    "                },\n",
    "                label=region_key,\n",
    "            )\n",
    "\n",
    "        selection_regions[\"fail\"] = postprocessing.Region(\n",
    "            cuts={\n",
    "                pt_branch: [pt_low, pt_high],\n",
    "                mass_branch: [m_low, m_high],\n",
    "                \"bbFatJetParTTXbb0\": [0.1, min(0.9, min_txbb)],\n",
    "            },\n",
    "            label=\"fail\",\n",
    "        )\n",
    "        print(f\"Selection regions for {year} with jshift {jshift}: {selection_regions.keys()}\")\n",
    "\n",
    "        fit_shape_var = postprocessing.ShapeVar(\n",
    "            mass_branch,\n",
    "            r\"$m_\\mathrm{reg}$ (GeV)\",\n",
    "            [n_mass_bins, m_low, m_high],\n",
    "            reg=True,\n",
    "        )\n",
    "\n",
    "        ttemps = postprocessing.get_templates(\n",
    "            events,\n",
    "            year=year,\n",
    "            sig_keys=sig_keys,\n",
    "            plot_sig_keys=sig_keys,\n",
    "            selection_regions=selection_regions,\n",
    "            shape_vars=[fit_shape_var],\n",
    "            systematics={},\n",
    "            template_dir=out_dir,\n",
    "            bg_keys=bkg_keys,\n",
    "            bg_order=bg_order,\n",
    "            bg_err_mcstat=False,\n",
    "            plot_dir=plot_dir,\n",
    "            prev_cutflow=cutflows,\n",
    "            weight_key=\"finalWeight\",\n",
    "            weight_shifts=weight_shifts_final,\n",
    "            plot_shifts=False,\n",
    "            show=False,\n",
    "            energy=13.6,\n",
    "            jshift=jshift,\n",
    "            blind=False,\n",
    "        )\n",
    "        templates = {**templates, **ttemps}\n",
    "\n",
    "    # Save the templates to a file\n",
    "    outfile = template_dir / f\"templates_{year}.root\"\n",
    "    save_to_root(outfile, templates)\n",
    "    # Save as a pickle file\n",
    "    outfile_pickle = template_dir / f\"templates_{year}.pkl\"\n",
    "    with outfile_pickle.open(\"wb\") as f:\n",
    "        pd.to_pickle(templates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b6bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh4b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
