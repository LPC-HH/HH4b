{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c56774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reloads imported files on edits\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from HH4b import utils\n",
    "from HH4b import postprocessing\n",
    "import itertools\n",
    "import correctionlib\n",
    "from collections import OrderedDict\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = [\"2022\", \"2022EE\", \"2023\", \"2023BPix\"]\n",
    "YEARS_COMBINED_DICT = {\n",
    "    \"2022All\": [\"2022\", \"2022EE\"],\n",
    "    \"2023All\": [\"2023\", \"2023BPix\"],\n",
    "}\n",
    "\n",
    "# tag = \"ZbbHT25July22_v14_25v2_zbb\"\n",
    "# tag = \"ZbbHT25July31_v14_25v2_zbb\"\n",
    "tag = \"ZbbHT25August02JMSR_v14_25v2_zbb\"\n",
    "\n",
    "STORAGE_PROJ_DIR = Path(\"/ceph/cms/store/user/zichun/bbbb\")\n",
    "DATA_DIR = STORAGE_PROJ_DIR / f\"skimmer/{tag}\"\n",
    "# PROCESSED_DIR = STORAGE_PROJ_DIR / f\"zbb_processed/skimmer/processed_{tag}\"\n",
    "PROCESSED_DIR = Path(\"processed\")\n",
    "PROCESSED_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "REPROCESS: bool = False  # if True, reprocess from the skimmed ntuples\n",
    "APPLY_Z_RECOIL_CORR: bool = True\n",
    "APPLY_TRIGGER_SF: bool = True\n",
    "\n",
    "SAMPLES_DICT = {\n",
    "    \"data\": [\"JetMET\"],\n",
    "    # QCD from data in CR\n",
    "    \"ttbar\": [\"TTto4Q\", \"TTtoLNu2Q\"],\n",
    "    \"Zto2Q\": [\"Zto2Q-4Jets\"],\n",
    "    \"Wto2Q\": [\"Wto2Q-3Jets\"],\n",
    "}\n",
    "MC_SAMPLES_LIST = [sample for sample in SAMPLES_DICT.keys() if sample != \"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_lumis = {\n",
    "    # in fb-1\n",
    "    \"2022\": 7.399935622,\n",
    "    \"2022EE\": 26.59626744,\n",
    "    \"2023\": 18.062658919,\n",
    "    \"2023BPix\": 9.506394514,\n",
    "}\n",
    "actual_lumis[\"2022All\"] = actual_lumis[\"2022\"] + actual_lumis[\"2022EE\"]\n",
    "actual_lumis[\"2023All\"] = actual_lumis[\"2023\"] + actual_lumis[\"2023BPix\"]\n",
    "actual_lumis[\"2022-2023\"] = actual_lumis[\"2022All\"] + actual_lumis[\"2023All\"]\n",
    "\n",
    "# convert fb-1 to pb-1\n",
    "for k, v in actual_lumis.items():\n",
    "    actual_lumis[k] = v * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to load from the ntuples\n",
    "sys_vars = [\"FSRPartonShower\", \"ISRPartonShower\", \"pileup\"]\n",
    "weight_shifts = sys_vars + [\"pdf_weights\", \"scale_weights\"]\n",
    "\n",
    "fatjet_vars = [\n",
    "    \"bbFatJetPt\",\n",
    "    \"bbFatJetEta\",\n",
    "    \"bbFatJetMsd\",\n",
    "    \"bbFatJetPNetMassLegacy\",\n",
    "]\n",
    "mass_vars = [\n",
    "    \"bbFatJetMsd\",\n",
    "    \"bbFatJetPNetMassLegacy\",\n",
    "]\n",
    "glopart_mass_vars = [\n",
    "    \"bbFatJetParT2massVis\",\n",
    "    \"bbFatJetParT2massRes\",\n",
    "    \"bbFatJetParT3massGeneric\",\n",
    "    \"bbFatJetParT3massX2p\",\n",
    "]\n",
    "glopart_txbb_vars = [\n",
    "    \"bbFatJetParT2TXbb\",\n",
    "    \"bbFatJetParT3TXbb\",\n",
    "]\n",
    "fatjet_vars += glopart_mass_vars + glopart_txbb_vars\n",
    "mass_vars += glopart_mass_vars\n",
    "\n",
    "pt_variations = []\n",
    "for jesr, ud in itertools.product([\"JES\", \"JER\"], [\"up\", \"down\"]):\n",
    "    pt_variations.append(f\"bbFatJetPt_{jesr}_{ud}\")\n",
    "\n",
    "mass_variations = []\n",
    "for jmsr, ud in itertools.product([\"JMS\", \"JMR\"], [\"up\", \"down\"]):\n",
    "    for var in mass_vars:\n",
    "        mass_variations.append(f\"{var}_{jmsr}_{ud}\")\n",
    "\n",
    "\n",
    "base_columns = [(var, 2) for var in fatjet_vars] + [(\"weight\", 1)]\n",
    "\n",
    "load_columns_pt_var = []\n",
    "for pt_var in pt_variations:\n",
    "    load_columns_pt_var.append((pt_var, 2))\n",
    "\n",
    "load_columns_mass_var = []\n",
    "for mass_var in mass_variations:\n",
    "    load_columns_mass_var.append((mass_var, 2))\n",
    "\n",
    "load_weight_shifts = []\n",
    "for var, ud in itertools.product(sys_vars, [\"Up\", \"Down\"]):\n",
    "    load_weight_shifts.append((f\"weight_{var}{ud}\", 1))\n",
    "\n",
    "MC_common_extra_columns = load_columns_mass_var + load_columns_pt_var + load_weight_shifts\n",
    "\n",
    "ZQQ_extra_columns = [(\"GenZPt\", 1), (\"GenZBB\", 1), (\"GenZCC\", 1), (\"bbFatJetVQQMatch\", 2)]\n",
    "WQQ_extra_columns = [(\"GenWPt\", 1), (\"GenWCS\", 1), (\"GenWUD\", 1), (\"bbFatJetVQQMatch\", 2)]\n",
    "\n",
    "extra_columns_dict = {\n",
    "    \"data\": [],\n",
    "    # \"qcd\": load_weight_shifts,\n",
    "    \"ttbar\": MC_common_extra_columns,\n",
    "    \"hbb\": MC_common_extra_columns,\n",
    "    \"Zto2Q\": MC_common_extra_columns + ZQQ_extra_columns,\n",
    "    \"Wto2Q\": MC_common_extra_columns + WQQ_extra_columns,\n",
    "}\n",
    "extra_columns_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00fd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "triggers = {\n",
    "    \"2022\": [\n",
    "        \"AK8PFJet500\",\n",
    "        \"AK8PFJet420_MassSD30\",\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "    ],\n",
    "    \"2022EE\": [\n",
    "        \"AK8PFJet500\",\n",
    "        \"AK8PFJet420_MassSD30\",\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "    ],\n",
    "    \"2023\": [\n",
    "        \"AK8PFJet500\",\n",
    "        \"AK8PFJet420_MassSD30\",\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "        \"AK8PFJet230_SoftDropMass40_PNetBB0p06\",\n",
    "    ],\n",
    "    \"2023BPix\": [\n",
    "        \"AK8PFJet500\",\n",
    "        \"AK8PFJet420_MassSD30\",\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet230_SoftDropMass40_PNetBB0p06\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger efficiency corrections\n",
    "trigger_sf_dir = Path(\"../corrections/data/trigger_sfs\").resolve()\n",
    "trigger_eff_txbb = {\n",
    "    year: correctionlib.CorrectionSet.from_file(\n",
    "        str(trigger_sf_dir / f\"fatjet_triggereff_{year}_txbbGloParT_QCD.json\")\n",
    "    )\n",
    "    for year in YEARS\n",
    "}\n",
    "trigger_eff_ptmsd = {\n",
    "    year: correctionlib.CorrectionSet.from_file(\n",
    "        str(trigger_sf_dir / f\"fatjet_triggereff_{year}_ptmsd_QCD.json\")\n",
    "    )\n",
    "    for year in YEARS\n",
    "}\n",
    "\n",
    "\n",
    "def _compute_SF(mc_eff_set, data_eff_set, *args):\n",
    "    \"\"\"Helper function to compute scale factor and error for a given efficiency set.\"\"\"\n",
    "    # Evaluate MC efficiencies\n",
    "    mc_eff_nom = mc_eff_set.evaluate(*args, \"nominal\")\n",
    "    mc_eff_err_up = mc_eff_set.evaluate(*args, \"stat_up\")\n",
    "    mc_eff_err_down = mc_eff_set.evaluate(*args, \"stat_dn\")\n",
    "    mc_eff_err = np.maximum(np.abs(mc_eff_err_up), np.abs(mc_eff_err_down))\n",
    "\n",
    "    # Evaluate data efficiencies\n",
    "    data_eff_nom = data_eff_set.evaluate(*args, \"nominal\")\n",
    "    data_eff_up = data_eff_set.evaluate(*args, \"stat_up\")\n",
    "    data_eff_down = data_eff_set.evaluate(*args, \"stat_dn\")\n",
    "    data_eff_err = np.maximum(np.abs(data_eff_up), np.abs(data_eff_down))\n",
    "\n",
    "    # Compute scale factor and propagate errors\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        sf_nom = data_eff_nom / mc_eff_nom\n",
    "        sf_err = sf_nom * np.sqrt(\n",
    "            (data_eff_err / data_eff_nom) ** 2 + (mc_eff_err / mc_eff_nom) ** 2\n",
    "        )\n",
    "\n",
    "    # set sf to 1 if mc_eff_nom is zero to avoid division by zero\n",
    "    sf_nom = np.where(mc_eff_nom == 0, 1.0, sf_nom)\n",
    "    sf_err = np.where(mc_eff_nom == 0, 0.0, sf_err)\n",
    "    sf_err = np.where(data_eff_nom == 0, 0.0, sf_err)\n",
    "    # sf_nom = np.where(sf_nom > 2.0, 1.0, sf_nom)  # restrict scale factor to a maximum of 2.0\n",
    "    # sf_err = np.where(sf_nom > 2.0, 0.0, sf_err)  # restrict scale factor error to a maximum of 2.0\n",
    "\n",
    "    return sf_nom, sf_err\n",
    "\n",
    "\n",
    "def eval_trigger_sf(\n",
    "    txbb: np.ndarray, pt: np.ndarray, msd: np.ndarray, year: str\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Evaluate trigger scale factors with error propagation.\"\"\"\n",
    "\n",
    "    # txbb scale factor\n",
    "    mc_eff_set = trigger_eff_txbb[year][f\"fatjet_triggereffmc_{year}_txbbGloParT\"]\n",
    "    data_eff_set = trigger_eff_txbb[year][f\"fatjet_triggereffdata_{year}_txbbGloParT\"]\n",
    "    sf_txbb_nom, sf_txbb_err = _compute_SF(mc_eff_set, data_eff_set, txbb)\n",
    "    zero_sf_mask = sf_txbb_nom == 0\n",
    "    sf_txbb_nom[zero_sf_mask] = 1.0\n",
    "    sf_txbb_err[zero_sf_mask] = 0.0\n",
    "\n",
    "    # ptmsd scale factor\n",
    "    mc_eff_set = trigger_eff_ptmsd[year][f\"fatjet_triggereffmc_{year}_ptmsd\"]\n",
    "    data_eff_set = trigger_eff_ptmsd[year][f\"fatjet_triggereffdata_{year}_ptmsd\"]\n",
    "    sf_ptmsd_nom, sf_ptmsd_err = _compute_SF(mc_eff_set, data_eff_set, pt, msd)\n",
    "    zero_sf_mask = sf_ptmsd_nom == 0\n",
    "    sf_ptmsd_nom[zero_sf_mask] = 1.0\n",
    "    sf_ptmsd_err[zero_sf_mask] = 0.0\n",
    "\n",
    "    # Combine scale factors\n",
    "    sf = sf_txbb_nom * sf_ptmsd_nom\n",
    "    sf_err = sf * np.sqrt((sf_txbb_err / sf_txbb_nom) ** 2 + (sf_ptmsd_err / sf_ptmsd_nom) ** 2)\n",
    "\n",
    "    sf_up = sf + sf_err\n",
    "    sf_down = sf - sf_err\n",
    "\n",
    "    # Ensure scale factors are not negative\n",
    "    extreme_val_mask = sf_down < 0\n",
    "    sf[extreme_val_mask] = 1.0\n",
    "    sf_up[extreme_val_mask] = 1.0\n",
    "    sf_down[extreme_val_mask] = 1.0\n",
    "\n",
    "    return sf, sf_up, sf_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba476244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, apply the Z->2Q corrections from ZMuMu measurement\n",
    "if APPLY_Z_RECOIL_CORR:\n",
    "\n",
    "    corr_dir = Path(\"ZMuMu_corrs\")\n",
    "    corr_dict = {}\n",
    "\n",
    "    for year in [\"2022\", \"2023\"]:\n",
    "        corr_file = corr_dir / f\"corr_{year}.json\"\n",
    "        if not corr_file.exists():\n",
    "            raise FileNotFoundError(f\"Correction file {corr_file} does not exist.\")\n",
    "\n",
    "        # Load the correction\n",
    "        corr = correctionlib.CorrectionSet.from_file(str(corr_file))\n",
    "        corr_dict[year] = corr\n",
    "        print(f\"Loaded correction for {year} from {corr_file}\")\n",
    "else:\n",
    "    corr_dict = None\n",
    "    print(\"Z->2Q corrections are not applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_era_path(year):\n",
    "    return PROCESSED_DIR / f\"Zbb_events_{year}.pkl\"\n",
    "\n",
    "\n",
    "def get_combined_path(combined_year):\n",
    "    return PROCESSED_DIR / f\"Zbb_events_{combined_year}.pkl\"\n",
    "\n",
    "\n",
    "# Check if all combined years exist\n",
    "all_combined_exist = all(\n",
    "    get_combined_path(combined_year).exists() for combined_year in YEARS_COMBINED_DICT.keys()\n",
    ")\n",
    "\n",
    "if not REPROCESS and all_combined_exist:\n",
    "    # Load all combined years directly\n",
    "    print(\"Loading all combined years...\")\n",
    "    events_combined = {}\n",
    "    for combined_year in YEARS_COMBINED_DICT.keys():\n",
    "        combined_path = get_combined_path(combined_year)\n",
    "        print(f\"Loading combined year {combined_year}...\")\n",
    "        with combined_path.open(\"rb\") as f:\n",
    "            events_combined[combined_year] = pd.read_pickle(f)\n",
    "    print(\"All combined years loaded!\")\n",
    "\n",
    "else:\n",
    "    # ============================================================================\n",
    "    # STEP 1: Process each era individually\n",
    "    # ============================================================================\n",
    "\n",
    "    for year in YEARS:\n",
    "        era_path = get_era_path(year)\n",
    "\n",
    "        if REPROCESS or not era_path.exists():\n",
    "            print(f\"Processing era: {year}\")\n",
    "            events_era = {}\n",
    "\n",
    "            # Process each sample for this era\n",
    "            for sample, sample_list in SAMPLES_DICT.items():\n",
    "                print(f\"Loading {sample} for {year}...\")\n",
    "                triggers_cols = [(trigger, 1) for trigger in triggers[year]]\n",
    "\n",
    "                columns = triggers_cols + base_columns + extra_columns_dict.get(sample, [])\n",
    "                dataframes = {\n",
    "                    **utils.load_samples(\n",
    "                        data_dir=str(DATA_DIR),\n",
    "                        samples={sample: sample_list},\n",
    "                        year=year,\n",
    "                        columns=utils.format_columns(columns),\n",
    "                        variations=True,\n",
    "                        weight_shifts=weight_shifts,\n",
    "                        lumi_dict=actual_lumis,\n",
    "                    )\n",
    "                }\n",
    "\n",
    "                # Process and concatenate dataframes for this sample\n",
    "                sample_dfs = []\n",
    "                for key, df in dataframes.items():\n",
    "                    # Handle pT variations\n",
    "                    for pt_var in [\"bbFatJetPt\"] + pt_variations:\n",
    "                        if pt_var not in df.columns:\n",
    "                            for i in range(2):\n",
    "                                df[f\"{pt_var}{i}\"] = df[(\"bbFatJetPt\", i)].copy()\n",
    "\n",
    "                    # Handle mass variations\n",
    "                    for mass_var in mass_vars + mass_variations:\n",
    "                        if mass_var not in df.columns:\n",
    "                            for i in range(2):\n",
    "                                df[f\"{mass_var}{i}\"] = df[(mass_var.split(\"_\")[0], i)].copy()\n",
    "\n",
    "                    if sample != \"data\":\n",
    "                        # Evaluate trigger scale factors\n",
    "                        sf, sf_up, sf_down = eval_trigger_sf(\n",
    "                            txbb=df[(\"bbFatJetParT2TXbb\", 0)].values,\n",
    "                            pt=df[(\"bbFatJetPt\", 0)].values,\n",
    "                            msd=df[(\"bbFatJetMsd\", 0)].values,\n",
    "                            year=year,\n",
    "                        )\n",
    "                        df[\"SF_trigger\"] = sf\n",
    "                        df[\"SF_trigger_up\"] = sf_up\n",
    "                        df[\"SF_trigger_down\"] = sf_down\n",
    "\n",
    "                    sample_dfs.append(df)\n",
    "\n",
    "                # Concatenate all dataframes for this sample\n",
    "                events_era[sample] = pd.concat(sample_dfs, ignore_index=True)\n",
    "                print(f\"  {sample}: {len(events_era[sample])} events\")\n",
    "\n",
    "                # Clear intermediate dataframes to free memory\n",
    "                del dataframes, sample_dfs\n",
    "\n",
    "            # Save this era's data\n",
    "            with era_path.open(\"wb\") as f:\n",
    "                pd.to_pickle(events_era, f)\n",
    "            print(f\"Era {year} saved to {era_path}\")\n",
    "\n",
    "            # Clear era data to free memory\n",
    "            del events_era\n",
    "        else:\n",
    "            print(f\"Era {year} already processed at {era_path}\")\n",
    "\n",
    "    print(\"Individual era processing complete!\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # STEP 2: Combine eras into combined years\n",
    "    # ============================================================================\n",
    "\n",
    "    events_combined = {}\n",
    "\n",
    "    for combined_year, year_list in YEARS_COMBINED_DICT.items():\n",
    "        combined_path = get_combined_path(combined_year)\n",
    "\n",
    "        if REPROCESS or not combined_path.exists():\n",
    "            print(f\"\\nCombining eras for {combined_year}: {year_list}\")\n",
    "\n",
    "            # Load each era\n",
    "            era_data = {}\n",
    "            for year in year_list:\n",
    "                era_path = get_era_path(year)\n",
    "                if era_path.exists():\n",
    "                    print(f\"Loading era {year}...\")\n",
    "                    with era_path.open(\"rb\") as f:\n",
    "                        era_data[year] = pd.read_pickle(f)\n",
    "                else:\n",
    "                    print(f\"Warning: Era file {era_path} not found!\")\n",
    "\n",
    "            # Combine samples across eras\n",
    "            events_combined[combined_year] = {}\n",
    "            for sample in SAMPLES_DICT.keys():\n",
    "                sample_dfs = []\n",
    "                for year in year_list:\n",
    "                    if year in era_data and sample in era_data[year]:\n",
    "                        sample_dfs.append(era_data[year][sample])\n",
    "\n",
    "                if sample_dfs:\n",
    "                    events_combined[combined_year][sample] = pd.concat(\n",
    "                        sample_dfs, ignore_index=True\n",
    "                    )\n",
    "                    print(f\"  {sample}: {len(events_combined[combined_year][sample])} events\")\n",
    "\n",
    "            # Save combined year\n",
    "            with combined_path.open(\"wb\") as f:\n",
    "                pd.to_pickle(events_combined[combined_year], f)\n",
    "            print(f\"Combined year {combined_year} saved to {combined_path}\")\n",
    "\n",
    "            # Clear era data to free memory for next iteration\n",
    "            del era_data\n",
    "        else:\n",
    "            # If combined year already exists, load it\n",
    "            print(f\"Combined year {combined_year} already exists, loading...\")\n",
    "            with combined_path.open(\"rb\") as f:\n",
    "                events_combined[combined_year] = pd.read_pickle(f)\n",
    "\n",
    "    print(\"\\nAll processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07954dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply ZQQ corrections if needed\n",
    "if APPLY_Z_RECOIL_CORR:\n",
    "    print(\"Applying Zto2Q corrections...\")\n",
    "    for year in YEARS_COMBINED_DICT.keys():\n",
    "        # apply corrections to the events\n",
    "        corr = corr_dict[year.replace(\"All\", \"\")][\"GenZPtWeight\"]\n",
    "        GenZ_pt = events_combined[year][\"Zto2Q\"][\"GenZPt\"].values[:, 0]\n",
    "        sf_nom = corr.evaluate(GenZ_pt, \"nominal\")\n",
    "        sf_up = corr.evaluate(GenZ_pt, \"stat_up\")\n",
    "        sf_down = corr.evaluate(GenZ_pt, \"stat_dn\")\n",
    "        events_combined[year][\"Zto2Q\"][\"SF_GenZPt\"] = sf_nom\n",
    "        events_combined[year][\"Zto2Q\"][\"SF_GenZPt_up\"] = sf_up\n",
    "        events_combined[year][\"Zto2Q\"][\"SF_GenZPt_down\"] = sf_down\n",
    "\n",
    "        # apply the scale factors to the final weight\n",
    "        weight = events_combined[year][\"Zto2Q\"][\"finalWeight\"]\n",
    "        events_combined[year][\"Zto2Q\"][\"finalWeight\"] = weight * sf_nom\n",
    "        events_combined[year][\"Zto2Q\"][\"weight_GenZPtUp\"] = weight * sf_up\n",
    "        events_combined[year][\"Zto2Q\"][\"weight_GenZPtDown\"] = weight * sf_down\n",
    "\n",
    "        # Applied nominal SF to the other systematic variations\n",
    "        for sys_var, up_down in itertools.product(sys_vars, [\"Up\", \"Down\"]):\n",
    "            weight_name = f\"weight_{sys_var}{up_down}\"\n",
    "            weight = events_combined[year][\"Zto2Q\"][weight_name].values[:, 0]\n",
    "            events_combined[year][\"Zto2Q\"][weight_name] = weight * sf_nom\n",
    "\n",
    "    print(\"Zto2Q corrections applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0274383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further split Zto2Q and Wto2Q events into different categories\n",
    "for year in YEARS_COMBINED_DICT.keys():\n",
    "    Zto2Q = events_combined[year][\"Zto2Q\"]\n",
    "    matched = Zto2Q[(\"bbFatJetVQQMatch\", 0)] == 1\n",
    "    is_ZBB = Zto2Q[(\"GenZBB\", 0)]\n",
    "    is_ZCC = Zto2Q[(\"GenZCC\", 0)]\n",
    "    is_ZQQ = ~(is_ZBB | is_ZCC)  # u, d, s quarks\n",
    "    ZtoBB = is_ZBB & matched\n",
    "    ZtoCC = is_ZCC & matched\n",
    "    ZtoQQ = is_ZQQ & matched\n",
    "    Z_unmatched = ~matched\n",
    "    events_combined[year][\"Zto2Q_BB\"] = Zto2Q[ZtoBB]\n",
    "    events_combined[year][\"Zto2Q_CC\"] = Zto2Q[ZtoCC]\n",
    "    events_combined[year][\"Zto2Q_QQ\"] = Zto2Q[ZtoQQ]\n",
    "    # leave the unmatched to bkg fits\n",
    "    # events_combined[year][\"Zto2Q_unmatched\"] = Zto2Q[Z_unmatched]\n",
    "\n",
    "# MC_SAMPLES_FINAL_LIST = MC_SAMPLES_LIST + [\"Zto2Q_BB\", \"Zto2Q_CC\", \"Zto2Q_QQ\", \"Zto2Q_unmatched\"]\n",
    "MC_SAMPLES_FINAL_LIST = MC_SAMPLES_LIST + [\"Zto2Q_BB\", \"Zto2Q_CC\", \"Zto2Q_QQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass and fail regions\n",
    "txbb_bins = [0.94, 0.97, 0.98, 0.99, 1.0]\n",
    "min_txbb = txbb_bins[0]\n",
    "# pT bins\n",
    "pt_bins = [300, 450, 550, 10000]\n",
    "\n",
    "txbb_bins = list(zip(txbb_bins[:-1], txbb_bins[1:]))\n",
    "pt_bins = list(zip(pt_bins[:-1], pt_bins[1:]))\n",
    "\n",
    "# Mass bins\n",
    "m_low, m_high = 50, 150\n",
    "bins = 5\n",
    "n_mass_bins = int((m_high - m_low) / bins)\n",
    "\n",
    "\n",
    "def save_to_root(outfile: Path, templates: dict):\n",
    "    with uproot.recreate(str(outfile)) as f_out:\n",
    "        for category in templates.keys():\n",
    "            hist = templates[category]\n",
    "            categories, _ = hist.axes\n",
    "            for sample in list(categories):\n",
    "                h = templates[category][{\"Sample\": sample}]\n",
    "                f_out[f\"{sample}_{category}\"] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_triggers(events, trigger_list):\n",
    "    print(f\"Selecting events with triggers: {trigger_list}\")\n",
    "    events_filtered = {}\n",
    "\n",
    "    for year in events.keys():\n",
    "        events_filtered[year] = {}\n",
    "        for sample in events[year].keys():\n",
    "            df = events[year][sample].copy()\n",
    "            mask = np.zeros(len(df), dtype=bool)\n",
    "            for trigger in trigger_list:\n",
    "                if trigger in df.columns:\n",
    "                    mask = mask | (df[trigger].values.reshape(-1) == 1)\n",
    "            events_filtered[year][sample] = df[mask].copy()\n",
    "            num_sel = mask.sum()\n",
    "            num_total = len(df)\n",
    "            print(\n",
    "                f\"Year: {year}, Sample: {sample}, Selected: {num_sel}, Total: {num_total}, Efficiency: {num_sel / num_total:.2%}\"\n",
    "            )\n",
    "    return events_filtered\n",
    "\n",
    "\n",
    "trigger_list_high_pt = [\n",
    "    \"AK8PFJet500\",\n",
    "    \"AK8PFJet420_MassSD30\",\n",
    "    \"AK8PFJet425_SoftDropMass40\",\n",
    "]\n",
    "\n",
    "trigger_list_PNet = [\n",
    "    \"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "    \"AK8PFJet230_SoftDropMass40_PNetBB0p06\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2fc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_high_pt = select_triggers(events_combined, trigger_list_high_pt)\n",
    "events_PNet = select_triggers(events_combined, trigger_list_PNet)\n",
    "del events_combined  # free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make msd > 40 cut for PNet events\n",
    "# for year, events in events_PNet.items():\n",
    "#     for sample, df in events.items():\n",
    "#         df = df[df[(\"bbFatJetMsd\", 0)] > 40]\n",
    "#         events_PNet[year][sample] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42369211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply trigger sf to events_PNet\n",
    "if APPLY_TRIGGER_SF:\n",
    "    print(\"Applying trigger scale factors to events_PNet...\")\n",
    "    for year in events_PNet.keys():\n",
    "        for sample in events_PNet[year].keys():\n",
    "            df = events_PNet[year][sample]\n",
    "            if sample != \"data\":\n",
    "                sf_nom = df[\"SF_trigger\"]\n",
    "                sf_up = df[\"SF_trigger_up\"]\n",
    "                sf_down = df[\"SF_trigger_down\"]\n",
    "\n",
    "                # apply the scale factors to the final weight\n",
    "                weight = df[\"finalWeight\"]\n",
    "                df[\"finalWeight\"] = weight * sf_nom\n",
    "                df[\"weight_TriggerUp\"] = weight * sf_up\n",
    "                df[\"weight_TriggerDown\"] = weight * sf_down\n",
    "\n",
    "                # apply the nominal SF to other weights\n",
    "                for sys_var, up_down in itertools.product(sys_vars, [\"Up\", \"Down\"]):\n",
    "                    weight_name = f\"weight_{sys_var}{up_down}\"\n",
    "\n",
    "                    if sys_var == \"GenZPt\" and weight_name not in df.columns:\n",
    "                        # skip for samples without this weight\n",
    "                        continue\n",
    "\n",
    "                    weight = df[weight_name].values[:, 0]\n",
    "                    df[weight_name] = weight * sf_nom\n",
    "\n",
    "            events_PNet[year][sample] = df\n",
    "\n",
    "    # do the same for the high pT events, but sf=1\n",
    "    print(\"Applying trigger scale factors to events_high_pt with 1s\")\n",
    "    for year in events_high_pt.keys():\n",
    "        for sample in events_high_pt[year].keys():\n",
    "            df = events_high_pt[year][sample]\n",
    "            if sample != \"data\":\n",
    "                sf_nom = 1\n",
    "\n",
    "                weight = df[\"finalWeight\"]\n",
    "                df[\"finalWeight\"] = df[\"finalWeight\"] * sf_nom\n",
    "                df[\"weight_TriggerUp\"] = 1\n",
    "                df[\"weight_TriggerDown\"] = 1\n",
    "\n",
    "                # apply the nominal SF to other weights\n",
    "                for sys_var, up_down in itertools.product(sys_vars, [\"Up\", \"Down\"]):\n",
    "                    weight_name = f\"weight_{sys_var}{up_down}\"\n",
    "\n",
    "                    if sys_var == \"GenZPt\" and weight_name not in df.columns:\n",
    "                        # skip for samples without this weight\n",
    "                        continue\n",
    "\n",
    "                    weight = df[weight_name].values[:, 0]\n",
    "                    df[weight_name] = weight * sf_nom\n",
    "\n",
    "            events_high_pt[year][sample] = df\n",
    "else:\n",
    "    print(\"Trigger scale factors are not applied to events_PNet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bkg_keys = [\"Zto2Q_CC\", \"Zto2Q_QQ\", \"Zto2Q_unmatched\", \"Wto2Q\", \"hbb\", \"ttbar\", \"qcd\"]\n",
    "# sig_keys = [\"Zto2Q_BB\"]\n",
    "# use this if you want to include Zto2Q_BB in the stack plot\n",
    "# bkg_keys = [\"Zto2Q_BB\", \"Zto2Q_CC\", \"Zto2Q_QQ\", \"Zto2Q_unmatched\", \"Wto2Q\", \"hbb\", \"ttbar\", \"qcd\"]\n",
    "# bkg_keys = [\"Zto2Q_BB\", \"Zto2Q_CC\", \"Zto2Q_QQ\", \"Zto2Q_unmatched\", \"Wto2Q\", \"ttbar\", \"qcd\"]\n",
    "# bkg_keys = [\"Zto2Q_BB\", \"Zto2Q_CC\", \"Zto2Q_QQ\", \"Wto2Q\", \"ttbar\", \"qcd\"]\n",
    "bkg_keys = [\"Zto2Q_BB\", \"Zto2Q_CC\", \"Zto2Q_QQ\", \"Wto2Q\", \"ttbar\"]\n",
    "sig_keys = []\n",
    "bg_order = list(reversed(bkg_keys))\n",
    "\n",
    "jshift_keys = [\"\"]\n",
    "for var, ud in itertools.product([\"JES\", \"JER\", \"JMS\", \"JMR\"], [\"up\", \"down\"]):\n",
    "    jshift_keys.append(f\"{var}_{ud}\")\n",
    "\n",
    "weight_shifts = {\n",
    "    \"pileup\": postprocessing.Syst(\n",
    "        samples=MC_SAMPLES_FINAL_LIST, label=\"Pileup\", years=list(YEARS_COMBINED_DICT.keys())\n",
    "    ),\n",
    "    # \"pdf\": postprocessing.Syst(samples=sig_keys, label=\"PDFAcc\", years=list(YEARS_COMBINED_DICT.keys())),\n",
    "    \"ISRPartonShower\": postprocessing.Syst(\n",
    "        samples=MC_SAMPLES_FINAL_LIST,\n",
    "        label=\"ISR Parton Shower\",\n",
    "        years=list(YEARS_COMBINED_DICT.keys()),\n",
    "    ),\n",
    "    \"FSRPartonShower\": postprocessing.Syst(\n",
    "        samples=MC_SAMPLES_FINAL_LIST,\n",
    "        label=\"FSR Parton Shower\",\n",
    "        years=list(YEARS_COMBINED_DICT.keys()),\n",
    "    ),\n",
    "    # \"pdf\": postprocessing.Syst(\n",
    "    #     samples=MC_SAMPLES_FINAL_LIST,\n",
    "    #     label=\"PDF weights\",\n",
    "    #     years=list(YEARS_COMBINED_DICT.keys()),\n",
    "    # ),\n",
    "    # \"scale\": postprocessing.Syst(\n",
    "    #     samples=MC_SAMPLES_FINAL_LIST,\n",
    "    #     label=\"Scale weights\",\n",
    "    #     years=list(YEARS_COMBINED_DICT.keys()),\n",
    "    # ),\n",
    "}\n",
    "\n",
    "if APPLY_Z_RECOIL_CORR:\n",
    "    weight_shifts[\"GenZPt\"] = postprocessing.Syst(\n",
    "        samples=[\"Zto2Q_BB\", \"Zto2Q_CC\", \"Zto2Q_QQ\"],\n",
    "        label=\"Gen Z pT correction derived from ZMuMu\",\n",
    "        years=list(YEARS_COMBINED_DICT.keys()),\n",
    "    )\n",
    "\n",
    "if APPLY_TRIGGER_SF:\n",
    "    weight_shifts[\"Trigger\"] = postprocessing.Syst(\n",
    "        samples=MC_SAMPLES_FINAL_LIST,\n",
    "        label=\"Trigger SF of the PNet trigger\",\n",
    "        years=list(YEARS_COMBINED_DICT.keys()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOPART_VERSION: int = 2\n",
    "# GLOPART_VERSION: int = 3\n",
    "\n",
    "if GLOPART_VERSION == 2:\n",
    "    tagger_branch = \"bbFatJetParT2TXbb\"\n",
    "elif GLOPART_VERSION == 3:\n",
    "    tagger_branch = \"bbFatJetParT3TXbb\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid GLOPART_VERSION: {GLOPART_VERSION}. Must be 2 or 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a93376",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in YEARS_COMBINED_DICT:\n",
    "    out_dir = Path(f\"templates_zbb_GloParTv{GLOPART_VERSION}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    template_dir = out_dir\n",
    "    template_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pt_low, pt_high in pt_bins:\n",
    "        pt_low_str = str(pt_low)\n",
    "        pt_high_str = str(pt_high)\n",
    "        pt_bin_key = f\"pT{pt_low_str}to{pt_high_str}\"\n",
    "\n",
    "        cutflows_dir = Path(f\"{out_dir}/cutflows/{year}\")\n",
    "        cutflows_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        plot_dir = Path(f\"{out_dir}/plots/{year}/{pt_bin_key}\")\n",
    "        plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        templates = {}\n",
    "\n",
    "        if pt_low < 450:\n",
    "            events = events_PNet[year]\n",
    "        else:\n",
    "            events = events_high_pt[year]\n",
    "\n",
    "        # Determine the pt and mass variations\n",
    "        for jshift in jshift_keys:\n",
    "            if jshift == \"\":\n",
    "                pt_branch = \"bbFatJetPt0\"\n",
    "                if GLOPART_VERSION == 2:\n",
    "                    mass_branch = \"bbFatJetParT2massVis0\"\n",
    "                elif GLOPART_VERSION == 3:\n",
    "                    # mass_branch = \"bbFatJetParT3massX2p0\"\n",
    "                    mass_branch = \"bbFatJetParT3massGeneric0\"\n",
    "            elif jshift.startswith(\"JES\") or jshift.startswith(\"JER\"):\n",
    "                pt_branch = f\"bbFatJetPt_{jshift}0\"\n",
    "                # mass_branch = \"bbFatJetParTmassVis0\"\n",
    "                if GLOPART_VERSION == 2:\n",
    "                    mass_branch = \"bbFatJetParT2massVis0\"\n",
    "                elif GLOPART_VERSION == 3:\n",
    "                    # mass_branch = \"bbFatJetParT3massX2p0\"\n",
    "                    mass_branch = \"bbFatJetParT3massGeneric0\"\n",
    "            elif jshift.startswith(\"JMS\") or jshift.startswith(\"JMR\"):\n",
    "                pt_branch = \"bbFatJetPt0\"\n",
    "                if GLOPART_VERSION == 2:\n",
    "                    mass_branch = f\"bbFatJetParT2massVis_{jshift}0\"\n",
    "                elif GLOPART_VERSION == 3:\n",
    "                    # mass_branch = f\"bbFatJetParT3massX2p_{jshift}0\"\n",
    "                    mass_branch = f\"bbFatJetParT3massGeneric_{jshift}0\"\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown jshift: {jshift}\")\n",
    "\n",
    "            # Different different pass regions based on TXbb and pT bins\n",
    "            selection_regions = {}\n",
    "            for txbb_low, txbb_high in txbb_bins:\n",
    "                # Convert to strings\n",
    "                txbb_low_str = str(txbb_low).replace(\".\", \"p\")\n",
    "                txbb_high_str = str(txbb_high).replace(\".\", \"p\")\n",
    "                region_key = f\"pass_TXbb{txbb_low_str}to{txbb_high_str}_{pt_bin_key}\"\n",
    "\n",
    "                cutflows = {}\n",
    "                for sample in events:\n",
    "                    cutflows[sample] = OrderedDict()\n",
    "                    cutflows[sample][\"Skimmer Preselection\"] = events[sample][\"finalWeight\"].sum()\n",
    "                    cutflows[sample][\"HLT\"] = events[sample][\"finalWeight\"].sum()\n",
    "                cutflows = pd.DataFrame.from_dict(cutflows).transpose()\n",
    "\n",
    "                # Create a region\n",
    "                selection_regions[region_key] = postprocessing.Region(\n",
    "                    cuts={\n",
    "                        pt_branch: [pt_low, pt_high],\n",
    "                        mass_branch: [m_low, m_high],\n",
    "                        f\"{tagger_branch}0\": [txbb_low, txbb_high],\n",
    "                    },\n",
    "                    label=region_key,\n",
    "                )\n",
    "\n",
    "            selection_regions[\"fail\"] = postprocessing.Region(\n",
    "                cuts={\n",
    "                    pt_branch: [pt_low, pt_high],\n",
    "                    mass_branch: [m_low, m_high],\n",
    "                    f\"{tagger_branch}0\": [0, min(0.9, min_txbb)],\n",
    "                },\n",
    "                label=\"fail\",\n",
    "            )\n",
    "            print(f\"Selection regions for {year} with jshift {jshift}: {selection_regions.keys()}\")\n",
    "\n",
    "            fit_shape_var = postprocessing.ShapeVar(\n",
    "                mass_branch,\n",
    "                r\"$m_\\mathrm{reg}$ (GeV)\",\n",
    "                [n_mass_bins, m_low, m_high],\n",
    "                reg=True,\n",
    "            )\n",
    "\n",
    "            ttemps = postprocessing.get_templates(\n",
    "                events,\n",
    "                year=year,\n",
    "                sig_keys=sig_keys,\n",
    "                plot_sig_keys=sig_keys,\n",
    "                selection_regions=selection_regions,\n",
    "                shape_vars=[fit_shape_var],\n",
    "                systematics={},\n",
    "                template_dir=out_dir,\n",
    "                bg_keys=bkg_keys,\n",
    "                bg_order=bg_order,\n",
    "                bg_err_mcstat=False,\n",
    "                plot_dir=plot_dir,\n",
    "                prev_cutflow=cutflows,\n",
    "                weight_key=\"finalWeight\",\n",
    "                weight_shifts=weight_shifts,\n",
    "                plot_shifts=False,\n",
    "                show=False,\n",
    "                energy=13.6,\n",
    "                jshift=jshift,\n",
    "                blind=False,\n",
    "            )\n",
    "            templates = {**templates, **ttemps}\n",
    "\n",
    "        # Save the templates to a file\n",
    "        outfile = template_dir / f\"templates_{year}_{pt_bin_key}.root\"\n",
    "        save_to_root(outfile, templates)\n",
    "        # Save as a pickle file\n",
    "        outfile_pickle = template_dir / f\"templates_{year}_{pt_bin_key}.pkl\"\n",
    "        with outfile_pickle.open(\"wb\") as f:\n",
    "            pd.to_pickle(templates, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf1417",
   "metadata": {},
   "source": [
    "# Softdrop Mass Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in YEARS_COMBINED_DICT:\n",
    "    out_dir = Path(f\"templates_zbb_GloParTv{GLOPART_VERSION}_mSD\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    template_dir = out_dir\n",
    "    template_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pt_low, pt_high in pt_bins:\n",
    "        pt_low_str = str(pt_low)\n",
    "        pt_high_str = str(pt_high)\n",
    "        pt_bin_key = f\"pT{pt_low_str}to{pt_high_str}\"\n",
    "\n",
    "        cutflows_dir = Path(f\"{out_dir}/cutflows/{year}\")\n",
    "        cutflows_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        plot_dir = Path(f\"{out_dir}/plots/{year}/{pt_bin_key}\")\n",
    "        plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        templates = {}\n",
    "\n",
    "        if pt_low < 450:\n",
    "            events = events_PNet[year]\n",
    "        else:\n",
    "            events = events_high_pt[year]\n",
    "\n",
    "        # Determine the pt and mass variations\n",
    "        for jshift in jshift_keys:\n",
    "            if jshift == \"\":\n",
    "                pt_branch = \"bbFatJetPt0\"\n",
    "                if GLOPART_VERSION == 2:\n",
    "                    mass_branch = \"bbFatJetMsd0\"\n",
    "                elif GLOPART_VERSION == 3:\n",
    "                    mass_branch = \"bbFatJetMsd0\"\n",
    "            elif jshift.startswith(\"JES\") or jshift.startswith(\"JER\"):\n",
    "                pt_branch = f\"bbFatJetPt_{jshift}0\"\n",
    "                if GLOPART_VERSION == 2:\n",
    "                    mass_branch = \"bbFatJetMsd0\"\n",
    "                elif GLOPART_VERSION == 3:\n",
    "                    mass_branch = \"bbFatJetMsd0\"\n",
    "            elif jshift.startswith(\"JMS\") or jshift.startswith(\"JMR\"):\n",
    "                pt_branch = \"bbFatJetPt0\"\n",
    "                if GLOPART_VERSION == 2:\n",
    "                    mass_branch = f\"bbFatJetMsd_{jshift}0\"\n",
    "                elif GLOPART_VERSION == 3:\n",
    "                    mass_branch = f\"bbFatJetMsd_{jshift}0\"\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown jshift: {jshift}\")\n",
    "\n",
    "            # Different different pass regions based on TXbb and pT bins\n",
    "            selection_regions = {}\n",
    "            for txbb_low, txbb_high in txbb_bins:\n",
    "                # Convert to strings\n",
    "                txbb_low_str = str(txbb_low).replace(\".\", \"p\")\n",
    "                txbb_high_str = str(txbb_high).replace(\".\", \"p\")\n",
    "                region_key = f\"pass_TXbb{txbb_low_str}to{txbb_high_str}_{pt_bin_key}\"\n",
    "\n",
    "                cutflows = {}\n",
    "                for sample in events:\n",
    "                    cutflows[sample] = OrderedDict()\n",
    "                    cutflows[sample][\"Skimmer Preselection\"] = events[sample][\"finalWeight\"].sum()\n",
    "                    cutflows[sample][\"HLT\"] = events[sample][\"finalWeight\"].sum()\n",
    "                cutflows = pd.DataFrame.from_dict(cutflows).transpose()\n",
    "\n",
    "                # Create a region\n",
    "                selection_regions[region_key] = postprocessing.Region(\n",
    "                    cuts={\n",
    "                        pt_branch: [pt_low, pt_high],\n",
    "                        mass_branch: [m_low, m_high],\n",
    "                        f\"{tagger_branch}0\": [txbb_low, txbb_high],\n",
    "                    },\n",
    "                    label=region_key,\n",
    "                )\n",
    "\n",
    "            selection_regions[\"fail\"] = postprocessing.Region(\n",
    "                cuts={\n",
    "                    pt_branch: [pt_low, pt_high],\n",
    "                    mass_branch: [m_low, m_high],\n",
    "                    f\"{tagger_branch}0\": [0, min(0.9, min_txbb)],\n",
    "                },\n",
    "                label=\"fail\",\n",
    "            )\n",
    "            print(f\"Selection regions for {year} with jshift {jshift}: {selection_regions.keys()}\")\n",
    "\n",
    "            fit_shape_var = postprocessing.ShapeVar(\n",
    "                mass_branch,\n",
    "                r\"$m_\\mathrm{reg}$ (GeV)\",\n",
    "                [n_mass_bins, m_low, m_high],\n",
    "                reg=True,\n",
    "            )\n",
    "\n",
    "            ttemps = postprocessing.get_templates(\n",
    "                events,\n",
    "                year=year,\n",
    "                sig_keys=sig_keys,\n",
    "                plot_sig_keys=sig_keys,\n",
    "                selection_regions=selection_regions,\n",
    "                shape_vars=[fit_shape_var],\n",
    "                systematics={},\n",
    "                template_dir=out_dir,\n",
    "                bg_keys=bkg_keys,\n",
    "                bg_order=bg_order,\n",
    "                bg_err_mcstat=False,\n",
    "                plot_dir=plot_dir,\n",
    "                prev_cutflow=cutflows,\n",
    "                weight_key=\"finalWeight\",\n",
    "                weight_shifts=weight_shifts,\n",
    "                plot_shifts=False,\n",
    "                show=False,\n",
    "                energy=13.6,\n",
    "                jshift=jshift,\n",
    "                blind=False,\n",
    "            )\n",
    "            templates = {**templates, **ttemps}\n",
    "\n",
    "        # Save the templates to a file\n",
    "        outfile = template_dir / f\"templates_{year}_{pt_bin_key}.root\"\n",
    "        save_to_root(outfile, templates)\n",
    "        # Save as a pickle file\n",
    "        outfile_pickle = template_dir / f\"templates_{year}_{pt_bin_key}.pkl\"\n",
    "        with outfile_pickle.open(\"wb\") as f:\n",
    "            pd.to_pickle(templates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5e6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh4b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
