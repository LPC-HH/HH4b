{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d00994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reloads imported files on edits\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from HH4b import utils\n",
    "from HH4b.hh_vars import LUMI\n",
    "import correctionlib\n",
    "import mplhep as hep\n",
    "\n",
    "hep.style.use(hep.style.CMS)\n",
    "\n",
    "from constants import MASS_RANGE, PT_RANGE, PT_BINS, MASS_BINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = [\"2022\", \"2022EE\", \"2023\", \"2023BPix\"]\n",
    "YEARS_COMBINED_DICT = {\n",
    "    \"2022All\": [\"2022\", \"2022EE\"],\n",
    "    \"2023All\": [\"2023\", \"2023BPix\"],\n",
    "}\n",
    "PROCESSED_PATH: Path = Path(\"processed/corr_DYLL_data.pkl\")\n",
    "(PROCESSED_PATH.parent).mkdir(parents=True, exist_ok=True)\n",
    "REPROCESS: bool = True  # if True, reprocess from the skimmed ntuples\n",
    "\n",
    "SAMPLES_DICT = {\n",
    "    \"data\": [f\"{key}_Run\" for key in [\"Muon\"]],\n",
    "    \"ttbar\": [\"TTto4Q\", \"TTtoLNu2Q\", \"TTto2L2Nu\"],\n",
    "    \"DYto2L\": [\"DYto2L\"],\n",
    "    \"VV\": [\"WW\", \"WZ\", \"ZZ\"],\n",
    "}\n",
    "\n",
    "PLOT_DIR = Path(\"plots/corr_DYLL_data\")\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_PATH = Path(\"corrs/DYLL_data.pkl\")\n",
    "(OUTPUT_PATH.parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "P4 = (\"Mass\", \"Pt\", \"Eta\", \"Phi\")\n",
    "columns = [(\"weight\", 1)]\n",
    "for i in range(2):\n",
    "    for branch in P4:\n",
    "        columns.append((f\"Muon{i+1}{branch}\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e926ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_POG_MUO_path(year: str, filename: str = \"muon_Z.json.gz\") -> Path:\n",
    "    corr_dir = Path(\"POG_MUO\")\n",
    "    if year == \"2022\":\n",
    "        subdir = \"2022_Summer22\"\n",
    "    elif year == \"2022EE\":\n",
    "        subdir = \"2022_Summer22EE\"\n",
    "    elif year == \"2023\":\n",
    "        subdir = \"2023_Summer23\"\n",
    "    elif year == \"2023BPix\":\n",
    "        subdir = \"2023_Summer23BPix\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown year: {year}\")\n",
    "    return corr_dir / subdir / filename\n",
    "\n",
    "\n",
    "CORR_DICT = {}\n",
    "\n",
    "for year in YEARS:\n",
    "    corr_path = get_POG_MUO_path(year)\n",
    "    if not corr_path.exists():\n",
    "        raise FileNotFoundError(f\"Correction file for {year} not found at {corr_path}\")\n",
    "\n",
    "    # Load the correction file\n",
    "    corr = correctionlib.CorrectionSet.from_file(str(corr_path))\n",
    "    CORR_DICT[year] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7294058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimuon_PtEtaPhiMass(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Calculate the dimuon mass from the muon 4-vectors.\"\"\"\n",
    "    muon1_pt = df[(\"Muon1Pt\", 0)]\n",
    "    muon1_eta = df[(\"Muon1Eta\", 0)]\n",
    "    muon1_phi = df[(\"Muon1Phi\", 0)]\n",
    "    muon1_mass = df[(\"Muon1Mass\", 0)]\n",
    "    muon1_px = muon1_pt * np.cos(muon1_phi)\n",
    "    muon1_py = muon1_pt * np.sin(muon1_phi)\n",
    "    muon1_pz = muon1_pt * np.sinh(muon1_eta)\n",
    "    muon1_E = np.sqrt(muon1_px**2 + muon1_py**2 + muon1_pz**2 + muon1_mass**2)\n",
    "\n",
    "    muon2_pt = df[(\"Muon2Pt\", 0)]\n",
    "    muon2_eta = df[(\"Muon2Eta\", 0)]\n",
    "    muon2_phi = df[(\"Muon2Phi\", 0)]\n",
    "    muon2_mass = df[(\"Muon2Mass\", 0)]\n",
    "    muon2_px = muon2_pt * np.cos(muon2_phi)\n",
    "    muon2_py = muon2_pt * np.sin(muon2_phi)\n",
    "    muon2_pz = muon2_pt * np.sinh(muon2_eta)\n",
    "    muon2_E = np.sqrt(muon2_px**2 + muon2_py**2 + muon2_pz**2 + muon2_mass**2)\n",
    "\n",
    "    dimuon_px = muon1_px + muon2_px\n",
    "    dimuon_py = muon1_py + muon2_py\n",
    "    dimuon_pz = muon1_pz + muon2_pz\n",
    "    dimuon_E = muon1_E + muon2_E\n",
    "\n",
    "    dimuon_pt = np.sqrt(dimuon_px**2 + dimuon_py**2)\n",
    "    dimuon_eta = np.arcsinh(dimuon_pz / np.sqrt(dimuon_px**2 + dimuon_py**2))\n",
    "    dimuon_phi = np.arctan2(dimuon_py, dimuon_px)\n",
    "    dimuon_mass = np.sqrt(dimuon_E**2 - (dimuon_px**2 + dimuon_py**2 + dimuon_pz**2))\n",
    "\n",
    "    return {\n",
    "        \"DimuonPt\": dimuon_pt,\n",
    "        \"DimuonEta\": dimuon_eta,\n",
    "        \"DimuonPhi\": dimuon_phi,\n",
    "        \"DimuonMass\": dimuon_mass,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_POG_MUO_corr(df: pd.DataFrame, corr: correctionlib.CorrectionSet) -> pd.DataFrame:\n",
    "    corr = CORR_DICT[year]\n",
    "\n",
    "    eta1 = df[(\"Muon1Eta\", 0)]\n",
    "    pt1 = df[(\"Muon1Pt\", 0)]\n",
    "    eta2 = df[(\"Muon2Eta\", 0)]\n",
    "    pt2 = df[(\"Muon2Pt\", 0)]\n",
    "\n",
    "    sf_nominal = 1.0\n",
    "    rel_errs_up_sq = 0.0\n",
    "    rel_errs_down_sq = 0.0\n",
    "\n",
    "    # HLT correction\n",
    "    HLT_key = (\n",
    "        \"NUM_Mu50_or_CascadeMu100_or_HighPtTkMu100_DEN_CutBasedIdGlobalhighPtId_and_TkIsoLoose\"\n",
    "    )\n",
    "    hlt_nom = corr[HLT_key].evaluate(eta1, pt1, \"nominal\")\n",
    "    hlt_up = corr[HLT_key].evaluate(eta1, pt1, \"systup\")\n",
    "    hlt_down = corr[HLT_key].evaluate(eta1, pt1, \"systdown\")\n",
    "\n",
    "    sf_nominal *= hlt_nom\n",
    "    rel_errs_up_sq += ((hlt_up - hlt_nom) / hlt_nom) ** 2\n",
    "    rel_errs_down_sq += ((hlt_down - hlt_nom) / hlt_nom) ** 2\n",
    "\n",
    "    for pt, eta in zip([pt1, pt2], [eta1, eta2]):\n",
    "        # Loose ID\n",
    "        looseId_key = \"NUM_LooseID_DEN_TrackerMuons\"\n",
    "        loose_nom = corr[looseId_key].evaluate(eta, pt, \"nominal\")\n",
    "        loose_up = corr[looseId_key].evaluate(eta, pt, \"systup\")\n",
    "        loose_down = corr[looseId_key].evaluate(eta, pt, \"systdown\")\n",
    "\n",
    "        sf_nominal *= loose_nom\n",
    "        rel_errs_up_sq += ((loose_up - loose_nom) / loose_nom) ** 2\n",
    "        rel_errs_down_sq += ((loose_down - loose_nom) / loose_nom) ** 2\n",
    "\n",
    "        # High Pt ID\n",
    "        highPtId_key = \"NUM_TrkHighPtID_DEN_TrackerMuons\"\n",
    "        highPtId_nom = corr[highPtId_key].evaluate(eta, pt, \"nominal\")\n",
    "        highPtId_up = corr[highPtId_key].evaluate(eta, pt, \"systup\")\n",
    "        highPtId_down = corr[highPtId_key].evaluate(eta, pt, \"systdown\")\n",
    "\n",
    "        sf_nominal *= highPtId_nom\n",
    "        rel_errs_up_sq += ((highPtId_up - highPtId_nom) / highPtId_nom) ** 2\n",
    "        rel_errs_down_sq += ((highPtId_down - highPtId_nom) / highPtId_nom) ** 2\n",
    "\n",
    "    # Calculate asymmetric uncertainties\n",
    "    total_relative_error_up = np.sqrt(rel_errs_up_sq)\n",
    "    total_relative_error_down = np.sqrt(rel_errs_down_sq)\n",
    "\n",
    "    sf_up = sf_nominal * (1 + total_relative_error_up)\n",
    "    sf_down = sf_nominal * (1 - total_relative_error_down)\n",
    "\n",
    "    weight = df[\"finalWeight\"]\n",
    "    return {\n",
    "        \"finalWeight\": weight * sf_nominal,\n",
    "        \"finalWeight_MUO_up\": weight * sf_up,\n",
    "        \"finalWeight_MUO_down\": weight * sf_down,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd771a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPROCESS or not PROCESSED_PATH.exists():\n",
    "    path_dir = (\n",
    "        \"/ceph/cms/store/user/zichun/bbbb/skimmer/DYLLtoDataHT25June4_v12_ZbbSFZMuMu_zbb-DYLL-data/\"\n",
    "    )\n",
    "\n",
    "    events_dict = {}\n",
    "    # Process eras\n",
    "    for year in YEARS:\n",
    "        dataframes = {\n",
    "            **utils.load_samples(\n",
    "                data_dir=path_dir,\n",
    "                samples=SAMPLES_DICT,\n",
    "                year=year,\n",
    "                columns=utils.format_columns(columns),\n",
    "                variations=True,\n",
    "                weight_shifts=[],\n",
    "            )\n",
    "        }\n",
    "        for sample, df in dataframes.items():\n",
    "            # Add Dimuon kinematics\n",
    "            df = df.assign(**get_dimuon_PtEtaPhiMass(df))\n",
    "            # Apply POG MUO corrections\n",
    "            df = df.assign(**get_POG_MUO_corr(df, corr=CORR_DICT[year]))\n",
    "            dataframes[sample] = df\n",
    "\n",
    "        events_dict[year] = dataframes\n",
    "\n",
    "    # Combine into years\n",
    "    events_combined = {year: {} for year in YEARS_COMBINED_DICT.keys()}\n",
    "    for sample in SAMPLES_DICT:\n",
    "        for combined_year, year_list in YEARS_COMBINED_DICT.items():\n",
    "            events_combined[combined_year][sample] = pd.concat(\n",
    "                [events_dict[year][sample] for year in year_list if sample in events_dict[year]]\n",
    "            )\n",
    "\n",
    "    # Save the processed data\n",
    "    with PROCESSED_PATH.open(\"wb\") as f:\n",
    "        pd.to_pickle(events_combined, f)\n",
    "else:\n",
    "    # Load the processed data\n",
    "    with PROCESSED_PATH.open(\"rb\") as f:\n",
    "        events_combined = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts = {\n",
    "    \"DimuonPt\": PT_RANGE,\n",
    "    \"DimuonMass\": MASS_RANGE,\n",
    "}\n",
    "\n",
    "events_combined_sel = {}\n",
    "for year, events in events_combined.items():\n",
    "    events_combined_sel[year] = {}\n",
    "    for sample, df in events.items():\n",
    "        # Apply cuts\n",
    "        for cut_name, (low, high) in cuts.items():\n",
    "            df = df[(df[cut_name] >= low) & (df[cut_name] <= high)]\n",
    "        # Reset index after filtering\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # Store the selected events\n",
    "        events_combined_sel[year][sample] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name, feature_label, bins in zip(\n",
    "    [\"DimuonMass\", \"DimuonPt\"],\n",
    "    [r\"$M^{\\mu\\mu}$ [GeV]\", r\"$p_\\mathrm{T}^{\\mu\\mu}$ [GeV]\"],\n",
    "    [MASS_BINS, PT_BINS],\n",
    "):\n",
    "    for year in events_combined_sel:\n",
    "        fig, (ax1, ax2) = plt.subplots(\n",
    "            2, 1, figsize=(12, 10), gridspec_kw={\"height_ratios\": [3, 1], \"hspace\": 0.1}\n",
    "        )\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "        # Initialize arrays for MC sum and error calculation\n",
    "        mc_total = np.zeros(len(bins) - 1)\n",
    "        mc_error_sq = np.zeros(len(bins) - 1)  # Sum of squares for statistical error\n",
    "        mc_syst_error_sq = np.zeros(len(bins) - 1)  # Sum of squares for systematic error\n",
    "        data_hist = None\n",
    "        data_error = None\n",
    "\n",
    "        # Plot individual samples and calculate MC sum\n",
    "        for sample in events_combined_sel[year]:\n",
    "            feature = events_combined_sel[year][sample][feature_name].to_numpy()\n",
    "            weight = events_combined_sel[year][sample][\"finalWeight\"].to_numpy()\n",
    "\n",
    "            # Create histogram\n",
    "            hist, _ = np.histogram(feature, bins=bins, weights=weight)\n",
    "\n",
    "            # Plot on main axis\n",
    "            ax1.hist(\n",
    "                feature,\n",
    "                bins=bins,\n",
    "                weights=weight,\n",
    "                label=sample,\n",
    "                histtype=\"step\",\n",
    "            )\n",
    "\n",
    "            if sample == \"data\":\n",
    "                data_hist = hist\n",
    "                # For data, error is sqrt(N) where N is unweighted counts\n",
    "                unweighted_hist, _ = np.histogram(feature, bins=bins)\n",
    "                data_error = np.sqrt(unweighted_hist)\n",
    "            else:\n",
    "                # Sum MC samples\n",
    "                mc_total += hist\n",
    "                # Statistical error: sum of weight squares\n",
    "                weight_sq_hist, _ = np.histogram(feature, bins=bins, weights=weight**2)\n",
    "                mc_error_sq += weight_sq_hist\n",
    "\n",
    "                # Systematic error from MUO variations\n",
    "                weight_up = events_combined_sel[year][sample][\"finalWeight_MUO_up\"].to_numpy()\n",
    "                weight_down = events_combined_sel[year][sample][\"finalWeight_MUO_down\"].to_numpy()\n",
    "\n",
    "                hist_up, _ = np.histogram(feature, bins=bins, weights=weight_up)\n",
    "                hist_down, _ = np.histogram(feature, bins=bins, weights=weight_down)\n",
    "\n",
    "                # Systematic uncertainty: max deviation from nominal\n",
    "                syst_var = np.maximum(np.abs(hist_up - hist), np.abs(hist_down - hist))\n",
    "                mc_syst_error_sq += syst_var**2\n",
    "\n",
    "        # Total MC error: quadrature sum of statistical and systematic\n",
    "        mc_stat_error = np.sqrt(mc_error_sq)\n",
    "        mc_syst_error = np.sqrt(mc_syst_error_sq)\n",
    "        mc_total_error = np.sqrt(mc_error_sq + mc_syst_error_sq)\n",
    "\n",
    "        # Main plot formatting\n",
    "        ax1.set_ylabel(\"Events\")\n",
    "        ax1.set_yscale(\"log\")\n",
    "        ax1.legend()\n",
    "        ax1.tick_params(axis=\"x\", labelbottom=False)  # Hide x-axis labels on top plot\n",
    "\n",
    "        # Set x-axis limits - extend to 2000 for pT plots\n",
    "        if feature_name == \"DimuonPt\":\n",
    "            ax1.set_xlim(200, 2000)\n",
    "\n",
    "        # Ratio plot\n",
    "        if data_hist is not None:\n",
    "            # Calculate ratio and its error\n",
    "            ratio = np.divide(\n",
    "                data_hist, mc_total, out=np.zeros_like(data_hist), where=mc_total != 0\n",
    "            )\n",
    "\n",
    "            # Error propagation for ratio: sqrt((σ_data/MC)² + (data*σ_MC_total/MC²)²)\n",
    "            ratio_error = np.zeros_like(ratio)\n",
    "            mask = mc_total > 0\n",
    "            ratio_error[mask] = np.sqrt(\n",
    "                (data_error[mask] / mc_total[mask]) ** 2\n",
    "                + (data_hist[mask] * mc_total_error[mask] / mc_total[mask] ** 2) ** 2\n",
    "            )\n",
    "\n",
    "            # Plot ratio with error bars\n",
    "            ax2.errorbar(bin_centers, ratio, yerr=ratio_error, fmt=\"ko\", markersize=3, capsize=2)\n",
    "\n",
    "            # Add horizontal line at y=1\n",
    "            ax2.axhline(y=1, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "            # Ratio plot formatting\n",
    "            ax2.set_xlabel(feature_label)\n",
    "            ax2.set_ylabel(\"Data / MC\")\n",
    "            ax2.set_ylim(0.5, 1.5)  # Adjust as needed\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "            # Set x-axis limits for ratio plot too\n",
    "            if feature_name == \"DimuonPt\":\n",
    "                ax2.set_xlim(PT_BINS[0], PT_BINS[-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        hep.cms.label(\n",
    "            ax=ax1,\n",
    "            label=\"Work in Progress\",\n",
    "            data=True,\n",
    "            year=year.replace(\"All\", \"\"),\n",
    "            com=13.6,\n",
    "            lumi=(round(LUMI[year] / 1000, 2)),\n",
    "        )\n",
    "\n",
    "        plt.savefig(PLOT_DIR / f\"{feature_name}_{year}.pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6ff74",
   "metadata": {},
   "source": [
    "# Derive $f_\\mathrm{DYLL \\to Data} (p_\\mathrm{T}^{\\mu\\mu})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36febf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_sample = \"DYto2L\"\n",
    "\n",
    "SF_dict = {}\n",
    "\n",
    "for year in events_combined_sel:\n",
    "    data_counts = np.zeros(len(PT_BINS) - 1)\n",
    "    signal_counts = np.zeros(len(PT_BINS) - 1)\n",
    "    background_counts = np.zeros(len(PT_BINS) - 1)\n",
    "\n",
    "    signal_counts_up = np.zeros(len(PT_BINS) - 1)\n",
    "    signal_counts_down = np.zeros(len(PT_BINS) - 1)\n",
    "    background_counts_up = np.zeros(len(PT_BINS) - 1)\n",
    "    background_counts_down = np.zeros(len(PT_BINS) - 1)\n",
    "\n",
    "    # Statistical error tracking\n",
    "    data_sumw2 = np.zeros(len(PT_BINS) - 1)\n",
    "    signal_sumw2 = np.zeros(len(PT_BINS) - 1)\n",
    "    background_sumw2 = np.zeros(len(PT_BINS) - 1)\n",
    "\n",
    "    # Process each sample\n",
    "    for sample in events_combined_sel[year]:\n",
    "        pt = events_combined_sel[year][sample][\"DimuonPt\"].to_numpy()\n",
    "        weight = events_combined_sel[year][sample][\"finalWeight\"].to_numpy()\n",
    "\n",
    "        # Create histogram for this sample in PT bins\n",
    "        hist, _ = np.histogram(pt, bins=PT_BINS, weights=weight)\n",
    "        hist_sumw2, _ = np.histogram(pt, bins=PT_BINS, weights=weight**2)\n",
    "\n",
    "        if sample == \"data\":\n",
    "            data_counts = hist\n",
    "            data_sumw2 = hist_sumw2\n",
    "        elif sample == signal_sample:\n",
    "            signal_counts = hist\n",
    "            signal_sumw2 = hist_sumw2\n",
    "            # Signal systematic variations\n",
    "            weight_up = events_combined_sel[year][sample][\"finalWeight_MUO_up\"].to_numpy()\n",
    "            weight_down = events_combined_sel[year][sample][\"finalWeight_MUO_down\"].to_numpy()\n",
    "            signal_counts_up, _ = np.histogram(pt, bins=PT_BINS, weights=weight_up)\n",
    "            signal_counts_down, _ = np.histogram(pt, bins=PT_BINS, weights=weight_down)\n",
    "        else:\n",
    "            # Background: other MCs\n",
    "            background_counts += hist\n",
    "            background_sumw2 += hist_sumw2\n",
    "            # Get systematic variations for background\n",
    "            weight_up = events_combined_sel[year][sample][\"finalWeight_MUO_up\"].to_numpy()\n",
    "            weight_down = events_combined_sel[year][sample][\"finalWeight_MUO_down\"].to_numpy()\n",
    "            hist_up, _ = np.histogram(pt, bins=PT_BINS, weights=weight_up)\n",
    "            hist_down, _ = np.histogram(pt, bins=PT_BINS, weights=weight_down)\n",
    "            background_counts_up += hist_up\n",
    "            background_counts_down += hist_down\n",
    "\n",
    "    # Calculate nominal scale factors and systematic variations\n",
    "    numerator = data_counts - background_counts\n",
    "    numerator_up = data_counts - background_counts_up\n",
    "    numerator_down = data_counts - background_counts_down\n",
    "\n",
    "    scale_factor = np.divide(\n",
    "        numerator, signal_counts, out=np.zeros_like(numerator), where=signal_counts > 0\n",
    "    )\n",
    "    scale_factor_syst_up = np.divide(\n",
    "        numerator_up, signal_counts_up, out=np.zeros_like(numerator_up), where=signal_counts_up > 0\n",
    "    )\n",
    "    scale_factor_syst_down = np.divide(\n",
    "        numerator_down,\n",
    "        signal_counts_down,\n",
    "        out=np.zeros_like(numerator_down),\n",
    "        where=signal_counts_down > 0,\n",
    "    )\n",
    "\n",
    "    # Calculate statistical errors\n",
    "    data_stat_err = np.sqrt(data_sumw2)\n",
    "    background_stat_err = np.sqrt(background_sumw2)\n",
    "    signal_stat_err = np.sqrt(signal_sumw2)\n",
    "\n",
    "    # Statistical error propagation for scale factor\n",
    "    numerator_stat_err = np.sqrt(data_stat_err**2 + background_stat_err**2)\n",
    "    numerator_rel_err = np.divide(\n",
    "        numerator_stat_err,\n",
    "        np.abs(numerator),\n",
    "        out=np.zeros_like(numerator_stat_err),\n",
    "        where=np.abs(numerator) > 0,\n",
    "    )\n",
    "    signal_rel_err = np.divide(\n",
    "        signal_stat_err, signal_counts, out=np.zeros_like(signal_stat_err), where=signal_counts > 0\n",
    "    )\n",
    "    sf_rel_stat_err = np.sqrt(numerator_rel_err**2 + signal_rel_err**2)\n",
    "    sf_stat_err = np.abs(scale_factor) * sf_rel_stat_err\n",
    "\n",
    "    # Systematic errors (difference from nominal)\n",
    "    sf_syst_err_up = np.abs(scale_factor_syst_up - scale_factor)\n",
    "    sf_syst_err_down = np.abs(scale_factor_syst_down - scale_factor)\n",
    "\n",
    "    # Combine systematic and statistical errors in quadrature\n",
    "    sf_total_err_up = np.sqrt(sf_syst_err_up**2 + sf_stat_err**2)\n",
    "    sf_total_err_down = np.sqrt(sf_syst_err_down**2 + sf_stat_err**2)\n",
    "\n",
    "    # Final up/down variations\n",
    "    scale_factor_up = scale_factor + sf_total_err_up\n",
    "    scale_factor_down = scale_factor - sf_total_err_down\n",
    "\n",
    "    SF_dict[year] = {\n",
    "        \"nominal\": scale_factor,\n",
    "        \"up\": scale_factor_up,\n",
    "        \"down\": scale_factor_down,\n",
    "        \"pt\": PT_BINS,\n",
    "    }\n",
    "\n",
    "# Save to pickle file\n",
    "with OUTPUT_PATH.open(\"wb\") as f:\n",
    "    pd.to_pickle(SF_dict, f)\n",
    "\n",
    "print(f\"Scale factors saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0337b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scale factors\n",
    "for year in SF_dict:\n",
    "    sf = SF_dict[year]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot nominal scale factor\n",
    "    ax.errorbar(\n",
    "        (PT_BINS[:-1] + PT_BINS[1:]) / 2,\n",
    "        sf[\"nominal\"],\n",
    "        xerr=(PT_BINS[1:] - PT_BINS[:-1]) / 2,\n",
    "        yerr=sf[\"up\"] - sf[\"nominal\"],\n",
    "        fmt=\"o\",\n",
    "        color=\"blue\",\n",
    "        markersize=5,\n",
    "        capsize=3,\n",
    "    )\n",
    "\n",
    "    # plot 1\n",
    "    ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Nominal SF = 1\")\n",
    "\n",
    "    ax.set_xlabel(r\"$p_\\mathrm{T}^{\\mu\\mu}$ [GeV]\")\n",
    "    ax.set_ylabel(\"Scale Factor\")\n",
    "    # ax.set_xlim(PT_BINS[0], PT_BINS[-1])\n",
    "    ax.set_ylim(0.5, 1.5)\n",
    "    # ax.grid(True)\n",
    "\n",
    "    hep.cms.label(\n",
    "        ax=ax,\n",
    "        label=\"Work in Progress\",\n",
    "        data=True,\n",
    "        year=year.replace(\"All\", \"\"),\n",
    "        com=13.6,\n",
    "        lumi=(round(LUMI[year] / 1000, 2)),\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_DIR / f\"SF_{year}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f8b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh4b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
