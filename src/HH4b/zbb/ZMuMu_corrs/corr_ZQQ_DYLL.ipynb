{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reloads imported files on edits\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072fa344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from HH4b import utils\n",
    "from HH4b.hh_vars import LUMI\n",
    "import mplhep as hep\n",
    "\n",
    "hep.style.use(hep.style.CMS)\n",
    "\n",
    "from constants import (\n",
    "    MASS_RANGE,\n",
    "    MASS_BINS,\n",
    "    PT_RANGE,\n",
    "    PT_BINS,\n",
    "    BR_Z_QQ,\n",
    "    BR_Z_EE,\n",
    "    BR_Z_MUMU,\n",
    "    BR_Z_TAUTAU,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = [\"2022\", \"2022EE\", \"2023\", \"2023BPix\"]\n",
    "YEARS_COMBINED_DICT = {\n",
    "    \"2022All\": [\"2022\", \"2022EE\"],\n",
    "    \"2023All\": [\"2023\", \"2023BPix\"],\n",
    "}\n",
    "\n",
    "PROCESSED_PATH: Path = Path(\"processed/corr_ZQQ_DYLL.pkl\")\n",
    "(PROCESSED_PATH.parent).mkdir(parents=True, exist_ok=True)\n",
    "REPROCESS: bool = False  # if True, reprocess from the skimmed ntuples\n",
    "\n",
    "SAMPLES_DICT = {\n",
    "    \"DYto2L\": [\"DYto2L\"],\n",
    "    \"Zto2Q\": [\"Zto2Q\"],\n",
    "}\n",
    "\n",
    "PLOT_DIR = Path(\"plots/corr_ZQQ_DYLL\")\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_PATH = Path(\"corrs/ZQQ_DYLL.pkl\")\n",
    "(OUTPUT_PATH.parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "P4 = (\"Mass\", \"Pt\", \"Eta\", \"Phi\")\n",
    "DYLL_columns = [(\"weight\", 1)]\n",
    "ZQQ_columns = [(\"weight\", 1)]\n",
    "\n",
    "for i in range(2):\n",
    "    for branch in P4:\n",
    "        DYLL_columns.append((f\"GenLep{i+1}{branch}\", 1))\n",
    "        ZQQ_columns.append((f\"GenQ{i+1}{branch}\", 1))\n",
    "\n",
    "for branch in P4:\n",
    "    DYLL_columns.append((f\"GenZ{branch}\", 1))\n",
    "    ZQQ_columns.append((f\"GenZ{branch}\", 1))\n",
    "\n",
    "COLUMN_DICT = {\n",
    "    \"DYto2L\": DYLL_columns,\n",
    "    \"Zto2Q\": ZQQ_columns,\n",
    "}\n",
    "\n",
    "SAMPLE_BR = {\n",
    "    \"DYto2L\": BR_Z_EE + BR_Z_MUMU + BR_Z_TAUTAU,\n",
    "    \"Zto2Q\": BR_Z_QQ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GenZ_PtEtaPhiMass(df: pd.DataFrame, particle: str) -> pd.Series:\n",
    "    \"\"\"Calculate the GenZ mass from the muon 4-vectors.\"\"\"\n",
    "    part1_pt = df[(f\"{particle}1Pt\", 0)]\n",
    "    part1_eta = df[(f\"{particle}1Eta\", 0)]\n",
    "    part1_phi = df[(f\"{particle}1Phi\", 0)]\n",
    "    part1_mass = df[(f\"{particle}1Mass\", 0)]\n",
    "    part1_px = part1_pt * np.cos(part1_phi)\n",
    "    part1_py = part1_pt * np.sin(part1_phi)\n",
    "    part1_pz = part1_pt * np.sinh(part1_eta)\n",
    "    part1_E = np.sqrt(part1_px**2 + part1_py**2 + part1_pz**2 + part1_mass**2)\n",
    "\n",
    "    part2_pt = df[(f\"{particle}2Pt\", 0)]\n",
    "    part2_eta = df[(f\"{particle}2Eta\", 0)]\n",
    "    part2_phi = df[(f\"{particle}2Phi\", 0)]\n",
    "    part2_mass = df[(f\"{particle}2Mass\", 0)]\n",
    "    part2_px = part2_pt * np.cos(part2_phi)\n",
    "    part2_py = part2_pt * np.sin(part2_phi)\n",
    "    part2_pz = part2_pt * np.sinh(part2_eta)\n",
    "    part2_E = np.sqrt(part2_px**2 + part2_py**2 + part2_pz**2 + part2_mass**2)\n",
    "\n",
    "    GenZ_px = part1_px + part2_px\n",
    "    GenZ_py = part1_py + part2_py\n",
    "    GenZ_pz = part1_pz + part2_pz\n",
    "    GenZ_E = part1_E + part2_E\n",
    "\n",
    "    GenZ_pt = np.sqrt(GenZ_px**2 + GenZ_py**2)\n",
    "    GenZ_eta = np.arcsinh(GenZ_pz / np.sqrt(GenZ_px**2 + GenZ_py**2))\n",
    "    GenZ_phi = np.arctan2(GenZ_py, GenZ_px)\n",
    "    GenZ_mass = np.sqrt(GenZ_E**2 - (GenZ_px**2 + GenZ_py**2 + GenZ_pz**2))\n",
    "\n",
    "    return {\n",
    "        \"GenZFromDecayPt\": GenZ_pt,\n",
    "        \"GenZFromDecayEta\": GenZ_eta,\n",
    "        \"GenZFromDecayPhi\": GenZ_phi,\n",
    "        \"GenZFromDecayMass\": GenZ_mass,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPROCESS or not PROCESSED_PATH.exists():\n",
    "    path_dir = (\n",
    "        \"/ceph/cms/store/user/zichun/bbbb/skimmer/ZQQtoDYLLHT25June4_v12_ZbbSFZMuMu_zbb-Zto2Q-DYLL\"\n",
    "    )\n",
    "\n",
    "    events_dict = {}\n",
    "    # Process eras\n",
    "    for year in tqdm(YEARS):\n",
    "        events_dict[year] = {}\n",
    "\n",
    "        for sample, sample_list in SAMPLES_DICT.items():\n",
    "            columns = COLUMN_DICT[sample]\n",
    "            temp_dataframes = utils.load_samples(\n",
    "                data_dir=path_dir,\n",
    "                samples={sample: sample_list},\n",
    "                year=year,\n",
    "                columns=utils.format_columns(columns),\n",
    "                variations=True,\n",
    "                weight_shifts=[],\n",
    "            )\n",
    "\n",
    "            # Process each dataframe in temp_dataframes\n",
    "            for sample_name, df in temp_dataframes.items():\n",
    "                # Add GenZ kinematics\n",
    "                if sample_name == \"DYto2L\":\n",
    "                    particle = \"GenLep\"\n",
    "                elif sample_name == \"Zto2Q\":\n",
    "                    particle = \"GenQ\"\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown sample name: {sample_name}\")\n",
    "\n",
    "                df = df.assign(**get_GenZ_PtEtaPhiMass(df, particle))\n",
    "                events_dict[year][sample_name] = df\n",
    "\n",
    "    # Combine into years\n",
    "    events_combined = {year: {} for year in YEARS_COMBINED_DICT.keys()}\n",
    "    for sample in SAMPLES_DICT:\n",
    "        for combined_year, year_list in YEARS_COMBINED_DICT.items():\n",
    "            events_combined[combined_year][sample] = pd.concat(\n",
    "                [\n",
    "                    events_dict[year][sample]\n",
    "                    for year in year_list\n",
    "                    if year in events_dict and sample in events_dict[year]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    with PROCESSED_PATH.open(\"wb\") as f:\n",
    "        pd.to_pickle(events_combined, f)\n",
    "    print(f\"Processed data saved to {PROCESSED_PATH}\")\n",
    "else:\n",
    "    print(f\"Loading processed data from {PROCESSED_PATH}\")\n",
    "    with PROCESSED_PATH.open(\"rb\") as f:\n",
    "        events_combined = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c9606",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts = {\n",
    "    (\"GenZPt\", 0): PT_RANGE,\n",
    "    (\"GenZMass\", 0): (0, np.inf),\n",
    "}\n",
    "\n",
    "events_combined_sel = {}\n",
    "for year, events in events_combined.items():\n",
    "    events_combined_sel[year] = {}\n",
    "    for sample, df in events.items():\n",
    "        # Apply cuts\n",
    "        for cut_name, (low, high) in cuts.items():\n",
    "            df = df[(df[cut_name] >= low) & (df[cut_name] <= high)]\n",
    "        # Reset index after filtering\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # Store the selected events\n",
    "        events_combined_sel[year][sample] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f531bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which samples go in numerator and denominator\n",
    "numerator_samples = [\"DYto2L\"]  # Add more samples as needed\n",
    "denominator_samples = [\"Zto2Q\"]  # Add more samples as needed\n",
    "\n",
    "for feature_name, feature_label, bins in zip(\n",
    "    [(\"GenZMass\", 0), (\"GenZPt\", 0), \"GenZFromDecayMass\", \"GenZFromDecayPt\"],\n",
    "    [r\"Z Mass [GeV]\", r\"Z $p_\\mathrm{T}$ [GeV]\", r\"Z Mass [GeV]\", r\"Z $p_\\mathrm{T}$ [GeV]\"],\n",
    "    [MASS_BINS, PT_BINS, MASS_BINS, PT_BINS],\n",
    "):\n",
    "    for year in events_combined_sel:\n",
    "        fig, (ax1, ax2) = plt.subplots(\n",
    "            2, 1, figsize=(12, 10), gridspec_kw={\"height_ratios\": [3, 1], \"hspace\": 0.1}\n",
    "        )\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "        # Initialize arrays for numerator and denominator\n",
    "        numerator_total = np.zeros(len(bins) - 1)\n",
    "        numerator_error_sq = np.zeros(len(bins) - 1)\n",
    "        denominator_total = np.zeros(len(bins) - 1)\n",
    "        denominator_error_sq = np.zeros(len(bins) - 1)\n",
    "\n",
    "        # Plot individual samples and calculate sums\n",
    "        for sample in events_combined_sel[year]:\n",
    "            feature = events_combined_sel[year][sample][feature_name].to_numpy()\n",
    "            branching_ratio = SAMPLE_BR[sample]\n",
    "            # weight = events_combined_sel[year][sample][\"finalWeight\"].to_numpy() / branching_ratio\n",
    "            weight = events_combined_sel[year][sample][\"finalWeight\"].to_numpy()\n",
    "\n",
    "            # Alternative: Calculate both without density, then normalize manually\n",
    "            hist_counts, bin_edges = np.histogram(feature, bins=bins, weights=weight, density=False)\n",
    "            weight_sq_hist, _ = np.histogram(feature, bins=bins, weights=weight**2, density=False)\n",
    "\n",
    "            # Calculate normalization factors\n",
    "            bin_widths = bin_edges[1:] - bin_edges[:-1]\n",
    "            total_sum_weights = np.sum(weight)\n",
    "\n",
    "            # Convert to density\n",
    "            hist = hist_counts / (bin_widths * total_sum_weights)\n",
    "            weight_sq_hist_density = weight_sq_hist / (bin_widths * total_sum_weights) ** 2\n",
    "\n",
    "            # Plot on main axis with density normalization\n",
    "            ax1.hist(\n",
    "                feature,\n",
    "                bins=bins,\n",
    "                weights=weight,\n",
    "                label=sample,\n",
    "                histtype=\"step\",\n",
    "                density=True,\n",
    "            )\n",
    "\n",
    "            # Determine which sum this sample contributes to\n",
    "            if sample in numerator_samples:\n",
    "                numerator_total += hist\n",
    "                numerator_error_sq += weight_sq_hist_density\n",
    "            elif sample in denominator_samples:\n",
    "                denominator_total += hist\n",
    "                denominator_error_sq += weight_sq_hist_density\n",
    "\n",
    "        numerator_error = np.sqrt(numerator_error_sq)\n",
    "        denominator_error = np.sqrt(denominator_error_sq)\n",
    "\n",
    "        # Main plot formatting\n",
    "        ax1.set_ylabel(\"Density\")\n",
    "        ax1.set_yscale(\"log\")\n",
    "        ax1.legend()\n",
    "        ax1.tick_params(axis=\"x\", labelbottom=False)  # Hide x-axis labels on top plot\n",
    "\n",
    "        # Set x-axis limits - extend to 2000 for pT plots\n",
    "        if feature_name == \"DimuonPt\":\n",
    "            ax1.set_xlim(200, 2000)\n",
    "\n",
    "        # Ratio plot\n",
    "        if len(numerator_samples) > 0 and len(denominator_samples) > 0:\n",
    "            # Calculate ratio and its error\n",
    "            ratio = np.divide(\n",
    "                numerator_total,\n",
    "                denominator_total,\n",
    "                out=np.zeros_like(numerator_total),\n",
    "                where=denominator_total != 0,\n",
    "            )\n",
    "\n",
    "            # Error propagation for ratio: sqrt((σ_num/denom)² + (num*σ_denom/denom²)²)\n",
    "            ratio_error = np.zeros_like(ratio)\n",
    "            mask = denominator_total > 0\n",
    "            ratio_error[mask] = np.sqrt(\n",
    "                (numerator_error[mask] / denominator_total[mask]) ** 2\n",
    "                + (numerator_total[mask] * denominator_error[mask] / denominator_total[mask] ** 2)\n",
    "                ** 2\n",
    "            )\n",
    "\n",
    "            # Plot ratio with error bars\n",
    "            ax2.errorbar(bin_centers, ratio, yerr=ratio_error, fmt=\"ko\", markersize=3, capsize=2)\n",
    "\n",
    "            # Add horizontal line at y=1\n",
    "            ax2.axhline(y=1, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "            # Create dynamic label for ratio plot\n",
    "            numerator_label = \" + \".join(numerator_samples)\n",
    "            denominator_label = \" + \".join(denominator_samples)\n",
    "            ratio_label = f\"{numerator_label} / {denominator_label}\"\n",
    "\n",
    "            # Ratio plot formatting\n",
    "            ax2.set_xlabel(feature_label)\n",
    "            ax2.set_ylabel(ratio_label)\n",
    "            ax2.set_ylim(0, 2)  # Adjust as needed\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "            # Set x-axis limits for ratio plot too\n",
    "            if feature_name == \"GenZPt\":\n",
    "                ax2.set_xlim(PT_BINS[0], PT_BINS[-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        hep.cms.label(\n",
    "            ax=ax1,\n",
    "            label=\"Work in Progress\",\n",
    "            data=True,\n",
    "            year=year.replace(\"All\", \"\"),\n",
    "            com=13.6,\n",
    "            lumi=(round(LUMI[year] / 1000, 2)),\n",
    "        )\n",
    "\n",
    "        if isinstance(feature_name, tuple):\n",
    "            name = feature_name[0]\n",
    "        else:\n",
    "            name = feature_name\n",
    "        plt.savefig(PLOT_DIR / f\"{name}_{year}.pdf\", bbox_inches=\"tight\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa59971",
   "metadata": {},
   "source": [
    "# Derive $f_\\mathrm{ZQQ \\to DYLL} (p_\\mathrm{T})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de4d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which samples go in numerator and denominator\n",
    "numerator_samples = [\"DYto2L\"]\n",
    "denominator_samples = [\"Zto2Q\"]\n",
    "\n",
    "SF_dict = {}\n",
    "\n",
    "for year in events_combined_sel:\n",
    "    numerator_counts = np.zeros(len(PT_BINS) - 1)\n",
    "    denominator_counts = np.zeros(len(PT_BINS) - 1)\n",
    "\n",
    "    # Statistical error tracking\n",
    "    numerator_sumw2 = np.zeros(len(PT_BINS) - 1)\n",
    "    denominator_sumw2 = np.zeros(len(PT_BINS) - 1)\n",
    "\n",
    "    # Process each sample\n",
    "    for sample in events_combined_sel[year]:\n",
    "        # pt = events_combined_sel[year][sample][\"GenZFromDecayPt\"].to_numpy()\n",
    "        pt = events_combined_sel[year][sample][(\"GenZPt\", 0)].to_numpy()\n",
    "        weight = events_combined_sel[year][sample][\"finalWeight\"].to_numpy()\n",
    "\n",
    "        # Create histogram for this sample in PT bins\n",
    "        hist, bin_edges = np.histogram(pt, bins=PT_BINS, weights=weight, density=True)\n",
    "        hist_sumw2, _ = np.histogram(\n",
    "            pt, bins=PT_BINS, weights=weight**2, density=False\n",
    "        )  # Used for statistical error\n",
    "\n",
    "        # Calculate bin widths for proper density error conversion\n",
    "        bin_widths = bin_edges[1:] - bin_edges[:-1]\n",
    "\n",
    "        # Convert sumw2 to density scale: divide by (bin_width * total_sum_weights)^2\n",
    "        total_sum_weights = np.sum(weight)\n",
    "        hist_sumw2_density = hist_sumw2 / (bin_widths * total_sum_weights) ** 2\n",
    "\n",
    "        # Determine which sum this sample contributes to\n",
    "        if sample in numerator_samples:\n",
    "            numerator_counts += hist\n",
    "            numerator_sumw2 += hist_sumw2_density\n",
    "        elif sample in denominator_samples:\n",
    "            denominator_counts += hist\n",
    "            denominator_sumw2 += hist_sumw2_density\n",
    "\n",
    "    # Calculate scale factors\n",
    "    scale_factor = np.divide(\n",
    "        numerator_counts,\n",
    "        denominator_counts,\n",
    "        out=np.zeros_like(numerator_counts),\n",
    "        where=denominator_counts > 0,\n",
    "    )\n",
    "\n",
    "    # Calculate statistical errors (scaled for density)\n",
    "    numerator_stat_err = np.sqrt(numerator_sumw2)\n",
    "    denominator_stat_err = np.sqrt(denominator_sumw2)\n",
    "\n",
    "    # Statistical error propagation for scale factor\n",
    "    # For ratio A/B: σ_ratio = |A/B| * sqrt((σ_A/A)² + (σ_B/B)²)\n",
    "    numerator_rel_err = np.divide(\n",
    "        numerator_stat_err,\n",
    "        np.abs(numerator_counts),\n",
    "        out=np.zeros_like(numerator_stat_err),\n",
    "        where=np.abs(numerator_counts) > 0,\n",
    "    )\n",
    "    denominator_rel_err = np.divide(\n",
    "        denominator_stat_err,\n",
    "        denominator_counts,\n",
    "        out=np.zeros_like(denominator_stat_err),\n",
    "        where=denominator_counts > 0,\n",
    "    )\n",
    "\n",
    "    sf_rel_stat_err = np.sqrt(numerator_rel_err**2 + denominator_rel_err**2)\n",
    "    sf_stat_err = np.abs(scale_factor) * sf_rel_stat_err\n",
    "\n",
    "    # Scale factor with statistical uncertainties\n",
    "    scale_factor_up = scale_factor + sf_stat_err\n",
    "    scale_factor_down = scale_factor - sf_stat_err\n",
    "\n",
    "    # Create dynamic labels for the scale factor\n",
    "    numerator_label = \"_\".join(numerator_samples)\n",
    "    denominator_label = \"_\".join(denominator_samples)\n",
    "\n",
    "    SF_dict[year] = {\n",
    "        \"nominal\": scale_factor,\n",
    "        \"up\": scale_factor_up,\n",
    "        \"down\": scale_factor_down,\n",
    "        \"pt\": PT_BINS,\n",
    "    }\n",
    "\n",
    "    # Print summary for this year\n",
    "    print(f\"\\n{year} Scale Factors ({numerator_label}/{denominator_label}):\")\n",
    "    pt_centers = (PT_BINS[:-1] + PT_BINS[1:]) / 2\n",
    "    for i, (pt_center, sf, sf_err) in enumerate(zip(pt_centers, scale_factor, sf_stat_err)):\n",
    "        print(f\"  pT bin {pt_center:.0f} GeV: {sf:.3f} ± {sf_err:.3f}\")\n",
    "\n",
    "# Save to pickle file\n",
    "with OUTPUT_PATH.open(\"wb\") as f:\n",
    "    pd.to_pickle(SF_dict, f)\n",
    "\n",
    "print(f\"\\nScale factors saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scale factors\n",
    "for year in SF_dict:\n",
    "    sf = SF_dict[year]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot nominal scale factor\n",
    "    ax.errorbar(\n",
    "        (PT_BINS[:-1] + PT_BINS[1:]) / 2,\n",
    "        sf[\"nominal\"],\n",
    "        xerr=(PT_BINS[1:] - PT_BINS[:-1]) / 2,\n",
    "        yerr=sf[\"up\"] - sf[\"nominal\"],\n",
    "        fmt=\"o\",\n",
    "        color=\"blue\",\n",
    "        markersize=5,\n",
    "        capsize=3,\n",
    "    )\n",
    "\n",
    "    # plot 1\n",
    "    ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Nominal SF = 1\")\n",
    "\n",
    "    ax.set_xlabel(r\"Z $p_\\mathrm{T}$ [GeV]\")\n",
    "    ax.set_ylabel(\"Scale Factor\")\n",
    "    # ax.set_xlim(PT_BINS[0], PT_BINS[-1])\n",
    "    ax.set_ylim(0.5, 1.5)\n",
    "    # ax.grid(True)\n",
    "\n",
    "    hep.cms.label(\n",
    "        ax=ax,\n",
    "        label=\"Work in Progress\",\n",
    "        data=True,\n",
    "        year=year.replace(\"All\", \"\"),\n",
    "        com=13.6,\n",
    "        lumi=(round(LUMI[year] / 1000, 2)),\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_DIR / f\"SF_{year}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e9360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh4b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
