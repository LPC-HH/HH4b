{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c56774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reloads imported files on edits\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from HH4b import utils\n",
    "import itertools\n",
    "import correctionlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YEARS = [\"2022\", \"2022EE\", \"2023\", \"2023BPix\", \"2024\", \"2025\"]\n",
    "YEARS = [\"2024\", \"2025\"]\n",
    "YEARS_COMBINED_DICT = {\n",
    "    # \"2022All\": [\"2022\", \"2022EE\"],\n",
    "    # \"2023All\": [\"2023\", \"2023BPix\"],\n",
    "    \"2024All\": [\"2024\"],\n",
    "    \"2025All\": [\"2025\"],\n",
    "}\n",
    "\n",
    "tag = \"nanov15_20251202_v15_signal\"\n",
    "\n",
    "STORAGE_PROJ_DIR = Path(\"/ceph/cms/store/user/zichun/bbbb\")\n",
    "DATA_DIR = STORAGE_PROJ_DIR / f\"skimmer/{tag}\"\n",
    "PROCESSED_DIR = STORAGE_PROJ_DIR / f\"signal_processed/skimmer/{tag}\"\n",
    "PROCESSED_DIR.mkdir(exist_ok=True, parents=True)\n",
    "PLOT_DIR = Path(\"bbbbskimmer_plots\") / tag\n",
    "PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "REPROCESS: bool = False  # if True, reprocess from the skimmed ntuples\n",
    "APPLY_TRIGGER_SF: bool = True\n",
    "\n",
    "SAMPLES_DICT = {\n",
    "    # referenced from hh_vars\n",
    "    \"data\": [\"JetMET\"],\n",
    "    \"hh4b\": [\"GluGlutoHHto4B\"],\n",
    "    \"vbfhh4b\": [\"VBFHHto4B_CV_1_C2V_1_C3_1\"],\n",
    "    \"vbfhh4b-k2v0\": [\"VBFHHto4B_CV_1_C2V_0_C3_1\"],\n",
    "    \"qcd\": [\n",
    "        # \"QCD_HT-200to400\",\n",
    "        \"QCD_HT-400to600\",\n",
    "        \"QCD_HT-600to800\",\n",
    "        \"QCD_HT-800to1000\",\n",
    "        \"QCD_HT-1000to1200\",\n",
    "        \"QCD_HT-1200to1500\",\n",
    "        \"QCD_HT-1500to2000\",\n",
    "        \"QCD_HT-2000\",\n",
    "    ],\n",
    "    \"ttbar\": [\"TTto4Q\", \"TTtoLNu2Q\", \"TTto2L2Nu\"],\n",
    "    \"vhtobb\": [\n",
    "        \"WplusH_Hto2B_Wto2Q_M-125\",\n",
    "        \"WminusH_Hto2B_Wto2Q_M-125\",\n",
    "        \"ZH_Hto2B_Zto2Q_M-125\",\n",
    "        \"ggZH_Hto2B_Zto2Q_M-125\",\n",
    "    ],\n",
    "    \"novhhtobb\": [\"GluGluHto2B_PT-200_M-125\", \"VBFHto2B_M-125\"],\n",
    "    \"tthtobb\": [\"ttHto2B_M-125\"],\n",
    "    \"zz\": [\"ZZ\"],\n",
    "    \"nozzdiboson\": [\"WW\", \"WZ\"],\n",
    "    \"vjets\": [\"Wto2Q-2Jets_Bin-PTQQ\", \"Zto2Q-2Jets_PTQQ\"],\n",
    "}\n",
    "MC_SAMPLES_LIST = [sample for sample in SAMPLES_DICT.keys() if sample != \"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HH4b.hh_vars import LUMI\n",
    "\n",
    "actual_lumis = {\n",
    "    \"2022\": 7.966932936,\n",
    "    \"2022EE\": 26.640878954,\n",
    "    \"2023\": 18.062658998,\n",
    "    \"2023BPix\": 9.692645994,\n",
    "    \"2024\": 107.125771309,\n",
    "    \"2025\": (20780.0 + 14000.0 + 25290.0 + 30350.0) / 1000,  # projected lumi in fb-1\n",
    "}\n",
    "for year, era_list in YEARS_COMBINED_DICT.items():\n",
    "    lumi_acc = 0\n",
    "    for era in era_list:\n",
    "        lumi_acc += actual_lumis[era]\n",
    "    actual_lumis[year] = lumi_acc\n",
    "\n",
    "\n",
    "# convert fb-1 to pb-1\n",
    "for k, v in actual_lumis.items():\n",
    "    actual_lumis[k] = v * 1000\n",
    "    # override\n",
    "    LUMI[k] = v * 1000\n",
    "print(\"Actual lumis (pb-1):\", actual_lumis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to load from the ntuples\n",
    "sys_vars = [\"FSRPartonShower\", \"ISRPartonShower\", \"pileup\"]\n",
    "weight_shifts = sys_vars + [\"pdf_weights\", \"scale_weights\"]\n",
    "\n",
    "fatjet_vars = [\n",
    "    \"bbFatJetPt\",\n",
    "    \"bbFatJetEta\",\n",
    "    \"bbFatJetPhi\",\n",
    "    \"bbFatJetMsd\",\n",
    "    \"bbFatJetPNetMassLegacy\",\n",
    "    \"bbFatJetParT3massGeneric\",\n",
    "    \"bbFatJetParT3massX2p\",\n",
    "    \"bbFatJetParT3TXbb\",\n",
    "]\n",
    "mass_vars = [\n",
    "    \"bbFatJetParT3massGeneric\",\n",
    "    \"bbFatJetParT3massX2p\",\n",
    "]\n",
    "\n",
    "pt_variations = []\n",
    "# for jesr, ud in itertools.product([\"JES\", \"JER\"], [\"up\", \"down\"]):\n",
    "#     pt_variations.append(f\"bbFatJetPt_{jesr}_{ud}\")\n",
    "\n",
    "mass_variations = []\n",
    "# for jmsr, ud in itertools.product([\"JMS\", \"JMR\"], [\"up\", \"down\"]):\n",
    "#     for var in mass_vars:\n",
    "#         mass_variations.append(f\"{var}_{jmsr}_{ud}\")\n",
    "\n",
    "\n",
    "base_columns = [(var, 2) for var in fatjet_vars] + [(\"weight\", 1)]\n",
    "\n",
    "load_columns_pt_var = []\n",
    "for pt_var in pt_variations:\n",
    "    load_columns_pt_var.append((pt_var, 2))\n",
    "\n",
    "load_columns_mass_var = []\n",
    "for mass_var in mass_variations:\n",
    "    load_columns_mass_var.append((mass_var, 2))\n",
    "\n",
    "load_weight_shifts = []\n",
    "for var, ud in itertools.product(sys_vars, [\"Up\", \"Down\"]):\n",
    "    load_weight_shifts.append((f\"weight_{var}{ud}\", 1))\n",
    "\n",
    "MC_common_extra_columns = load_columns_mass_var + load_columns_pt_var + load_weight_shifts\n",
    "MC_common_extra_columns = []  # TODO: disable variations for now\n",
    "\n",
    "extra_columns_dict = {\n",
    "    \"data\": [],\n",
    "}\n",
    "\n",
    "for sample in MC_SAMPLES_LIST:\n",
    "    extra_columns_dict[sample] = MC_common_extra_columns\n",
    "extra_columns_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00fd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "triggers = {\n",
    "    \"2022\": [\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "    ],\n",
    "    \"2022EE\": [\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "    ],\n",
    "    \"2023\": [\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "        \"AK8PFJet230_SoftDropMass40_PNetBB0p06\",\n",
    "    ],\n",
    "    \"2023BPix\": [\n",
    "        \"AK8PFJet425_SoftDropMass40\",\n",
    "        \"AK8PFJet230_SoftDropMass40_PNetBB0p06\",\n",
    "    ],\n",
    "    \"2024\": [\n",
    "        \"AK8PFJet400_SoftDropMass30\",\n",
    "        \"AK8PFJet425_SoftDropMass30\",\n",
    "        \"AK8PFJet230_SoftDropMass40_PNetBB0p06\",\n",
    "    ],\n",
    "    \"2025\": [\n",
    "        \"AK8PFJet400_SoftDropMass30\",\n",
    "        \"AK8PFJet425_SoftDropMass30\",\n",
    "        \"AK8PFJet230_SoftDropMass40_PNetBB0p06\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_era_path(year):\n",
    "    return PROCESSED_DIR / f\"events_{year}.pkl\"\n",
    "\n",
    "\n",
    "def get_combined_path(combined_year):\n",
    "    return PROCESSED_DIR / f\"events_{combined_year}.pkl\"\n",
    "\n",
    "\n",
    "# Check if all combined years exist\n",
    "all_combined_exist = all(\n",
    "    get_combined_path(combined_year).exists() for combined_year in YEARS_COMBINED_DICT.keys()\n",
    ")\n",
    "\n",
    "if not REPROCESS and all_combined_exist:\n",
    "    # Load all combined years directly\n",
    "    print(\"Loading all combined years...\")\n",
    "    events_combined = {}\n",
    "    for combined_year in YEARS_COMBINED_DICT.keys():\n",
    "        combined_path = get_combined_path(combined_year)\n",
    "        print(f\"Loading combined year {combined_year}...\")\n",
    "        with combined_path.open(\"rb\") as f:\n",
    "            events_combined[combined_year] = pd.read_pickle(f)\n",
    "    print(\"All combined years loaded!\")\n",
    "\n",
    "else:\n",
    "    # ============================================================================\n",
    "    # STEP 1: Process each era individually\n",
    "    # ============================================================================\n",
    "\n",
    "    for year in YEARS:\n",
    "        era_path = get_era_path(year)\n",
    "\n",
    "        if not era_path.exists():\n",
    "            print(f\"Processing era: {year}\")\n",
    "            events_era = {}\n",
    "\n",
    "            # Process each sample for this era\n",
    "            for sample, sample_list in SAMPLES_DICT.items():\n",
    "                print(f\"Loading {sample} for {year}...\")\n",
    "                triggers_cols = [(trigger, 1) for trigger in triggers[year]]\n",
    "\n",
    "                columns = triggers_cols + base_columns + extra_columns_dict.get(sample, [])\n",
    "                dataframes = {\n",
    "                    **utils.load_samples(\n",
    "                        data_dir=str(DATA_DIR),\n",
    "                        samples={sample: sample_list},\n",
    "                        year=\"2024\" if (year == \"2025\" and sample != \"data\") else year,\n",
    "                        columns=utils.format_columns(columns),\n",
    "                        variations=True,\n",
    "                        weight_shifts=weight_shifts,\n",
    "                    )\n",
    "                }\n",
    "\n",
    "                # Process and concatenate dataframes for this sample\n",
    "                sample_dfs = []\n",
    "                for key, df in dataframes.items():\n",
    "                    # Scale finalWeight for 2025 MC (which is actually 2024 MC)\n",
    "                    if year == \"2025\" and sample != \"data\":\n",
    "                        df[\"finalWeight\"] = df[\"finalWeight\"] * (\n",
    "                            actual_lumis[\"2025\"] / actual_lumis[\"2024\"]\n",
    "                        )\n",
    "\n",
    "                    sample_dfs.append(df)\n",
    "\n",
    "                # Concatenate all dataframes for this sample\n",
    "                events_era[sample] = pd.concat(sample_dfs, ignore_index=True)\n",
    "                print(f\"  {sample}: {len(events_era[sample])} events\")\n",
    "\n",
    "                # Clear intermediate dataframes to free memory\n",
    "                del dataframes, sample_dfs\n",
    "\n",
    "            # Save this era's data\n",
    "            with era_path.open(\"wb\") as f:\n",
    "                pd.to_pickle(events_era, f)\n",
    "            print(f\"Era {year} saved to {era_path}\")\n",
    "\n",
    "            # Clear era data to free memory\n",
    "            del events_era\n",
    "        else:\n",
    "            print(f\"Era {year} already processed at {era_path}\")\n",
    "\n",
    "    print(\"Individual era processing complete!\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # STEP 2: Combine eras into combined years\n",
    "    # ============================================================================\n",
    "\n",
    "    events_combined = {}\n",
    "\n",
    "    for combined_year, year_list in YEARS_COMBINED_DICT.items():\n",
    "        combined_path = get_combined_path(combined_year)\n",
    "\n",
    "        if REPROCESS or not combined_path.exists():\n",
    "            print(f\"\\nCombining eras for {combined_year}: {year_list}\")\n",
    "\n",
    "            # Load each era\n",
    "            era_data = {}\n",
    "            for year in year_list:\n",
    "                era_path = get_era_path(year)\n",
    "                if era_path.exists():\n",
    "                    print(f\"Loading era {year}...\")\n",
    "                    with era_path.open(\"rb\") as f:\n",
    "                        era_data[year] = pd.read_pickle(f)\n",
    "                else:\n",
    "                    print(f\"Warning: Era file {era_path} not found!\")\n",
    "\n",
    "            # Combine samples across eras\n",
    "            events_combined[combined_year] = {}\n",
    "            for sample in SAMPLES_DICT.keys():\n",
    "                sample_dfs = []\n",
    "                for year in year_list:\n",
    "                    if year in era_data and sample in era_data[year]:\n",
    "                        sample_dfs.append(era_data[year][sample])\n",
    "\n",
    "                if sample_dfs:\n",
    "                    events_combined[combined_year][sample] = pd.concat(\n",
    "                        sample_dfs, ignore_index=True\n",
    "                    )\n",
    "                    total_weights = events_combined[combined_year][sample][\"finalWeight\"].sum()\n",
    "                    total_events = len(events_combined[combined_year][sample])\n",
    "                    print(f\"  {sample}: {total_events} events (total weight: {total_weights})\")\n",
    "\n",
    "            # Save combined year\n",
    "            with combined_path.open(\"wb\") as f:\n",
    "                pd.to_pickle(events_combined[combined_year], f)\n",
    "            print(f\"Combined year {combined_year} saved to {combined_path}\")\n",
    "\n",
    "            # Clear era data to free memory for next iteration\n",
    "            del era_data\n",
    "        else:\n",
    "            # If combined year already exists, load it\n",
    "            print(f\"Combined year {combined_year} already exists, loading...\")\n",
    "            with combined_path.open(\"rb\") as f:\n",
    "                events_combined[combined_year] = pd.read_pickle(f)\n",
    "\n",
    "    print(\"\\nAll processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037cb911",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_sel = {}\n",
    "for year, events_dict in events_combined.items():\n",
    "    print(f\"Year {year} selection\")\n",
    "    events_sel[year] = {}\n",
    "    for sample, df in events_dict.items():\n",
    "        mask = (\n",
    "            (df[(\"bbFatJetPt\", 0)] > 250)\n",
    "            & (df[(\"bbFatJetPt\", 1)] > 250)\n",
    "            & (df[(\"bbFatJetMsd\", 0)] > 40)\n",
    "            & (df[(\"bbFatJetParT3TXbb\", 0)] > 0.3)\n",
    "            & (df[(\"bbFatJetParT3massX2p\", 0)] > 60)\n",
    "            & (df[(\"bbFatJetParT3massX2p\", 1)] > 60)\n",
    "        )\n",
    "        trigger_mask = np.zeros(len(df), dtype=bool)\n",
    "        for trigger in triggers[year.replace(\"All\", \"\")]:\n",
    "            trigger_mask |= df[(trigger, 0)] == 1\n",
    "        total_mask = mask & trigger_mask\n",
    "        total_sel_weights = df[\"finalWeight\"][total_mask].sum()\n",
    "        total_weights = df[\"finalWeight\"].sum()\n",
    "\n",
    "        print(\n",
    "            f\"    {sample}: {total_sel_weights:.2f} / {total_weights:.2f} selected ({100 * total_sel_weights / total_weights:.2f}%)\"\n",
    "        )\n",
    "        events_sel[year][sample] = df[total_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749578aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HH4b.utils import ShapeVar, singleVarHist\n",
    "from HH4b import plotting\n",
    "from HH4b.hh_vars import bg_keys\n",
    "\n",
    "sig_keys = [\"hh4b\", \"vbfhh4b\", \"vbfhh4b-k2v0\"]\n",
    "control_plot_vars = [\n",
    "    ShapeVar(var=\"bbFatJetPhi0\", label=\"Fatjet 0 $\\phi$\", bins=[50, -np.pi, np.pi]),\n",
    "    ShapeVar(var=\"bbFatJetPhi1\", label=\"Fatjet 1 $\\phi$\", bins=[50, -np.pi, np.pi]),\n",
    "    ShapeVar(var=\"bbFatJetParT3TXbb0\", label=r\"FatJet 0 GloParT-v3 $T_{Xbb}$\", bins=[50, 0.3, 1]),\n",
    "    ShapeVar(var=\"bbFatJetParT3TXbb1\", label=r\"FatJet 1 GloParT-v3 $T_{Xbb}$\", bins=[50, 0.0, 1]),\n",
    "    ShapeVar(\n",
    "        var=\"bbFatJetParT3massGeneric0\",\n",
    "        label=\"Fatjet 0 GloParT-v3 Regressed Mass (Generic) [GeV]\",\n",
    "        bins=[50, 50, 150],\n",
    "    ),\n",
    "    ShapeVar(\n",
    "        var=\"bbFatJetParT3massGeneric1\",\n",
    "        label=\"Fatjet 1 GloParT-v3 Regressed Mass (Generic) [GeV]\",\n",
    "        bins=[50, 50, 150],\n",
    "    ),\n",
    "    ShapeVar(\n",
    "        var=\"bbFatJetParT3massX2p0\",\n",
    "        label=\"Fatjet 0 GloParT-v3 Regressed Mass (X2p) [GeV]\",\n",
    "        bins=[50, 60, 150],\n",
    "    ),\n",
    "    ShapeVar(\n",
    "        var=\"bbFatJetParT3massX2p1\",\n",
    "        label=\"Fatjet 1 GloParT-v3 Regressed Mass (X2p) [GeV]\",\n",
    "        bins=[50, 60, 150],\n",
    "    ),\n",
    "    ShapeVar(var=\"bbFatJetPt0\", label=\"Fatjet 0 $p_{T}$ [GeV]\", bins=[50, 250, 2000]),\n",
    "    ShapeVar(var=\"bbFatJetPt1\", label=\"Fatjet 1 $p_{T}$ [GeV]\", bins=[50, 250, 2000]),\n",
    "    ShapeVar(var=\"bbFatJetEta0\", label=\"Fatjet 0 $\\eta$\", bins=[50, -2.5, 2.5]),\n",
    "    ShapeVar(var=\"bbFatJetEta1\", label=\"Fatjet 1 $\\eta$\", bins=[50, -2.5, 2.5]),\n",
    "]\n",
    "\n",
    "\n",
    "for year in YEARS_COMBINED_DICT.keys():\n",
    "    print(f\"Making control plots for {year}...\")\n",
    "    plot_dir = PLOT_DIR / f\"control/{year}\"\n",
    "    plot_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Find the normalization needed to reweight QCD\n",
    "    kwargs = {}\n",
    "\n",
    "    hists = {}\n",
    "    for i, shape_var in enumerate(control_plot_vars):\n",
    "        if shape_var.var not in hists:\n",
    "            hists[shape_var.var] = singleVarHist(\n",
    "                events_sel[year],\n",
    "                shape_var,\n",
    "                weight_key=\"finalWeight\",\n",
    "            )\n",
    "\n",
    "            qcd_norm = plotting.ratioHistPlot(\n",
    "                hists[shape_var.var],\n",
    "                year,\n",
    "                sig_keys,\n",
    "                bg_keys,\n",
    "                name=str(plot_dir / f\"{shape_var.var}\"),\n",
    "                show=False,\n",
    "                log=True,\n",
    "                plot_significance=False,\n",
    "                significance_dir=shape_var.significance_dir,\n",
    "                ratio_ylims=[0.2, 1.8],\n",
    "                bg_err_mcstat=True,\n",
    "                reweight_qcd=True,\n",
    "                xbin_gev=\"[GeV]\" in shape_var.label,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            # pick the normalization weight chosen for the first variable\n",
    "            if i == 0:\n",
    "                kwargs[\"qcd_norm\"] = qcd_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4989e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh4b-nanov15-bdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
