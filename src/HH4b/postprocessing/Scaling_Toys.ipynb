{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36408410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from HH4b import hh_vars, plotting, postprocessing, run_utils\n",
    "from HH4b.hh_vars import (\n",
    "    bg_keys,\n",
    "    mreg_strings,\n",
    "    samples_run3,\n",
    "    ttbarsfs_decorr_ggfbdt_bins,\n",
    "    ttbarsfs_decorr_txbb_bins,\n",
    "    ttbarsfs_decorr_vbfbdt_bins,\n",
    "    txbb_strings,\n",
    "    txbbsfs_decorr_pt_bins,\n",
    "    txbbsfs_decorr_txbb_wps,\n",
    ")\n",
    "from HH4b.postprocessing import (\n",
    "    Region,\n",
    "    combine_run3_samples,\n",
    "    corrections,\n",
    "    get_weight_shifts,\n",
    "    load_run3_samples,\n",
    ")\n",
    "from HH4b.utils import (\n",
    "    ShapeVar,\n",
    "    check_get_jec_var,\n",
    "    discretize_var,\n",
    "    get_var_mapping,\n",
    "    singleVarHist,\n",
    ")\n",
    "\n",
    "from HH4b.postprocessing.PostProcess import load_process_run3_samples\n",
    "\n",
    "mass_axis = hist.axis.Regular(16, 60, 220, name=\"mass\")\n",
    "bdt_bins = 100\n",
    "bdt_axis = hist.axis.Regular(bdt_bins, 0, 1, name=\"bdt\")\n",
    "xbb_bins = 100\n",
    "xbb_axis = hist.axis.Regular(xbb_bins, 0, 1, name=\"xbb\")\n",
    "diff_axis = hist.axis.Regular(50, -2, 2, name=\"diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b5400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_from_hist(h_hist, n_samples, rng):\n",
    "    \"\"\"\n",
    "    Get random values drawn from histogram\n",
    "    \"\"\"\n",
    "    h, bins = h_hist.to_numpy()\n",
    "\n",
    "    bin_midpoints = bins[:-1] + np.diff(bins) / 2\n",
    "    cdf = np.cumsum(h)\n",
    "    cdf = cdf / cdf[-1]\n",
    "    values = rng.random(n_samples)  # noqa: NPY002\n",
    "    value_bins = np.searchsorted(cdf, values)\n",
    "    random_from_cdf = bin_midpoints[value_bins]\n",
    "    return random_from_cdf\n",
    "\n",
    "\n",
    "def get_toy_from_3d_hist(h_hist, n_samples, rng):\n",
    "    \"\"\"\n",
    "    Get random values drawn from histogram\n",
    "    \"\"\"\n",
    "    h, x_bins, y_bins, z_bins = h_hist.to_numpy()\n",
    "\n",
    "    x_bin_midpoints = x_bins[:-1] + np.diff(x_bins) / 2\n",
    "    y_bin_midpoints = y_bins[:-1] + np.diff(y_bins) / 2\n",
    "    z_bin_midpoints = z_bins[:-1] + np.diff(z_bins) / 2\n",
    "    cdf = np.cumsum(h.ravel())\n",
    "    cdf = cdf / cdf[-1]\n",
    "    values = rng.random(n_samples)  # noqa: NPY002\n",
    "    value_bins = np.searchsorted(cdf, values)\n",
    "    x_idx, y_idx, z_idx = np.unravel_index(\n",
    "        value_bins, (len(x_bin_midpoints), len(y_bin_midpoints), len(z_bin_midpoints))\n",
    "    )\n",
    "    random_from_cdf = np.column_stack(\n",
    "        (x_bin_midpoints[x_idx], y_bin_midpoints[y_idx], z_bin_midpoints[z_idx])\n",
    "    )\n",
    "\n",
    "    return random_from_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf366a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    templates_tag=\"25June2ReRunBDTZbbSFs384Check\",\n",
    "    data_dir=\"/ceph/cms/store/user/dprimosc/bbbb/skimmer/\",\n",
    "    mass_bins=10,\n",
    "    tag=\"25May9_v12v2_private_signal\",\n",
    "    years=[\"2022\", \"2022EE\", \"2023\", \"2023BPix\"],\n",
    "    training_years=None,\n",
    "    mass=\"H2PNetMass\",\n",
    "    bdt_model=\"25Feb5_v13_glopartv2_rawmass\",\n",
    "    bdt_config=\"v13_glopartv2\",\n",
    "    txbb=\"glopart-v2\",\n",
    "    txbb_wps=[0.945, 0.85],\n",
    "    bdt_wps=[0.94, 0.755, 0.03],\n",
    "    method=\"abcd\",\n",
    "    vbf_txbb_wp=0.8,\n",
    "    vbf_bdt_wp=0.9825,\n",
    "    weight_ttbar_bdt=1.0,\n",
    "    # sig_keys=['hh4b', 'hh4b-kl0', 'hh4b-kl2p45', 'hh4b-kl5', 'vbfhh4b', 'vbfhh4b-k2v0', 'vbfhh4b-kv1p74-k2v1p37-kl14p4', 'vbfhh4b-kvm0p012-k2v0p03-kl10p2', 'vbfhh4b-kvm0p758-k2v1p44-klm19p3', 'vbfhh4b-kvm0p962-k2v0p959-klm1p43', 'vbfhh4b-kvm1p21-k2v1p94-klm0p94', 'vbfhh4b-kvm1p6-k2v2p72-klm1p36', 'vbfhh4b-kvm1p83-k2v3p57-klm3p39', 'vbfhh4b-kvm2p12-k2v3p87-klm5p96'],\n",
    "    sig_keys=[\"hh4b\"],\n",
    "    pt_first=300.0,\n",
    "    pt_second=250.0,\n",
    "    fom_vbf_samples=[\"vbfhh4b-k2v0\"],\n",
    "    fom_ggf_samples=[\"hh4b\"],\n",
    "    bdt_disc=True,\n",
    "    event_list=False,\n",
    "    event_list_dir=\"event_lists\",\n",
    "    bdt_roc=False,\n",
    "    control_plots=False,\n",
    "    fom_scan=False,\n",
    "    fom_scan_bin1=True,\n",
    "    fom_scan_bin2=True,\n",
    "    fom_scan_vbf=False,\n",
    "    templates=False,\n",
    "    vbf=True,\n",
    "    vbf_priority=False,\n",
    "    correct_vbf_bdt_shape=True,\n",
    "    blind=True,\n",
    "    rerun_inference=True,\n",
    "    scale_smear=False,\n",
    "    dummy_txbb_sfs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fom_window_by_mass = {\"H2PNetMass\": [110, 155]}\n",
    "blind_window_by_mass = {\"H2PNetMass\": [110, 140]}\n",
    "mass_window = np.array(fom_window_by_mass[args.mass])\n",
    "n_mass_bins = int((220 - 60) / args.mass_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c1828",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dict_postprocess = {}\n",
    "cutflows = {}\n",
    "from pathlib import Path\n",
    "\n",
    "# modify samples run3\n",
    "for year in samples_run3:\n",
    "    samples_run3[year][\"qcd\"] = [\n",
    "        \"QCD_HT-1000to1200\",\n",
    "        \"QCD_HT-1200to1500\",\n",
    "        \"QCD_HT-1500to2000\",\n",
    "        \"QCD_HT-2000\",\n",
    "        # \"QCD_HT-200to400\",\n",
    "        \"QCD_HT-400to600\",\n",
    "        \"QCD_HT-600to800\",\n",
    "        \"QCD_HT-800to1000\",\n",
    "    ]\n",
    "    for key in list(samples_run3[year]):\n",
    "        if \"hh4b\" in key and key != \"hh4b\":\n",
    "            del samples_run3[year][key]\n",
    "    print(samples_run3[year])\n",
    "\n",
    "# get top-level HH4b directory\n",
    "HH4B_DIR = \"/home/users/woodson/HH4b/\"\n",
    "plot_dir = Path(f\"{HH4B_DIR}/plots/PostProcess/{args.templates_tag}\")\n",
    "plot_dir.mkdir(exist_ok=True, parents=True)\n",
    "for year in args.years:\n",
    "    print(f\"\\n{year}\")\n",
    "    events, cutflow = load_process_run3_samples(\n",
    "        args,\n",
    "        year,\n",
    "        [],\n",
    "        args.control_plots,\n",
    "        plot_dir,\n",
    "        mass_window,\n",
    "        args.rerun_inference,\n",
    "    )\n",
    "    events_dict_postprocess[year] = events\n",
    "    cutflows[year] = cutflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processes = [\"data\"] + args.sig_keys + bg_keys\n",
    "bg_keys_combined = bg_keys.copy()\n",
    "if not args.control_plots and not args.bdt_roc:\n",
    "    if \"qcd\" in processes:\n",
    "        processes.remove(\"qcd\")\n",
    "    if \"qcd\" in bg_keys:\n",
    "        bg_keys.remove(\"qcd\")\n",
    "    if \"qcd\" in bg_keys_combined:\n",
    "        bg_keys_combined.remove(\"qcd\")\n",
    "\n",
    "if len(args.years) > 1:\n",
    "    # list of years available for a given process to scale to full lumi,\n",
    "    scaled_by_years = {\n",
    "        # \"zz\": [\"2022\", \"2022EE\", \"2023\"],\n",
    "    }\n",
    "    events_combined, scaled_by = combine_run3_samples(\n",
    "        events_dict_postprocess,\n",
    "        processes,\n",
    "        bg_keys=bg_keys_combined,\n",
    "        scale_processes=scaled_by_years,\n",
    "        years_run3=args.years,\n",
    "    )\n",
    "    print(\"Combined years\")\n",
    "else:\n",
    "    events_combined = events_dict_postprocess[args.years[0]]\n",
    "    scaled_by = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save events combined dictionary to a pickle file\n",
    "import pickle\n",
    "with open(f\"{HH4B_DIR}/data/events_combined_{args.templates_tag}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(events_combined, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b68cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_histograms(mass_array, xbb_array, bdt_array):\n",
    "    mask = (mass_array > 150) | (mass_array < 110)\n",
    "    h_mass = hist.Hist(mass_axis)\n",
    "    h_mass.fill(mass_array[mask])\n",
    "    h_xbb = hist.Hist(xbb_axis)\n",
    "    h_xbb.fill(xbb_array[mask])\n",
    "    h_bdt = hist.Hist(bdt_axis)\n",
    "    h_bdt.fill(bdt_array[mask])\n",
    "\n",
    "    # sample toys from 3D distribution\n",
    "    h_mass_xbb_bdt = hist.Hist(mass_axis, xbb_axis, bdt_axis)\n",
    "    h_mass_xbb_bdt.fill(\n",
    "        mass=mass_array[mask],\n",
    "        xbb=xbb_array[mask],\n",
    "        bdt=bdt_array[mask],\n",
    "    )\n",
    "\n",
    "    # make 2D histograms\n",
    "    h_mass_xbb = hist.Hist(mass_axis, xbb_axis)\n",
    "    h_mass_xbb.fill(\n",
    "        mass=mass_array[mask],\n",
    "        xbb=xbb_array[mask],\n",
    "    )\n",
    "    h_mass_bdt = hist.Hist(mass_axis, bdt_axis)\n",
    "    h_mass_bdt.fill(\n",
    "        mass=mass_array[mask],\n",
    "        bdt=bdt_array[mask],\n",
    "    )\n",
    "    h_xbb_bdt = hist.Hist(xbb_axis, bdt_axis)\n",
    "    h_xbb_bdt.fill(\n",
    "        xbb=xbb_array[mask],\n",
    "        bdt=bdt_array[mask],\n",
    "    )\n",
    "\n",
    "    return h_mass, h_xbb, h_bdt, h_mass_xbb, h_mass_bdt, h_xbb_bdt, h_mass_xbb_bdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16093cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1D distributions\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "def plot_corner(h_mass, h_xbb, h_bdt, h_mass_xbb, h_mass_bdt, h_xbb_bdt):\n",
    "    fig, ax = plt.subplots(3, 3, figsize=(20, 20))\n",
    "    hep.histplot(h_mass, ax=ax[0, 0])\n",
    "    hep.histplot(h_xbb, ax=ax[1, 1])\n",
    "    hep.histplot(h_bdt, ax=ax[2, 2])\n",
    "    ax[0, 0].set_xlim(60, 220)\n",
    "    ax[1, 1].set_xlim(0, 1)\n",
    "    ax[1, 1].set_ylim(5e-1, 5e6)\n",
    "    ax[2, 2].set_xlim(0, 1)\n",
    "    ax[1, 1].set_yscale(\"log\")\n",
    "    ax[2, 2].set_yscale(\"log\")\n",
    "    hep.hist2dplot(h_mass_xbb, ax=ax[1, 0], norm=mpl.colors.LogNorm())\n",
    "    hep.hist2dplot(h_mass_bdt, ax=ax[2, 0], norm=mpl.colors.LogNorm())\n",
    "    hep.hist2dplot(h_xbb_bdt, ax=ax[2, 1], norm=mpl.colors.LogNorm())\n",
    "    ax[0, 1].axis(\"off\")\n",
    "    ax[0, 2].axis(\"off\")\n",
    "    ax[1, 2].axis(\"off\")\n",
    "    # tight layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"corners.pdf\", bbox_inches=\"tight\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_mass, h_xbb, h_bdt, h_mass_xbb, h_mass_bdt, h_xbb_bdt, h_mass_xbb_bdt = make_histograms(\n",
    "    events_combined[\"data\"][\"H2PNetMass\"],\n",
    "    events_combined[\"data\"][\"H2TXbb\"],\n",
    "    events_combined[\"data\"][\"bdt_score\"],\n",
    ")\n",
    "integral = np.sum(h_mass_xbb_bdt.values())\n",
    "\n",
    "fig = plot_corner(h_mass, h_xbb, h_bdt, h_mass_xbb, h_mass_bdt, h_xbb_bdt)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ce47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gaussian_kde from 3D distribution\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def logit(x):\n",
    "    \"\"\"Logit function.\"\"\"\n",
    "    return np.log(x / (1 - x))\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def minuit_transform(x, xmin=0, xmax=1):\n",
    "    \"\"\"Minuit transform: See https://root.cern.ch/download/minuit.pdf#page=8\"\"\"\n",
    "    return np.arcsin(2 * (x - xmin) / (xmax - xmin) - 1)\n",
    "def minuit_inverse_transform(x, xmin=0, xmax=1):\n",
    "    \"\"\"Inverse Minuit transform: See https://root.cern.ch/download/minuit.pdf#page=8\"\"\"\n",
    "    return (np.sin(x) + 1) * (xmax - xmin) / 2 + xmin\n",
    "\n",
    "data_array = events_combined[\"data\"][[\"H2PNetMass\", \"H2TXbb\", \"bdt_score\"]].to_numpy()\n",
    "transformed_data_array = np.column_stack(\n",
    "    (\n",
    "        minuit_transform(data_array[:, 0], xmin=60, xmax=220),\n",
    "        logit(data_array[:, 1]), \n",
    "        logit(data_array[:, 2]),\n",
    "    )\n",
    ")\n",
    "kde_3d = gaussian_kde(transformed_data_array.T, bw_method=\"silverman\")\n",
    "kde_2d = gaussian_kde(\n",
    "    transformed_data_array[:, 1:].T, bw_method=\"silverman\"\n",
    ")\n",
    "kde_1d = gaussian_kde(\n",
    "    transformed_data_array[:, 0], bw_method=\"silverman\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from HH4b.postprocessing.PostProcess import abcd, sideband, get_nevents_nosignal\n",
    "import numba as nb\n",
    "\n",
    "\n",
    "def scan_fom(\n",
    "    method: str,\n",
    "    events_combined: pd.DataFrame,\n",
    "    get_cut: Callable,\n",
    "    get_anti_cut: Callable,\n",
    "    xbb_cuts: np.ArrayLike,\n",
    "    bdt_cuts: np.ArrayLike,\n",
    "    mass_window: list[float],\n",
    "    bg_keys: list[str],\n",
    "    sig_keys: list[str],\n",
    "    fom: str = \"2sqrt(b)/s\",\n",
    "    mass: str = \"H2Msd\",\n",
    "):\n",
    "    \"\"\"Generic FoM scan for given region, defined in the ``get_cut`` function.\"\"\"\n",
    "\n",
    "    print(f\"Scanning {fom} with {method}\")\n",
    "    all_s = []\n",
    "    all_b = []\n",
    "    all_sideband_events = []\n",
    "    all_xbb_cuts = []\n",
    "    all_bdt_cuts = []\n",
    "    all_fom = []\n",
    "    for xbb_cut in xbb_cuts:\n",
    "        for bdt_cut in bdt_cuts:\n",
    "            if method == \"abcd\":\n",
    "                nevents_sig, nevents_bkg, _ = abcd(\n",
    "                    events_combined,\n",
    "                    get_cut,\n",
    "                    get_anti_cut,\n",
    "                    xbb_cut,\n",
    "                    bdt_cut,\n",
    "                    mass,\n",
    "                    mass_window,\n",
    "                    bg_keys,\n",
    "                    sig_keys,\n",
    "                )\n",
    "            else:\n",
    "                nevents_sig, nevents_bkg, _ = sideband(\n",
    "                    events_combined, get_cut, xbb_cut, bdt_cut, mass, mass_window, sig_keys\n",
    "                )\n",
    "\n",
    "            # number of events in data in sideband\n",
    "            cut = get_cut(events_combined[\"data\"], xbb_cut, bdt_cut)\n",
    "            nevents_sideband = get_nevents_nosignal(events_combined[\"data\"], cut, mass, mass_window)\n",
    "\n",
    "            if fom == \"s/sqrt(s+b)\":\n",
    "                if nevents_sig + nevents_bkg > 0:\n",
    "                    figure_of_merit = nevents_sig / np.sqrt(nevents_sig + nevents_bkg)\n",
    "                else:\n",
    "                    figure_of_merit = np.nan\n",
    "            elif fom == \"2sqrt(b)/s\":\n",
    "                if nevents_bkg > 0 and nevents_sig > 0:\n",
    "                    figure_of_merit = 2 * np.sqrt(nevents_bkg) / nevents_sig\n",
    "                else:\n",
    "                    figure_of_merit = np.nan\n",
    "            else:\n",
    "                raise ValueError(\"Invalid FOM\")\n",
    "\n",
    "            all_b.append(nevents_bkg)\n",
    "            all_s.append(nevents_sig)\n",
    "            all_sideband_events.append(nevents_sideband)\n",
    "            all_xbb_cuts.append(xbb_cut)\n",
    "            all_bdt_cuts.append(bdt_cut)\n",
    "            all_fom.append(figure_of_merit)\n",
    "\n",
    "    all_fom = np.array(all_fom)\n",
    "    all_b = np.array(all_b)\n",
    "    all_s = np.array(all_s)\n",
    "    all_sideband_events = np.array(all_sideband_events)\n",
    "    all_xbb_cuts = np.array(all_xbb_cuts)\n",
    "    all_bdt_cuts = np.array(all_bdt_cuts)\n",
    "\n",
    "    return all_fom, all_b, all_s, all_sideband_events, all_xbb_cuts, all_bdt_cuts\n",
    "\n",
    "\n",
    "def get_optimal_cuts(all_fom, all_b, all_s, all_sideband_events, all_xbb_cuts, all_bdt_cuts):\n",
    "\n",
    "    bdt_cuts = np.sort(np.unique(all_bdt_cuts))\n",
    "    xbb_cuts = np.sort(np.unique(all_xbb_cuts))\n",
    "\n",
    "    h_sb = hist.Hist(\n",
    "        hist.axis.Variable(list(bdt_cuts), name=\"bdt_cut\"),\n",
    "        hist.axis.Variable(list(xbb_cuts), name=\"xbb_cut\"),\n",
    "    )\n",
    "    h_s = hist.Hist(\n",
    "        hist.axis.Variable(list(bdt_cuts), name=\"bdt_cut\"),\n",
    "        hist.axis.Variable(list(xbb_cuts), name=\"xbb_cut\"),\n",
    "    )\n",
    "    h_b = hist.Hist(\n",
    "        hist.axis.Variable(list(bdt_cuts), name=\"bdt_cut\"),\n",
    "        hist.axis.Variable(list(xbb_cuts), name=\"xbb_cut\"),\n",
    "    )\n",
    "\n",
    "    for xbb_cut in xbb_cuts:\n",
    "        for bdt_cut in bdt_cuts:\n",
    "            # find index of this cut\n",
    "            idx = np.where((all_bdt_cuts == bdt_cut) & (all_xbb_cuts == xbb_cut))[0][0]\n",
    "            if all_s[idx] > 0.5 and all_b[idx] >= 2 and all_sideband_events[idx] >= 12:\n",
    "                h_sb.fill(bdt_cut, xbb_cut, weight=all_fom[idx])\n",
    "                h_b.fill(bdt_cut, xbb_cut, weight=all_b[idx])\n",
    "                h_s.fill(bdt_cut, xbb_cut, weight=all_s[idx])\n",
    "\n",
    "    masked_h_sb = np.ma.masked_equal(h_sb.values(), 0)\n",
    "\n",
    "    global_min = np.min(masked_h_sb)\n",
    "\n",
    "    if np.ma.is_masked(global_min):\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    masked_h_sb_min_diff = np.abs(masked_h_sb - global_min)\n",
    "\n",
    "    argmin_axis0 = np.argmin(masked_h_sb_min_diff, axis=0)\n",
    "    min_axis0 = np.min(masked_h_sb_min_diff, axis=0)\n",
    "\n",
    "    argmin_axis1 = np.argmin(masked_h_sb_min_diff, axis=1)\n",
    "    min_axis1 = np.min(masked_h_sb_min_diff, axis=1)\n",
    "\n",
    "    bdt_cut = h_sb.axes[0].edges[argmin_axis0[min_axis0 == 0]][0]\n",
    "    xbb_cut = h_sb.axes[1].edges[argmin_axis1[min_axis1 == 0]][0]\n",
    "\n",
    "    b = h_b.values()[argmin_axis0[min_axis0 == 0], argmin_axis1[min_axis1 == 0]][0]\n",
    "    s = h_s.values()[argmin_axis0[min_axis0 == 0], argmin_axis1[min_axis1 == 0]][0]\n",
    "\n",
    "    return global_min, bdt_cut, xbb_cut, h_sb, b, s\n",
    "\n",
    "\n",
    "def get_anti_cuts():\n",
    "\n",
    "    def anti_cut_ggf(events):\n",
    "        cut_xbb = events[\"H2TXbb\"] < 0.3\n",
    "        cut_bdt = events[\"bdt_score\"] < 0.6\n",
    "        return cut_xbb & cut_bdt\n",
    "\n",
    "    return anti_cut_ggf\n",
    "\n",
    "\n",
    "def get_cuts():\n",
    "\n",
    "    # bin 1 region\n",
    "    def get_cut_bin1(events, xbb_cut, bdt_cut):\n",
    "        cut_xbb = events[\"H2TXbb\"] > xbb_cut\n",
    "        cut_bdt = events[\"bdt_score\"] > bdt_cut\n",
    "        return cut_xbb & cut_bdt\n",
    "\n",
    "    return get_cut_bin1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b303a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_toys(ntoys, lumi_scale=1.0, method=\"2dkde\"):\n",
    "\n",
    "    bdt_cut_toys = []\n",
    "    xbb_cut_toys = []\n",
    "    s_toys = []\n",
    "    b_toys = []\n",
    "    fom_toys = []\n",
    "\n",
    "    # get numpy generator for reproducibility\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    for itoy in range(ntoys):\n",
    "        n_samples = rng.poisson(integral * lumi_scale)  # noqa: NPY002\n",
    "\n",
    "        if method == \"3dhist\":\n",
    "            mass_xbb_bdt_toy = get_toy_from_3d_hist(h_mass_xbb_bdt, n_samples, rng)\n",
    "            mass_toy = mass_xbb_bdt_toy[:, 0]\n",
    "            xbb_toy = mass_xbb_bdt_toy[:, 1]\n",
    "            bdt_toy = mass_xbb_bdt_toy[:, 2]\n",
    "        elif method == \"1dhist\":\n",
    "            mass_toy = get_toy_from_hist(h_mass, n_samples, rng)\n",
    "            xbb_toy = get_toy_from_hist(h_xbb, n_samples, rng)\n",
    "            bdt_toy = get_toy_from_hist(h_bdt, n_samples, rng)\n",
    "        elif method == \"3dkde\":\n",
    "            sampled_transformed_data = kde_3d.resample(n_samples, seed=rng).T\n",
    "            mass_toy = minuit_inverse_transform(sampled_transformed_data[:, 0], xmin=60, xmax=220)\n",
    "            xbb_toy = sigmoid(sampled_transformed_data[:, 1])\n",
    "            bdt_toy = sigmoid(sampled_transformed_data[:, 2])\n",
    "        elif method == \"2dkde\":\n",
    "            sampled_transformed_data = kde_2d.resample(n_samples, seed=rng).T\n",
    "            xbb_toy = sigmoid(sampled_transformed_data[:, 0])\n",
    "            bdt_toy = sigmoid(sampled_transformed_data[:, 1])\n",
    "            mass_toy = minuit_inverse_transform(\n",
    "                kde_1d.resample(n_samples, seed=rng)[0], xmin=60, xmax=220\n",
    "            )            \n",
    "\n",
    "        events_toy = {}\n",
    "        for key in events_combined:\n",
    "            if key != \"data\":\n",
    "                events_toy[key] = events_combined[key][\n",
    "                    [\"H2PNetMass\", \"bdt_score\", \"H2TXbb\", \"weight\"]\n",
    "                ].copy()\n",
    "                events_toy[key][\"weight\"] *= lumi_scale  # scale by lumi\n",
    "            else:\n",
    "                events_toy[\"data\"] = pd.DataFrame(\n",
    "                    {\n",
    "                        \"H2PNetMass\": mass_toy,\n",
    "                        \"bdt_score\": bdt_toy,\n",
    "                        \"H2TXbb\": xbb_toy,\n",
    "                        \"weight\": np.ones_like(mass_toy),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        all_fom, all_b, all_s, all_sideband_events, all_xbb_cuts, all_bdt_cuts = scan_fom(\n",
    "            args.method,\n",
    "            events_toy,\n",
    "            get_cuts(),\n",
    "            get_anti_cuts(),\n",
    "            np.arange(0.9, 0.999, 0.0025),\n",
    "            np.arange(0.9, 0.999, 0.0025),\n",
    "            mass_window,\n",
    "            bg_keys=bg_keys,\n",
    "            sig_keys=args.fom_ggf_samples,\n",
    "            mass=args.mass,\n",
    "        )\n",
    "\n",
    "        global_min, bdt_cut, xbb_cut, h_sb, b, s = get_optimal_cuts(\n",
    "            all_fom, all_b, all_s, all_sideband_events, all_xbb_cuts, all_bdt_cuts\n",
    "        )\n",
    "        if global_min is None:\n",
    "            print(f\"Skipping toy {itoy} due to no valid cuts found.\")\n",
    "            continue\n",
    "\n",
    "        bdt_cut_toys.append(bdt_cut)\n",
    "        xbb_cut_toys.append(xbb_cut)\n",
    "        s_toys.append(s)\n",
    "        b_toys.append(b)\n",
    "        fom_toys.append(global_min)\n",
    "\n",
    "        print(f\"Toy: {itoy + 1}\")\n",
    "        print(f\"Optimal cuts: bdt_cut={bdt_cut:.4f}, xbb_cut={xbb_cut:.4f}\")\n",
    "        print(\n",
    "            f\"2sqrt(b)/s={global_min:.4f}, b={b:.4f}, s={s:.4f}, s/b={s/b:.4f}, s/sqrt(b)={s/np.sqrt(b):.4f}\"\n",
    "        )\n",
    "\n",
    "    return bdt_cut_toys, xbb_cut_toys, s_toys, b_toys, fom_toys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0560519",
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi_scale = 138.0 / 62.0\n",
    "ntoys = 1\n",
    "bdt_cut_toys, xbb_cut_toys, s_toys, b_toys, fom_toys = run_toys(\n",
    "    ntoys, lumi_scale=lumi_scale, method=\"2dkde\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a82437",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntoys = 1\n",
    "bdt_cut_toys_ls1, xbb_cut_toys_ls1, s_toys_ls1, b_toys_ls1, fom_toys_ls1 = run_toys(\n",
    "    ntoys, lumi_scale=1, method=\"2dkde\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133fd8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fom, all_b, all_s, all_sideband_events, all_xbb_cuts, all_bdt_cuts = scan_fom(\n",
    "    args.method,\n",
    "    events_combined,\n",
    "    get_cuts(),\n",
    "    get_anti_cuts(),\n",
    "    np.arange(0.9, 0.999, 0.0025),\n",
    "    np.arange(0.9, 0.999, 0.0025),\n",
    "    mass_window,\n",
    "    bg_keys=bg_keys,\n",
    "    sig_keys=args.fom_ggf_samples,\n",
    "    mass=args.mass,\n",
    ")\n",
    "\n",
    "global_min, bdt_cut, xbb_cut, h_sb, b, s = get_optimal_cuts(\n",
    "    all_fom, all_b, all_s, all_sideband_events, all_xbb_cuts, all_bdt_cuts\n",
    ")\n",
    "print(f\"Data\")\n",
    "print(f\"Optimal cuts: bdt_cut={bdt_cut:.4f}, xbb_cut={xbb_cut:.4f}\")\n",
    "print(\n",
    "    f\"2sqrt(b)/s={global_min:.4f}, b={b:.4f}, s={s:.4f}, s/b={s/b:.4f}, s/sqrt(b)={s/np.sqrt(b):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    fom_toys,\n",
    "    bins=np.linspace(0, 10, 51),\n",
    "    alpha=0.7,\n",
    "    label=f\"Toys, L=138/fb, median FoM={np.median(fom_toys):.1f}\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.hist(\n",
    "    fom_toys_ls1,\n",
    "    bins=np.linspace(0, 10, 51),\n",
    "    alpha=0.7,\n",
    "    label=f\"Toys, L=62/fb, median FoM={np.median(fom_toys_ls1):.1f}\",\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.axvline(\n",
    "    global_min,\n",
    "    ymin=0,\n",
    "    ymax=0.75,\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Data, L=62/fb, FoM={global_min:.1f}\",\n",
    ")\n",
    "plt.xlabel(r\"Optimal FoM=$2\\sqrt{B}/S$\")\n",
    "plt.ylabel(\"Number of Toys\")\n",
    "# set x-axis limits\n",
    "plt.xlim(0, 10)\n",
    "# get current axes and set y-axis limits\n",
    "plt.ylim(0, 30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c0e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh4b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
