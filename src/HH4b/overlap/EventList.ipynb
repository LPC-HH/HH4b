{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reloads imported files on edits\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from HH4b.postprocessing.PostProcess import load_process_run3_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting Parquet file: /ceph/cms/store/user/cmantill/bbbb/skimmer/24Sep25_v12v2_private_signal/2022/QCD_HT-400to600/parquet/out_1.parquet\n",
      "\n",
      "File Metadata:\n",
      "  Number of Rows: 83\n",
      "  Number of Columns: 256\n",
      "  Number of Row Groups: 1\n",
      "\n",
      "Schema:\n",
      "An error occurred: 'pyarrow._parquet.ParquetSchema' object has no attribute 'num_columns'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def inspect_parquet_file(file_path):\n",
    "    \"\"\"Inspect the contents of a Parquet file.\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nInspecting Parquet file: {file_path}\\n\")\n",
    "\n",
    "        # Open the Parquet file metadata\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "        # Display general file metadata\n",
    "        print(\"File Metadata:\")\n",
    "        print(f\"  Number of Rows: {parquet_file.metadata.num_rows}\")\n",
    "        print(f\"  Number of Columns: {parquet_file.metadata.num_columns}\")\n",
    "        print(f\"  Number of Row Groups: {parquet_file.num_row_groups}\")\n",
    "\n",
    "        # Display schema information\n",
    "        print(\"\\nSchema:\")\n",
    "        for col in range(parquet_file.schema):\n",
    "            print(f\"  - {col}\")\n",
    "\n",
    "        # Display column names\n",
    "        print(\"\\nColumns:\")\n",
    "        for column in parquet_file.schema.names:\n",
    "            print(f\"  - {column}\")\n",
    "\n",
    "        # Optional: Read a sample of the data (first 5 rows)\n",
    "        print(\"\\nPreview of the first 5 rows of data:\")\n",
    "        df = pd.read_parquet(file_path)\n",
    "        print(df.head())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "inspect_parquet_file(\n",
    "    \"/ceph/cms/store/user/cmantill/bbbb/skimmer/24Sep25_v12v2_private_signal/2022/QCD_HT-400to600/parquet/out_1.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"24May24_v12_private_signal\"\n",
    "args = Namespace(\n",
    "    templates_tag=\"24June27\",\n",
    "    data_dir=\"/ceph/cms/store/user/cmantill/bbbb/skimmer/\",\n",
    "    # tag=data_folder,\n",
    "    # years=[\"2022\"],\n",
    "    training_years=None,\n",
    "    mass=\"H2PNetMass\",\n",
    "    bdt_model=\"24May31_lr_0p02_md_8_AK4Away\",\n",
    "    bdt_config=\"24May31_lr_0p02_md_8_AK4Away\",\n",
    "    txbb_wps=[0.975, 0.92],\n",
    "    bdt_wps=[0.98, 0.88, 0.03],\n",
    "    method=\"sideband\",\n",
    "    vbf_txbb_wp=0.95,\n",
    "    vbf_bdt_wp=0.98,\n",
    "    sig_keys=[\"hh4b\", \"vbfhh4b\"],\n",
    "    pt_first=300,\n",
    "    pt_second=250,\n",
    "    bdt_roc=False,\n",
    "    control_plots=False,\n",
    "    fom_scan=False,\n",
    "    fom_scan_bin1=True,\n",
    "    fom_scan_bin2=True,\n",
    "    fom_scan_vbf=False,\n",
    "    templates=False,\n",
    "    legacy=True,\n",
    "    vbf=True,\n",
    "    vbf_priority=False,\n",
    "    weight_ttbar_bdt=1,\n",
    "    blind=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_training_keys = [\"qcd\", \"vbfhh4b-k2v0\", \"hh4b\", \"ttbar\"]\n",
    "mass_window = np.array([105, 150])\n",
    "years = [\"2022\", \"2022EE\", \"2023\", \"2023BPix\"]\n",
    "\n",
    "ev_dicts = []\n",
    "for year in years:\n",
    "    ev_dict, _ = load_process_run3_samples(\n",
    "        args,\n",
    "        year=year,\n",
    "        bdt_training_keys=bdt_training_keys,\n",
    "        control_plots=False,\n",
    "        plot_dir=\"plot_dir\",\n",
    "        mass_window=mass_window,\n",
    "    )\n",
    "    ev_dicts.append((year, ev_dict))\n",
    "\n",
    "\"\"\"\n",
    "python3 PostProcess.py --templates-tag 24June27 --tag 24May24_v12_private_signal --mass H2PNetMass --legacy --bdt-config 24May31_lr_0p02_md_8_AK4Away --bdt-model 24May31_lr_0p02_md_8_AK4Away --txbb-wps 0.975 0.92 --bdt-wps 0.98 0.88 0.03 --vbf-txbb-wp 0.95 --vbf-bdt-wp 0.98 --no-bdt-roc --no-fom-scan --no-fom-scan-bin2 --no-fom-scan-bin1 --data-dir /ceph/cms/store/user/cmantill/bbbb/skimmer/ --method abcd --no-vbf-priority --vbf --no-fom-scan-vbf --pt-second 250 --templates --years 2022 --sig-keys hh4b vbfhh4b\n",
    "\"\"\"\n",
    "# make array with event nr, signal category\n",
    "# make mask with signal category cutoffs, apply to array\n",
    "\n",
    "# save as root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# select columns to extract for eventlist\n",
    "eventlist_dict = [\n",
    "    \"event\",\n",
    "    \"bdt_score\",\n",
    "    \"bdt_score_vbf\",\n",
    "    \"H2TXbb\",\n",
    "    \"H2Msd\",\n",
    "    \"run\",\n",
    "    \"H2PNetMass\",\n",
    "    \"luminosityBlock\",\n",
    "]\n",
    "keys_to_save = [\"hh4b\", \"vbfhh4b\", \"data\"]\n",
    "\n",
    "# Ensure the eventlist folder exists\n",
    "eventlist_folder = \"eventlist_files_2025Jan6\"\n",
    "os.makedirs(eventlist_folder, exist_ok=True)\n",
    "\n",
    "# TODO: check how data is loaded (should reference the folder names somewhere)\n",
    "# TODO: print eventlist_dict to see if you can get keys \"data\" and vbfhh4b\n",
    "# Loop over all years and save event lists for each year in separate root files\n",
    "for year, ev_dict in ev_dicts:\n",
    "    for key in ev_dict.keys():\n",
    "        if \"data\" in key or \"hh4b\" in key or \"vbfhh4b\" in key:\n",
    "            print(key)\n",
    "            tree_df = ev_dict[key]\n",
    "            event_list = tree_df[eventlist_dict]\n",
    "            array_to_save = {col: event_list[col].to_numpy() for col in event_list.columns}\n",
    "\n",
    "            # Define the ROOT file path\n",
    "            file_path = f\"{eventlist_folder}/eventlist_boostedHH4b_{year}.root\"\n",
    "\n",
    "            # Check if the ROOT file already exists\n",
    "            if os.path.exists(file_path):\n",
    "                # File exists, use update mode to append the new tree\n",
    "                with uproot.update(file_path) as file:\n",
    "                    file[key] = array_to_save  # Append new tree\n",
    "            else:\n",
    "                # File doesn't exist, create a new one\n",
    "                with uproot.recreate(file_path) as file:\n",
    "                    file[key] = array_to_save  # Create the first tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-check file contents\n",
    "years = [\"2022\", \"2022EE\", \"2023\", \"2023BPix\"]\n",
    "dfs_from_root = {}\n",
    "\n",
    "for year in years:\n",
    "    # Define the ROOT file path\n",
    "    file_path = f\"{eventlist_folder}/eventlist_boostedHH4b_{year}.root\"\n",
    "\n",
    "    with uproot.open(file_path) as file:\n",
    "        for key in file.keys():\n",
    "            tree = file[key]\n",
    "            arrays = tree.arrays(library=\"np\")\n",
    "            df = pd.DataFrame(arrays)\n",
    "\n",
    "            # Make dict\n",
    "            dfs_from_root[(year, key)] = df\n",
    "\n",
    "# test_df = dfs_from_root[\"2022\"] - dfs_from_root[\"2022EE\"]\n",
    "# print(test_df)\n",
    "# Display df\n",
    "for (year, key), df in dfs_from_root.items():\n",
    "    print(f\"DataFrame for year {year}, tree: '{key}':\\n{df}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh4b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
