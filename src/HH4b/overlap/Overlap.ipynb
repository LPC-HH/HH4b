{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from HH4b.utils import load_samples, format_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load samples from Resolved group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Documents/UCSD/Research/HH4b/GluGlutoHHto4B_kl-1p00_kt-1p00_c2-0p00_TuneCP5_13p6TeV_powheg-pythia8_tree.root\n",
      "Number of raw events:  2377354\n",
      "Number of resolved dataframe entries:  2377354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/mambaforge/envs/hh4b/lib/python3.9/site-packages/uproot/interpretation/library.py:747: FutureWarning: MultiIndex.is_integer is deprecated. Use pandas.api.types.is_integer_dtype instead.\n",
      "  if hasattr(index, \"is_integer\") and index.is_integer():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of boosted dataframe entries:  571933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w8/jgj1y4sx0mg8849sgcwfdfgh0000gn/T/ipykernel_18597/2844448702.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"dHiggsDeltaRegMass\"] = np.sqrt(\n"
     ]
    }
   ],
   "source": [
    "# dir = \"../../../../data/overlap/Main_PNet_MinDiag_w4j35_w2bj30_dHHjw30_withoutSyst_25April2024_2022_0L/mc/parts/\"\n",
    "dir = \"/Users/daniel/Documents/UCSD/Research/HH4b\"\n",
    "samples = {\n",
    "    \"hh4b\": \"GluGlutoHHto4B_kl-1p00_kt-1p00_c2-0p00_TuneCP5_13p6TeV_powheg-pythia8_tree.root\",\n",
    "}\n",
    "\n",
    "ak8_columns = [\n",
    "    \"ak8_pt\",\n",
    "    \"ak8_eta\",\n",
    "    \"ak8_phi\",\n",
    "    \"ak8_jetId\",\n",
    "    \"ak8_msoftdrop\",\n",
    "    \"ak8_mass\",\n",
    "    \"ak8_tau3\",\n",
    "    \"ak8_tau2\",\n",
    "    \"ak8_Txbb\",\n",
    "    \"ak8_PQCDb\",\n",
    "    \"ak8_PQCDbb\",\n",
    "    \"ak8_PQCDothers\",\n",
    "    \"ak8_particleNet_mass\",\n",
    "]\n",
    "\n",
    "columns_to_load_resolved = [\n",
    "    \"passmetfilters\",\n",
    "    \"passjetvetomap\",\n",
    "    \"passTrig_HLT_QuadPFJet70_50_40_35_PFBTagParticleNet_2BTagSum0p65\",\n",
    "    \"passL1unprescaled_HLT_QuadPFJet70_50_40_35_PFBTagParticleNet_2BTagSum0p65\",\n",
    "    \"passTrigObjMatching_HLT_QuadPFJet70_50_40_35_PFBTagParticleNet_2BTagSum0p65\",\n",
    "    \"avgbdisc_twoldgbdiscjets\",\n",
    "    \"alljets_ht\",\n",
    "    \"dHH_NbtagM\",\n",
    "    \"dHH_H1_regmass\",\n",
    "    \"dHH_H2_regmass\",\n",
    "    \"event\",\n",
    "    \"lumiwgt\",  # luminosity in fb 26.6717 for 2022EE\n",
    "    \"xsecWeight\",  #  xsec * 1000 / sum('genEventSumw'), xsec in pb\n",
    "    \"genWeight\",\n",
    "    \"puWeight\",\n",
    "    \"trgSF_HLT_QuadPFJet70_50_40_35_PFBTagParticleNet_2BTagSum0p65_central\",\n",
    "    \"btagSF_central\",\n",
    "    \"passTrig_HLT_AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\",\n",
    "    \"passTrig_HLT_AK8PFJet425_SoftDropMass40\",\n",
    "]\n",
    "columns_to_load_boosted = columns_to_load_resolved + [\n",
    "    \"n_ak8\",\n",
    "    \"ak8_pt\",\n",
    "    \"ak8_eta\",\n",
    "    \"ak8_phi\",\n",
    "    \"ak8_jetId\",\n",
    "    \"ak8_msoftdrop\",\n",
    "    \"ak8_mass\",\n",
    "    \"ak8_tau3\",\n",
    "    \"ak8_tau2\",\n",
    "    \"ak8_Txbb\",\n",
    "    \"ak8_PQCDb\",\n",
    "    \"ak8_PQCDbb\",\n",
    "    \"ak8_PQCDothers\",\n",
    "    \"ak8_particleNet_mass\",\n",
    "    \"pass_resolved_skim\",  # trigger & >=4 jets with some pt cuts and >= 2 bjets above 30 GeV\n",
    "    \"pass_boosted_skim\",  # trigger & >=2  tight AK8 jets with pT > 250 GeV and |eta|<2.4\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "    2b: dHH_NbtagM == 2\n",
    "    4b: dHH_NbtagM == 4\n",
    "    asr_4b: ASR_4b\n",
    "    asr_2b: ASR_2b\n",
    "    acr_4b: ACR_4b\n",
    "    acr_2b: ACR_2b\n",
    "    vsr_4b: VSR_4b\n",
    "    vsr_2b: VSR_2b\n",
    "    vcr_4b: VCR_4b\n",
    "    vcr_2b: VCR_2b\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_resolved_masks(df):\n",
    "    regions = {}\n",
    "    regions[\"RES\"] = (\n",
    "        (df[\"passTrig_HLT_QuadPFJet70_50_40_35_PFBTagParticleNet_2BTagSum0p65\"])\n",
    "        & (df[\"passL1unprescaled_HLT_QuadPFJet70_50_40_35_PFBTagParticleNet_2BTagSum0p65\"])\n",
    "        & (df[\"passTrigObjMatching_HLT_QuadPFJet70_50_40_35_PFBTagParticleNet_2BTagSum0p65\"])\n",
    "        & (df[\"passmetfilters\"])\n",
    "        & (df[\"passjetvetomap\"])\n",
    "        & (df[\"avgbdisc_twoldgbdiscjets\"] > 0.65)\n",
    "        & (df[\"alljets_ht\"] > 0)\n",
    "    )\n",
    "\n",
    "    # Calculate variables\n",
    "    # df['AR_dHM'] = np.sqrt((df['dHH_H1_regmass'] - 125)**2 + (df['dHH_H2_regmass'] - 120)**2)\n",
    "    # df['VR_dHM'] = np.sqrt((df['dHH_H1_regmass'] - 185)**2 + (df['dHH_H2_regmass'] - 182)**2)\n",
    "    # Define additional regions based on these variables\n",
    "    # df['ASR_4b'] = (df['AR_dHM'] < 30) & (df['dHH_NbtagM'] == 4)\n",
    "    # df['ACR_4b'] = (df['AR_dHM'] >= 30) & (df['AR_dHM'] < 55) & (df['dHH_NbtagM'] == 4)\n",
    "    # df['VSR_4b'] = (df['VR_dHM'] < 30) & (df['dHH_NbtagM'] == 4)\n",
    "    # df['VCR_4b'] = (df['VR_dHM'] >= 30) & (df['VR_dHM'] < 55) & (df['dHH_NbtagM'] == 4)\n",
    "\n",
    "    df[\"dHiggsDeltaRegMass\"] = np.sqrt(\n",
    "        ((df[\"dHH_H1_regmass\"] - 125.0) * (df[\"dHH_H1_regmass\"] - 125.0))\n",
    "        + ((df[\"dHH_H2_regmass\"] - 120.0) * (df[\"dHH_H2_regmass\"] - 120.0))\n",
    "    )\n",
    "\n",
    "    regions = {\n",
    "        **regions,\n",
    "        \"RES4b\": (regions[\"RES\"] & (df[\"dHH_NbtagM\"] == 4)),\n",
    "        \"RES4bSR\": (regions[\"RES\"] & (df[\"dHiggsDeltaRegMass\"] < 30.0) & (df[\"dHH_NbtagM\"] == 4)),\n",
    "    }\n",
    "\n",
    "    return regions\n",
    "\n",
    "\n",
    "for sample, sample_file in samples.items():\n",
    "    print(f\"{dir}/{sample_file}\")\n",
    "    tree = uproot.open(f\"{dir}/{sample_file}:Events\")\n",
    "    print(\"Number of raw events: \", len(tree.arrays([\"event\"])[\"event\"]))\n",
    "    #########################################\n",
    "    # Load resolved data as a pandas DataFrame\n",
    "    df = tree.arrays(columns_to_load_resolved, library=\"pd\")\n",
    "    print(\"Number of resolved dataframe entries: \", len(df[\"event\"]))\n",
    "\n",
    "    # Weights\n",
    "    df[\"resolved_weight\"] = (\n",
    "        df[\"lumiwgt\"]\n",
    "        * df[\"xsecWeight\"]\n",
    "        * df[\"genWeight\"]\n",
    "        * df[\"puWeight\"]\n",
    "        * df[\"trgSF_HLT_QuadPFJet70_50_40_35_PFBTagParticleNet_2BTagSum0p65_central\"]\n",
    "        * df[\"btagSF_central\"]\n",
    "    )\n",
    "\n",
    "    # Define resolved regions\n",
    "    regions = get_resolved_masks(df)\n",
    "\n",
    "    # Get resolved yields and counts\n",
    "    resolved_yields = {\n",
    "        region: [np.sum(df[\"resolved_weight\"][region_mask])]\n",
    "        for region, region_mask in regions.items()\n",
    "    }\n",
    "    resolved_counts = {\n",
    "        region: int(df[\"event\"][region_mask].shape[0]) for region, region_mask in regions.items()\n",
    "    }\n",
    "\n",
    "    #########################################\n",
    "    # Load boosted data as a pandas DataFrame\n",
    "    df_b = tree.arrays(columns_to_load_boosted, library=\"pd\")\n",
    "    # Ask for at least 2 ak8 jets in boosted pandas dataframe\n",
    "    df_b = df_b[\n",
    "        (df_b[\"n_ak8\"] >= 2)\n",
    "        & (df_b[\"passmetfilters\"])\n",
    "        & (df_b[\"passjetvetomap\"])\n",
    "        & (\n",
    "            (df_b[\"passTrig_HLT_AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\"])\n",
    "            | (df_b[\"passTrig_HLT_AK8PFJet425_SoftDropMass40\"])\n",
    "        )\n",
    "    ].copy()\n",
    "    print(\"Number of boosted dataframe entries: \", len(df_b[\"event\"]))\n",
    "\n",
    "    # Add weights\n",
    "    df_b[\"boosted_weight\"] = (\n",
    "        df_b[\"lumiwgt\"] * df_b[\"xsecWeight\"] * df_b[\"genWeight\"] * df_b[\"puWeight\"]\n",
    "    )\n",
    "    df_b[\"resolved_weight\"] = (\n",
    "        df_b[\"lumiwgt\"]\n",
    "        * df_b[\"xsecWeight\"]\n",
    "        * df_b[\"genWeight\"]\n",
    "        * df_b[\"puWeight\"]\n",
    "        * df_b[\"trgSF_HLT_QuadPFJet70_50_40_35_PFBTagParticleNet_2BTagSum0p65_central\"]\n",
    "        * df_b[\"btagSF_central\"]\n",
    "    )\n",
    "\n",
    "    # Order jets by fatjet Xbb\n",
    "    df_ak8 = df_b.reset_index()\n",
    "    df_ak8 = df_ak8.sort_values(by=[\"entry\", \"ak8_Txbb\"], ascending=[True, False]).set_index(\n",
    "        [\"entry\", \"subentry\"]\n",
    "    )\n",
    "    subindex = df_ak8.sort_index().index.get_level_values(1)\n",
    "    df_ak8 = df_ak8.reset_index()\n",
    "    df_ak8[\"subentry\"] = subindex\n",
    "    df_ak8 = df_ak8.set_index([\"entry\", \"subentry\"])\n",
    "\n",
    "    # For boosted, yields must be obtained for one of the entries\n",
    "    jet0 = df_ak8.query(\"subentry == 0\")\n",
    "    jet1 = df_ak8.query(\"subentry == 1\")\n",
    "\n",
    "    # Define resolved regions for this dataframe\n",
    "    resolved_regions = {\n",
    "        **{f\"BST-{region}\": region_mask for region, region_mask in get_resolved_masks(jet0).items()}\n",
    "    }\n",
    "\n",
    "    # NOTE!!!: you must use .to_numpy() to get the masks with jet0 otherwise you cannot do an OR\n",
    "    boosted_regions = {\n",
    "        \"BST30060-X0bb08\": (\n",
    "            (jet0[\"ak8_pt\"] >= 300).to_numpy()\n",
    "            & (jet1[\"ak8_pt\"] >= 300).to_numpy()\n",
    "            & (jet0[\"ak8_msoftdrop\"] >= 60).to_numpy()\n",
    "            & (jet1[\"ak8_msoftdrop\"] >= 60).to_numpy()\n",
    "            & (jet0[\"ak8_Txbb\"] >= 0.8).to_numpy()\n",
    "        ),\n",
    "        \"BST25060-X0bb08\": (\n",
    "            (jet0[\"ak8_pt\"] >= 250).to_numpy()\n",
    "            & (jet1[\"ak8_pt\"] >= 250).to_numpy()\n",
    "            & (jet0[\"ak8_msoftdrop\"] >= 60).to_numpy()\n",
    "            & (jet1[\"ak8_msoftdrop\"] >= 60).to_numpy()\n",
    "            & (jet0[\"ak8_Txbb\"] >= 0.8).to_numpy()\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # compute overlap\n",
    "    overlap_regions = {\n",
    "        \"RES4b-BST30060-X0bb08\": (\n",
    "            resolved_regions[\"BST-RES4b\"] & boosted_regions[\"BST30060-X0bb08\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    boosted_yields = {\n",
    "        region: [np.sum(jet0[\"boosted_weight\"][region_mask])]\n",
    "        for region, region_mask in boosted_regions.items()\n",
    "    }\n",
    "    boosted_counts = {\n",
    "        region: int(jet0[\"event\"][region_mask].shape[0])\n",
    "        for region, region_mask in boosted_regions.items()\n",
    "    }\n",
    "\n",
    "    overlap_yields = {\n",
    "        region: [np.sum(jet0[\"resolved_weight\"][region_mask])]\n",
    "        for region, region_mask in overlap_regions.items()\n",
    "    }\n",
    "    overlap_counts = {\n",
    "        region: int(jet0[\"event\"][region_mask].shape[0])\n",
    "        for region, region_mask in overlap_regions.items()\n",
    "    }\n",
    "\n",
    "    # make yields and  counts dataframe\n",
    "    df_yields = pd.DataFrame(\n",
    "        {\n",
    "            \"sample\": \"hh4b\",\n",
    "            **resolved_yields,\n",
    "            **boosted_yields,\n",
    "            **overlap_yields,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_counts = pd.DataFrame(\n",
    "        {\n",
    "            \"sample\": \"hh4b\",\n",
    "            \"all\": int(df[\"event\"].shape[0]),\n",
    "            **resolved_counts,\n",
    "            **boosted_counts,\n",
    "            **overlap_counts,\n",
    "        },\n",
    "        index=[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_yields.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_counts.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.any(df[\"genWeight\"] < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross check yield with boosted ntuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../data/skimmer/24Apr23LegacyLowerThresholds_v12_private_signal/\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../data/skimmer/24Apr23LegacyLowerThresholds_v12_private_signal/2022EE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_dir, samples_dict \u001b[38;5;129;01min\u001b[39;00m sample_dirs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(input_dir)\n\u001b[1;32m     25\u001b[0m     events_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mevents_dict,\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# this function will load files (only the columns selected), apply filters and compute a weight per event\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mload_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43msamples_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvariations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_columns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreorder_legacy_txbb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     36\u001b[0m     }\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhh4b_v12_private\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key)\n",
      "File \u001b[0;32m~/Documents/UCSD/Research/HH4b/src/HH4b/utils.py:310\u001b[0m, in \u001b[0;36mload_samples\u001b[0;34m(data_dir, samples, year, filters, columns, variations, weight_shifts, reorder_legacy_txbb)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03mLoads events with an optional filter.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03mDivides MC samples by the total pre-skimming, to take the acceptance into account.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m Path(data_dir) \u001b[38;5;241m/\u001b[39m year\n\u001b[0;32m--> 310\u001b[0m full_samples_list \u001b[38;5;241m=\u001b[39m \u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# get all directories in data_dir\u001b[39;00m\n\u001b[1;32m    311\u001b[0m events_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# label - key of sample in events_dict\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# selector - string used to select directories to load in for this sample\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../data/skimmer/24Apr23LegacyLowerThresholds_v12_private_signal/2022EE'"
     ]
    }
   ],
   "source": [
    "year = \"2022EE\"\n",
    "\n",
    "sample_dirs = {\n",
    "    f\"../../../../data/skimmer/24Apr23LegacyLowerThresholds_v12_private_signal/\": {\n",
    "        \"hh4b_v12_private\": [\"GluGlutoHHto4B_kl-1p00_kt-1p00_c2-0p00_TuneCP5_13p6TeV\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "load_columns = [\n",
    "    (\"bbFatJetPNetTXbbLegacy\", 2),\n",
    "    (\"bbFatJetPNetMassLegacy\", 2),\n",
    "    (\"bbFatJetPNetTXbb\", 2),\n",
    "    (\"bbFatJetPNetMass\", 2),\n",
    "    (\"bbFatJetMsd\", 2),\n",
    "    (\"bbFatJetPt\", 2),\n",
    "    (\"weight\", 1),\n",
    "    (\"trigger_sf\", 1),\n",
    "    (\"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\", 1),\n",
    "    (\"AK8PFJet425_SoftDropMass40\", 1),\n",
    "]\n",
    "\n",
    "events_dict = {}\n",
    "for input_dir, samples_dict in sample_dirs.items():\n",
    "    print(input_dir)\n",
    "    events_dict = {\n",
    "        **events_dict,\n",
    "        # this function will load files (only the columns selected), apply filters and compute a weight per event\n",
    "        **load_samples(\n",
    "            input_dir,\n",
    "            samples_dict,\n",
    "            year,\n",
    "            variations=False,\n",
    "            columns=format_columns(load_columns),\n",
    "            reorder_legacy_txbb=True,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "for key in [\"hh4b_v12_private\"]:\n",
    "    print(key)\n",
    "    events = events_dict[key]\n",
    "\n",
    "    boosted_mask = (\n",
    "        (events[\"bbFatJetPt\"][0] > 300)\n",
    "        & (events[\"bbFatJetPt\"][1] > 300)\n",
    "        & (events[\"bbFatJetPNetTXbb\"][0] > 0.8)\n",
    "        & (events[\"bbFatJetMsd\"][0] > 60)\n",
    "        & (events[\"bbFatJetMsd\"][1] > 60)\n",
    "        & np.any(\n",
    "            [\n",
    "                events[\"AK8PFJet250_SoftDropMass40_PFAK8ParticleNetBB0p35\"],\n",
    "                events[\"AK8PFJet425_SoftDropMass40\"],\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    print(np.sum(events[\"finalWeight\"][boosted_mask]), len(events[\"finalWeight\"][boosted_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>505.399161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155.528530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134.532464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-399.780456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>532.347110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169186</th>\n",
       "      <td>-219.166446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169187</th>\n",
       "      <td>399.780456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169188</th>\n",
       "      <td>505.399161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169189</th>\n",
       "      <td>437.040691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169190</th>\n",
       "      <td>437.040691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169191 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "0       505.399161\n",
       "1       155.528530\n",
       "2       134.532464\n",
       "3      -399.780456\n",
       "4       532.347110\n",
       "...            ...\n",
       "169186 -219.166446\n",
       "169187  399.780456\n",
       "169188  505.399161\n",
       "169189  437.040691\n",
       "169190  437.040691\n",
       "\n",
       "[169191 rows x 1 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# events[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[16:26:10] /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/dmlc-core/src/io/local_filesys.cc:209: Check failed: allow_null:  LocalFileSystem::Open \"trained_bdt.model\": No such file or directory\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x00000001871e99b4 dmlc::LogMessageFatal::~LogMessageFatal() + 116\n  [bt] (1) 2   libxgboost.dylib                    0x00000001874a3f17 dmlc::io::LocalFileSystem::Open(dmlc::io::URI const&, char const*, bool) + 903\n  [bt] (2) 3   libxgboost.dylib                    0x000000018748fa2c dmlc::Stream::Create(char const*, char const*, bool) + 76\n  [bt] (3) 4   libxgboost.dylib                    0x000000018720a683 XGBoosterLoadModel + 787\n  [bt] (4) 5   libffi.8.dylib                      0x00000001090d8972 ffi_call_unix64 + 82\n  [bt] (5) 6   ???                                 0x0000000301ac7b80 0x0 + 12912982912\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m bdt_model \u001b[38;5;241m=\u001b[39m XGBClassifier()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the model \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mbdt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrained_bdt.model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# prepare data\u001b[39;00m\n\u001b[1;32m     17\u001b[0m dtest \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(df[df_ak8]) \n",
      "File \u001b[0;32m~/mambaforge/envs/hh4b/lib/python3.9/site-packages/xgboost/sklearn.py:777\u001b[0m, in \u001b[0;36mXGBModel.load_model\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Booster\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m Booster({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs})\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m meta_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_booster()\u001b[38;5;241m.\u001b[39mattr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit_learn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta_str \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# FIXME(jiaming): This doesn't have to be a problem as most of the needed\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# information like num_class and objective is in Learner class.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/hh4b/lib/python3.9/site-packages/xgboost/core.py:2441\u001b[0m, in \u001b[0;36mBooster.load_model\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, (\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[1;32m   2438\u001b[0m     \u001b[38;5;66;03m# assume file name, cannot use os.path.exist to check, file can be\u001b[39;00m\n\u001b[1;32m   2439\u001b[0m     \u001b[38;5;66;03m# from URL.\u001b[39;00m\n\u001b[1;32m   2440\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname))\n\u001b[0;32m-> 2441\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterLoadModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2442\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2443\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mbytearray\u001b[39m):\n\u001b[1;32m   2444\u001b[0m     buf \u001b[38;5;241m=\u001b[39m fname\n",
      "File \u001b[0;32m~/mambaforge/envs/hh4b/lib/python3.9/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [16:26:10] /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/dmlc-core/src/io/local_filesys.cc:209: Check failed: allow_null:  LocalFileSystem::Open \"trained_bdt.model\": No such file or directory\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x00000001871e99b4 dmlc::LogMessageFatal::~LogMessageFatal() + 116\n  [bt] (1) 2   libxgboost.dylib                    0x00000001874a3f17 dmlc::io::LocalFileSystem::Open(dmlc::io::URI const&, char const*, bool) + 903\n  [bt] (2) 3   libxgboost.dylib                    0x000000018748fa2c dmlc::Stream::Create(char const*, char const*, bool) + 76\n  [bt] (3) 4   libxgboost.dylib                    0x000000018720a683 XGBoosterLoadModel + 787\n  [bt] (4) 5   libffi.8.dylib                      0x00000001090d8972 ffi_call_unix64 + 82\n  [bt] (5) 6   ???                                 0x0000000301ac7b80 0x0 + 12912982912\n\n"
     ]
    }
   ],
   "source": [
    "# run inference\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "model_path = '\"/Users/daniel/Documents/UCSD/Research/HH4b/src/HH4b/boosted/bdt_trainings_run3/v1_msd30_nomulticlass'\n",
    "\n",
    "# Create an XGBClassifier instance\n",
    "bdt_model = XGBClassifier()\n",
    "\n",
    "# Load the model\n",
    "bdt_model.load_model(\"trained_bdt.model\")\n",
    "\n",
    "# prepare data\n",
    "dtest = xgb.DMatrix(df[df_ak8])\n",
    "\n",
    "# Make predictions\n",
    "predictions = bdt_model.predict(dtest)\n",
    "probabilities = bdt_model.predict_proba(dtest)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "roc_auc = roc_auc_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
